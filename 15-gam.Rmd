# 일반화가법모형 {#gam}

일반화가법모형을 처음 다룬 논문들로는 [@Hastie1986]이 있다. 일반화가법모형에 대한 참고문헌으로는 [@Wood2006]이 있다. 최근에 나온 좋은 책으로는 [@Yee2015]가 있으며 이 책은 주로 **벡터 일반화선형모형(vector genearlized linear model, VGLMs)** 및 **벡터 일반화가법모형(vector generalized addtive model, VGAMs)**에 초점을 맞추고 있고 R 코드를 포함하고 있다.

일반화가법모형은 일반화 선형모형이며 선형 predictor가 smooth functions of covariate의 합(sum) 형태로 표현된 모형을 말한다. 일반적으로는
$$g(\mu_{i})=\mathbf{A}_{i}\boldsymbol{\theta}+f_{1}(x_{1i})+f_{2}(x_{2i})+f_{3}(x_{3i})+\ldots$$
로 모형 structure를 표현할 수 있다. 이 때 $\mu_{i}\equiv E(Y_{i})$이고 $Y_{i}\sim \text{EF}(\mu_{i},\phi)$이다. $Y_{i}$는 설명변수들이고, $\text{EF}(\mu_{i},\phi)$는 평균 $\mu_{i}$와 척도모수(scale parameter) $\phi$인 지수족 분포(exponential family distribution)를 나타낸다. $\mathbf{A}_{i}$는 어떤 strictly parametric model components의 모형 행렬의 행(row)를 나타낸다. $\theta$는 대응되는 모수 벡터이다. $f_{j}$는 covariates들 $x_{k}$들의 smooth functions이다.

## 가법모형들(additive models)

회귀분석의 **가법모형(additive model)**은
$$E[Y|\mathbf{X}=\mathbf{x}]=\alpha+\sum_{j=1}^{p}f_{j}(x_j)$$
로 표현한다. 선형모형은 $f_{j}(x_{j})=\beta_{j}x_{j}$인 가법모형의 특별한 경우이다. $f_{j}$는 임의의 비선형 함수도 될 수 있다는 점에서 가법모형이 좀 더 일반적인 형태의 모형이라고 할 수 있다.

## 벡터 일반화가법모형(vector GAMs)

스므딩에는 다음과 같은 네 가지 종류가 있다.

1. $y$는 스칼라, $x$는 일변량

2. $y$는 스칼라, $\mathbf{x}$는 다변량

3. $\mathbf{y}$는 벡터, $x$는 일변량

4. $\mathbf{y}$는 벡터, $\mathbf{x}$는 다변량

일반적으로 얘기하는 벡터 일반화가법모형은 (3)에 해당한다.

## 퇴각 적합화(backfitting)

**퇴각 적합화(backfitting)**는 **가우스-자이델 방법(Gauss-Seidel method)**이라고도 불리는 방법으로, GAM을 적합하기 위한 단순하면서도 아름다운 방법이다. 이 알고리즘의 기본 아이디어는 가법모형의 각 smooth component들에 대해 iteratively하게 가법모형의 smooth partial residuals를 계산하는 것이다. 

다음과 같은 가법모형에서 추정을 하고 싶다고 가정하자.
$$y_{i}=\alpha + \sum_{j=1}^{m}f_{j}(x_{ji})+\epsilon_{i},$$
여기서 $f_{i}$는 부드러운 함수들(smooth functions)이며 공변량 $x_{j}$는 때때로 벡터이기도 하다. $\hat{\mathbf{f}}_{j}$를 $i$째 원소의 추정값이 $f_{j}(x_{ji})$인 벡터라고 하자. 그러면 기본적인 퇴각 적합화 알고리즘은 다음과 같다.

1. Set $\alpha=\bar{y}$ and $\hat{\mathbf{f}}_{j}=\mathbf{0}$ for $j=1,\ldots , m$.

2. Repeat step 3 to 5 until teh estimates $\hat{\mathbf{f}}_{j}=\mathbf{0}$ stop changing.

3. For $j=1,\ldots ,m$, repeat steps 4 and 5.

4. Calculate partial residuals:
$$\mathbf{e}_{P}^{j}=\mathbf{y}-\hat{\alpha}-\sum_{k\neq j}\hat{\mathbf{f}}_{k}.$$

5. Set $\hat{\mathbf{f}}_{j}$ equal to the result of smoothing $e_{p}^{j}$ with respect to $x_{j}$.

## R 예제(R-gam)

```{r, message=F, echo=F}
library(wavethresh)
```

알고리즘을 짜면 다음과 같다.

```{r, comment=">", eval=F, echo=T}
f<-x*0;alpha<-mean(y);ok <- TRUE
while (ok) { # backfitting loop
  for (i in 1:m) { # loop through the smooth terms
    ep <- y - rowSums(f[,-i]) - alpha
    b <- smooth.spline(x[,i],ep,df=edf[i])
    f[,i] <- predict(b,x[,i])$y
  }
  rss <- sum((y-rowSums(f))ˆ2)
  if (abs(rss-rss0)<1e-6*rss) ok <- FALSE
  rss0 <- rss
}
```
