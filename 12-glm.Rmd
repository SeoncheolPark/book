# 일반화선형모형 {#glm}

[@Nelder1972]는 몇가지 중요한 회귀분석 모형들을 통합한 개념인 **일반화선형모형(generalized linear model, GLM)**을 발표하였다. 이 논문에서 **포아송 회귀(Poisson regression)**와 **로지스틱 회귀(logistic regression)**가 GLM의 특수한 경우임을 보이고 추정을 위해 **반복재가중최소제곱(iteratively reweighted least squares, IRLS)** 알고리즘이 사용될 수 있음을 보였다.

## 일반화선형모형의 기본(GLM basics)

다음과 같은 독립인 샘플 자료 $(\mathbf{x}_{i}, y_{i}), i=1,\ldots, n$이 있다고 하자. 이 떄 $y_{i}$는 반응변수, $n$은 표본의 크기 그리고 $\mathbf{x}_{i}=(x_{i1},\ldots, x_{ip})^{T}$는 $p$개의 설명변수들의 벡터이다. ($x_{i1}=1$로 보통 놓는다)

**일반화선형모형(generalized linear model)**은 일반적으로 다음의 세 개의 파트로 구성된다.

1. **random component**: $f(y;\mu)$는 $Y$의 분포를 특정한다.

2. **systematic component**: $\eta = \boldsymbol{\beta}^{T}\mathbf{x}$는 알려진 설명변수들로 설명될 수 있는 $Y$의 variation을 특정한다.

3. **link function**: $g(\mu)=\boldsymbol{\beta}^{T}\mathbf{x}$는 이 둘을 묶는 역할을 한다. 종종 $g$는 단순히 **링크(link)**라고 불리기도 한다.

$\eta$는 **선형예측변수(linear predictor)**로 알려져있다. 무작위 성분 $f(y;\mu)$는 보통 $E(Y|\mathbf{x})=\mu(\mathbf{x})$인 지수족분포(평균함수)로 모델링한다. GLM은 $g$가 알려져 있을 때 다음과 같은 모형을 적합한다.
$$g(\mu(\mathbf{x}_{i}))=\eta_{i}=\boldsymbol{\beta}^{T}\mathbf{x}_{i}=\beta_{1}x_{i1}+\ldots+\beta_{p}x_{ip}.$$ 
이 때 $g$는 strictly monotonicity해야 하고 $\mu$의 range에서 적어도 두 번 미분이 가능해야 한다. $g$의 주된 역할은 평균을 변환시켜 최적화 문제가 잘 작동하도록 하는 것이다. 벡테 GLM 표현으로는 위 식을 다음과 같이 표현한다.
$$g_{1}(\mu(\mathbf{x}_{i}))=\eta_{i1}=\boldsymbol{\beta}_{1}^{T}\mathbf{x}_{i}=\beta_{(1)1}x_{i1}+\ldots+\beta_{(1)p}x_{ip}.$$ 
즉 모형에 한 개 이상의 선형번수를 허락하는 것이다.

관찰값이 한 개 있을때, 지수족의 확률밀도함수는 다음과 같이 쓸 수 있다.
$$f(y;\theta, \phi)=\exp\{ \frac{y\theta-b(\theta)}{a(\phi)} +c(y,\phi) \}.$$
이 때 $\theta$는 **자연모수(natural parameter)** 또는 **정준모수(canonical parameter)**로 불리고, $\phi$는 **산포모수(dispersion parameter)**라고 불린다. 그리고 $a,b,c$는 알려진 함수들이다. $\phi$가 알려졌으면, $Y$의 분포는 one-parameter canonical exponential family member가 된다. $\phi$가 알려져있지 않을 때에는 이것은 nuisance parameter가 되며 주로 method of moments로 추정된다. 많은 GLM 이론에서 $\phi$는 알려져 있지 않다고 하다라도 모수로 취급하지 않고 상수로 취급하는 경향이 있다.

```{example, name="정규분포는 지수족의 멤버"}
정규분포는 다음과 같이 지수족으로 표현할 수 있다.
\begin{align*}
f(y;\mu)&=\frac{1}{\sigma\sqrt{2\pi}}\exp[-\frac{(y-\mu)^{2}}{2\sigma^{2}}]\\
&=\exp[\frac{-y^{2}+2y\mu -\mu^{2}}{2\sigma^{2}}-\log (\sigma\sqrt{2\pi})]\\
&=\exp[\frac{y\mu -\mu^{2}/2}{\sigma^{2}}-\frac{y^{2}}{2\sigma^{2}}-\log (\sigma\sqrt{2\pi})]
\end{align*}
이 때 $\theta=\mu$, $b(\theta)=\theta^{2}/2\equiv \mu^{2}/2$, $a(\phi)=\phi=\sigma^{2}$이고 $c(\phi,y)=-y^{2}/(2\phi) -\log(\sqrt{\phi 2\pi})\equiv -y^{2}/(2\sigma^{2})-\log(\sigma\sqrt{2\pi})$가 된다.

```

지수족 분포의 평균과 분산에 대해 $a,b,\phi$를 이용하여 좀 더 일반적인 표현을 얻는 것도 가능하다. $y$가 주어졌을 때 $\theta$의 로그 가능도를 추정하는 것은 단순히 $\log\{f(y;\theta)\}$를 $\theta$에 대한 함수로 생각하면 된다. 즉
$$l(\theta)= \frac{y\theta-b(\theta)}{a(\phi)} +c(y,\phi)$$
이며 따라서 이것의 편미분은 $l$을 확률변수로 다룸으로써 얻을 수 있다.
$$\frac{\partial l}{\partial \theta}=\frac{y-b'(\theta)}{a(\phi)}.$$
특별한 관찰값 $y$를 확률변수로 대체하면 $\frac{\partial l}{\partial \theta}$의 기댓값을 구하는 것이 가능하다.
$$\mathbb{E}(\frac{\partial l}{\partial \theta})=\frac{\mathbb{E}(Y)-b'(\theta)}{a(\phi)}.$$
$\mathbb{E}(\frac{\partial l}{\partial \theta})=0$으로 하고 다시 정렬하면 다음과 같은 식을 얻을 수 있다.
$$\mathbb{E}(Y)=b'(\theta).$$
이 말은 즉슨, 어떤 지수족 확률변수의 기댓값은 $b$를 $\theta$에 대해 한 번 미분함으로써 얻을 수 있다는 것이다. 가능도를 한 번 더 미분하면 다음 식을 얻는다.
$$\frac{\partial^{2} l}{\partial \theta^{2}}=-\frac{b''(\theta)}{a(\phi)}.$$
이것을 일반적인 결과에 넣으면, $\mathbb{E}(\frac{\partial^{2} l}{\partial \theta^{2}})=-\mathbb{E}\{(\frac{\partial l}{\partial \theta})^{2}\}$를 얻는다. 또한 이것을 이용해 다음 식을 얻을 수 있다.
$$\frac{b''(\theta)}{a(\phi)}=\frac{\mathbb{E}[\{Y-b'(\theta)\}^{2}]}{a(\phi)^{2}}.$$
잘 정리하면 다음과 같은 일반적인 결과를 얻을 수 있다.
$$\text{var}(Y)=b''(\theta)a(\phi).$$
만약 우리가 $\theta$를 알고 있다면 GLM을 다루는 데 어떤 어려움도 없다. 그러나 만약 우리가 $\phi$를 모르고 있다면 $a(\phi)=\frac{\phi}{A}$ ($A$는 알려진 상수)로 표현할 수 있지 않는 이상 상황이 어려워진다.

### 일반화선형모형의 적합(fitting generalized linear models)

이 부분은 [@Wood2006]을 참고하였다. 독립 설명변수들의 $n$-벡터 $\mathbf{Y}$의 GLM 모형이
$$g(\mu_{i})=\mathbf{X}_{i}\boldsymbol{\beta}, \qquad{Y_{i}\sim f(y_{i};\theta_{i})}$$
라고하자. 이 때 $f(y_{i};\theta_{i})$는 지수족 분포를 나타내며 $\theta$는 $\mu_{i}$에 의해(즉 궁극적으로 $\boldsymbol{\beta}$에 의해)결정되는 정준모수다. $\mathbf{Y}$의 관찰값 $\mathbf{y}$가 주어졌을 때, $\boldsymbol{\beta}$의 최대우도추정이 가능하다. $Y_{i}$는 상호 독립이라고 했으므로, $\boldsymbol{\beta}$의 가능도는 다음과 같다.
$$L(\boldsymbol{\beta})=\prod_{i=1}^{n}f(y_{i};\theta_{i}).$$
따라서 $\boldsymbol{\beta}$의 로그-가능도는
$$l(\boldsymbol{\beta})=\sum_{i=1}^{n}\log \{ f(y_{i};\theta_{i}) \}=\sum_{i=1}^{n}\frac{y_{i}\theta_{i}-b_{i}(\theta_{i})}{a_{i}(\phi)}+c_{i}(\phi,y_{i})$$
이다. 어떤 경우에는 $a,b,c$가 $i$에 따라 달라질 수도 있다. 한편 $\phi$는 $i$에 따라 보통 같다고 놓는다. 앞 절에서 논의한대로 가장 간단한 $a_{i}(\phi)=\frac{\phi}{A_{i}}$ (이 때 $A_{i}$는 알려진 상수, 주로 1)를 가정하자. 그러면
$$l(\boldsymbol{\beta})=\sum_{i=1}^{n}A_{i}\frac{y_{i}\theta_{i}-b_{i}(\theta_{i})}{\phi}+c_{i}(\phi,y_{i})$$
가 된다. 이 로그-가능도를 최대화하기 위해서는 보통 **뉴턴의 방법(Newton's method)**를 사용한다. 이것은 $l$의 그라디언트 벡터와 헤시안을 계산해야 한다.
$$\frac{\partial l}{\partial \beta_{j}}=\frac{1}{\phi}\sum_{i=1}^{n}A_{i}\Big( y_{i}\frac{\partial \theta_{i}}{\partial \beta_{j}}-b_{i}'(\theta_{i})\frac{\partial \theta_{i}}{\partial \beta_{j}}   \Big).$$
또한 **체인룰(chain rule)**에 따라
$$\frac{\partial \theta_{i}}{\partial \beta_{j}}=\frac{d\theta_{i}}{d\mu_{i}}\frac{\partial\mu_{i}}{\partial \beta_{j}} =\frac{d\theta_{i}}{d\mu_{i}}\frac{\partial\eta_{i}}{\partial \beta_{j}}\frac{d\mu_{i}}{d\eta_{i}}=\frac{X_{ij}}{g'(\mu_{i})b''(\theta_{i})}.$$
마지막 등식은 $\frac{d\mu_{i}}{d\eta_{i}}=g'(\mu_{i})$ 그리고 $\frac{\partial\eta_{i}}{\partial \beta_{j}}=X_{ij}$, $\frac{d\theta_{i}}{d\mu_{i}}=\frac{1}{b;;(\theta_{i})}$라는 사실로부터 얻어진다. 이들을 잘 정리하면,
$$\frac{\partial l}{\partial \beta_{j}}=\frac{1}{\phi}\sum_{i=1}^{n}\frac{y_{i}-b_{i}'(\theta)}{g'(\mu_{i})b_{i}''(\theta_{i})/A_{i}}X_{ij}=\frac{1}{\phi}\sum_{i=1}^{n}\frac{y_{i}-\mu_{i}}{g'(\mu_{i})V(\mu_{i})}X_{ij}$$
를 얻는다. 다시 미분하면 다음을 얻는다.
\begin{align*}
\frac{\partial^{2}l}{\partial \beta_{j}\partial \beta_{k}}&=-\frac{1}{\phi}\sum_{i=1}^{n}\Big\{ \frac{X_{ik}X_{ij}}{g'(\mu_{i})^{2}V(\mu_{i})} - \frac{(y_{i}-\mu_{i})V'(\mu_{i})X_{ik}X_{ij}}{g'(\mu_{i})^{2}V(\mu_{i})^{2}}+\frac{(y_{i}-\mu_{i})X_{ij}g''(\mu_{i})X_{ik}}{g'(\mu_{i})^{3}V(\mu_{i})} \Big\}\\
&-\frac{1}{\phi}\sum_{i=1}^{n}\frac{X_{ik}X_{ij}\alpha(\mu_{i})}{g'(\mu_{i})^{2}V(\mu_{i})}.
\end{align*}
이 때 $\alpha(\mu_{i})= 1+ (y_{i}-\mu_{i})\{ \frac{V'(\mu_{i})}{V(\mu_{i})} + \frac{g''(\mu_{i})}{g'(\mu_{i})} \}$이다. 표현 $\mathbb{E}(\frac{\partial^{2}l}{\partial \beta_{j}\partial \beta_{k}})$ 또한 똑같지만, $\alpha(\mu_{i})= 1$이다. 따라서 $\mathbf{W}=\text{diag}(w_{i})$, $w_{i}=\frac{a(\mu_{i})}{g'(\mu_{i})^{2}V(\mu_{i})}$로 정의하면, 로그-가능도의 헤시안은 $-\frac{\mathbf{XWX}}{\phi}$가 되며, 헤시안의 기댓값은 $\alpah(\mu_{i})=1$로 놓음으로써 얻을 수 있다. $\alpha(\mu_{i})=1$로 계산된 가중치들을 **피셔 가중치(Fisher weight)**들이라고 부른다. $\mathbf{G}=\text{diag}\{\frac{g'(\mu_{i})}{\alpha(\mu_{i})}\}$로 정의함으로써, 그라디언트 벡터의 로그-가능도는 $\frac{\mathbf{X}^{T}\mathbf{WG}(\mathbf{y}-\boldsymbol{\mu})}{\phi}$가 된다. 그러면 뉴턴 업데이트는 다음과 같은 형태를 갖는다.
\begin{align*}
\boldsymbol{\beta}^{[k+1]}&=\\boldsymbol{\beta}^{[k]}+(\mathbf{XWX})^{-1}\mathbf{X}^{T}\mathbf{WG}(\mathbf{y}-\boldsymbol{\mu})\\
&=(\mathbf{XWX})^{-1}\mathbf{X}^{T}\mathbf{W}\{\mathbf{G}(\mathbf{y}-\boldsymbol{\mu})+\mathbf{X}\boldsymbol{\beta}^{[k]}\}\\
&-(\mathbf{XWX})^{-1}\mathbf{X}^{T}\mathbf{W}\mathbf{z}.
\end{align*}
이 때 $z_{i}=g'(\mu_{i})\frac{y_{i}-\mu_{i}}{\alpha(\mu_{i})}+\eta_{i}$이다. ($\boldsymbol{\eta}\equiv\mathbf{X}\boldsymbol{\beta}$이다.) 결국 이 이생 방정식은 $\boldsymbol{\beta}$에 대한 가중 최소자승방법 중 하나라고 볼 수 있다.

결국 GLM은 **반복 재가중 최소자승(iteratively re-weighted least squares)** 알고리즘으로 추정되 수 있으며, 다음과 같다.

1. $\hat{\mu}_{i}=y_{i}+\delta_{i}$, $\hat{\eta}_{i}=g(\hat{\mu}_{i})$로 초기화한다. $\delta_{i}$는 보통 0으로 놓으며, $\hat{\eta}_{i}$가 유한함을 보장하기 위해 작은 상수일 수도 있다. 수렴할 때까지 아래 두 단계를 반복한다.

2. Pseudo-data $z_{i}=g'(\hat{\mu}_{i})\frac{y_{i}-\hat{\mu}_{i}}{\alpha(\hat{\mu}_{i})}+\hat{\eta}_{i}$와 반복 가중치 $w_{i}=\frac{a(\hat{\mu}_{i})}{g'(\hat{\mu}_{i})^{2}V(\hat{\mu}_{i})}$를 계산한다.

3. 가중 최소자승
$$\sum_{i=1}^{n}w_{i}(z_{i}-\mathbf{X}_{i}\boldsymbol{\beta})^{2}$$
을 최소화하는 $\hat{\boldsymbol{\beta}}$를 찾는다. 그리고 $\hat{\boldsymbol{\eta}}=\mathbf{X}\hat{\boldsymbol{\beta}}$로, $\hat{\mu}_{i}=g^{-1}(\hat{\eta}_{i})$로 갱신한다.

## 일변량 스므딩(univariate smoothing)

[@Yee2015]에 따르면 스므더(smoother)의 종류에는 다음이 있다.

1. Regression or series smoothers (polynomial regression, regression splines, P-splines, Fourier regression, filtering, wavelets)

2. Smoothing splines (with roughness penalties, e.g., cubic smoothing splines, O-splines, P-splines)

3. Local regression (Nadaraya-Watson estimator, kernel smoothers, Lowess, Loess, it generalizes to local likelihood)

4. Nearest-neighbour smoothers (running means, running lines, running medians)

**고전적인 스므딩 문제(classical smoothing problem)**는 다음 모형에 대해 임의의 부드러운 함수(smooth function) $f$를 추정하는 문제이다.

$$y_{i}=f(x_{i})+\epsilon_{i}, \qquad{\epsilon_{i} \sim  (0, \sigma_{i}^{2})}$$

일반적으로 자료를 $x_{1}<x_{2}<\ldots <x_{n}$ 등으로 크기에 따라 정렬할 수 있다고 가정하면, 모든 스므딩 방법의 기본 아이디어는 **이웃(neighbourhood)**을 정의하는 것이다. 타겟 포인트 $x_{0}$가 있을때 이것의 이웃의 크기를 정하는 일은 함수의 부드러움을 결정하는 문제와 같이 때문에 매우 중요하다.

## 벡터 일반화선형모형(vector generalized linear models)

이 절의 내용은 [@Yee2015]를 따른다.

## 일반화선형모형의 한계점(limations of glm)

[@Yee2015]에 의하면 GLM은 잘 알려진 지수족 내의 일차원 분포에서만 잘 적용된다는 한계점을 가지고 있다.
