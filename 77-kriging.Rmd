# 크리깅 {#kriging}

이 문서에서는 지금까지 다뤘던 추정 방법들을 가지고 **공간 예측(spatial prediction)**을 하는 방법들에 대해 다루겠다. 공간 예측을 다른 말로 **크리깅(Kriging)**이라 부른다.

어떤 정사각형 공간에 공간적 상관관계를 가지는 자료들이 놓여있다고 가정해보자. 몇 개의 장소가 관측되어있지 않다고 했을 때, 그 결측된 장소에 대한 가장 최선의 추축은 무엇인가? 이러한 물음이 크리깅 컨셉을 생각하게 한다. 크리깅은 다른 말로 **최소 선형 불편 예측(best linear unbiased prediction, BLUP)**라고도 하는데, 그 장소에서 예측한 값들은 다른 장소에서 예측한 값들의 선형 결합이 된다. 이런 선형 결합의 계수를 결정하기 위해서 공분산이 중요한 역할을 하게 된다.

## 공간 예측(spatial prediction)

$Z(\mathbf{s})$를 spatial process라고 하자. 목표는 $n$개의 관측값(data) $\{ Z(\mathbf{s}_{1}), \cdots  Z(\mathbf{s}_{n}) \}$을 이용해 관측되어지지 않은 장소 $\mathbf{s}_{0}$의 $Z(\mathbf{s}_{0})$을 predict하는 것이다. 그런데 **문제는 $Z(\mathbf{s}_{0})$또한 확률변수라는 것이다**(그래서 predict라는 말을 쓴다고 한다).

그렇다면 어떤 기준을 가지고 prediction할 것인가? 가장 일반적인 기준으로는 **mean-square prediction error (MSPE)**를 계산하는 것이다. 우선 몇 가지 notation들을 정리해보자.

$$\mathbf{Z}=(Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n}))^{T}: \text{ data vector}$$
$$Z_{0}\equiv Z(\mathbf{s}_{0}) \text{ (간단히 쓰기 위함)}$$
$$T=Z_{0}: \text{ predict하고싶은 것}$$
$$\hat{T}=\hat{Z}_{0}: \text{ "prediction"}=t(\mathbf{Z}) \text{ ( 데이터들에 대한 함수)}$$

Prediction error를 $(T-\hat{T})^{2}$이라고 하면 MSPE는 $E(T-\hat{T})^{2}$라고 하며
$$\hat{T}=\text{argmin}_{\tilde{T}\in t(\mathbf{Z})}E(T-\tilde{T})^{2}$$
을 만족하는 $\hat{T}$를 **best predictior (BP)**라 부른다.

Restriction을 걸어 다른 predction을 할 수도 있다. 예를 들면 **best linear predictor (BLP)**, **best linear unbiased predictor (BLUP)**등이다. BLUP를 많이 쓴다고 한다. Random effect에서 등장하는 개념과 똑같으나 계산할 때만 spatial covariance로 넣어 계산하는 것이라고 생각할 수도 있다.

여기서 말하는 **결론: $\hat{T}=E(Z_{0}|\mathbf{Z})$**

Geostatistics에서 spation prediction은 1950년대 남아공 마이닝 엔지니어 D.G. Krige의 이름을 따 **크리깅(Kriging)**이라 부른다. 이 크리깅의 종류는 여러가지가 있다.

## 일반 크리깅(universal Kriging)

$$\text{Model: } \mathbf{Z}(\mathbf{s})=\mathbf{x}^{T}(\mathbf{s})\boldsymbol{\beta}+\boldsymbol{\epsilon}(\mathbf{s}) \text{ (spatial dependence는 } \boldsymbol{\epsilon}(\mathbf{s})\text{에서 나온다)}$$
$$\text{Data: } \mathbf{Z}=(Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n}))^{T}$$
$$\text{Want to predict } Z_{0}=Z(\mathbf{s}_{0})=\mathbf{x}^{T}(\mathbf{s}_{0})\boldsymbol{\beta}+\mathbf+\boldsymbol{\epsilon}(\mathbf{s}_{0})\stackrel{let}{=}\mathbf{x}_{0}\boldsymbol{\beta}+\boldsymbol{\epsilon}_{0}$$

일단 covariance structure가 필요하다.
$$
\text{Cov}\left[\begin{array}
{r}
\mathbf{Z}\\
Z_{0}\\
\end{array}\right]
=
\left[\begin{array}
{rr}
\boldsymbol{\Sigma}_{n\times n} & \mathbf{T}_{n \times 1}\\
\mathbf{T}_{1\times n}^{T} & \sigma_{0}^{2}\\
\end{array}\right]
$$
로 놓는다. 여기서 $\boldsymbol{\Sigma}=\text{Cov}(\mathbf{Z})$, $\mathbf{T}_{j}=\text{Cov}(Z(\mathbf{s}_{j}), Z_{0})$, $\sigma_{0}^{2}=\text{Var}(Z_{0})$이다.

지금부터 MSPE를 minimize하는 predictor들을 찾는 방법을 다룰 것이다. 이 방법들은 공간통계에 국한된 것이 아니고 mixed effect model 등에도 해당되는 것이다.

### 라그랑즈 승수 접근법(Lagrange multiplier approach)

Linear unbiased predictor
$$\hat{Z}_{0}=\sum_{i=1}^{n}\lambda_{i}z_{i}=\boldsymbol{\lambda}^{T}\mathbf{Z} \text{ and } E(\hat{Z}_{0}-Z_{0})$$
를 가정하자. 여기서 주의하여야 할 점은 $Z_{0}$자체도 random이므로 $E(\hat{Z}_{0})=Z_{0}$라 할 수 없다는 것이다. $\mathbf{Z}=\boldsymbol{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon}$을 이용하면

\begin{eqnarray*}
E(\hat{Z}_{0})&=&E(\boldsymbol{\lambda}^{T}\mathbf{Z})\\
&\Longrightarrow& E(\boldsymbol{\lambda}^{T}(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}))=\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
&\Longleftrightarrow& \boldsymbol{\lambda}^{T}\mathbf{X}\boldsymbol{\beta}=\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
&\Longleftrightarrow& \boldsymbol{\lambda}^{T}\mathbf{X}\mathbf{x}_{0}^{T} \textbf{ (unbiasedness 조건)}\\
\end{eqnarray*}

마지막 줄이 성립하는 이유는 $\boldsymbol{\lambda}^{T}\mathbf{X}$, $\mathbf{x}_{0}^{T}$는 행렬이 아닌 벡터지만 임의의 $\boldsymbol{\beta}$에 대해 세번째 줄 식이 모두 만족해야 하기 때문이다.

$E(\hat{Z}_{0}-Z_{0})^{2}$을 minimize하기 위해 우선

\begin{eqnarray*}
\hat{Z}_{0}-Z_{0} &=& \boldsymbol{\lambda}^{T}(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}-(\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\epsilon_{0}))\\
&=&\boldsymbol{\lambda}^{T}\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\mathbf{x}_{0}^{T}\boldsymbol{\beta}-\epsilon_{0}\\
&=&\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\mathbf{x}_{0}^{T}\boldsymbol{\beta}-\epsilon_{0}\text{ (unbiased constraint를 쓴다)}\\
&=&\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0}
\end{eqnarray*}

마지막 식을 보면, unbiasedness 조건을 줬더니 놀랍게도 $\boldsymbol{\beta}$가 없어졌음을 알 수 있다. 이는 $\boldsymbol{\beta}$를 뭘 쓸지 고민할 필요 없이 prediction이 가능하다는 의미라고 한다.

그러면 $E(\hat{Z}_{0}-Z_{0})^{2}$의 계산은

\begin{eqnarray*}
E(\hat{Z}_{0}-Z_{0})^{2}&=&E(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0})^{2}\\
&=&E((\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{2})+E(\epsilon_{0}^{2})-2E(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0})\\
&=&\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}\boldsymbol{\lambda}+\sigma_{0}^{2}-2\boldsymbol{\lambda}^{T}\mathbf{T}
\end{eqnarray*}

$E((\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{2})$를 계산하기 위해 $\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}$이 스칼라라는 것을 이용, $(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{T}=\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\boldsymbol{\lambda}$임를 이용하였다.

이제 $\boldsymbol{\lambda}^{T}\mathbf{X}\mathbf{x}_{0}^{T}$ 제한이 걸린 ($\star$)를 최소화하는 문제 (constraint optimization problem)을 풀기 위해 **Lagrange multiplier**를 쓰도록 하자. 참고로 나중에 추정을 해야 하지만 지금은 $\boldsymbol{\Sigma}, \sigma_{0}^{2}$을 안다고 가정한다. 다음
$$
L(\boldsymbol{\lambda},\boldsymbol{\nu})=\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}\boldsymbol{\lambda}-2\boldsymbol{\lambda}^{T}\mathbf{T}+\sigma_{0}^{2}-2(\boldsymbol{\lambda}^{T}\mathbf{X}-\mathbf{x}_{0}^{T})\boldsymbol{\nu} \text{ (계산을 편하게 하기 위해 constraint에 2를 곱합)}
$$
식을 미분하면 minimizer를 구할 수 있다.
$$\frac{\partial L(\boldsymbol{\lambda},\boldsymbol{\nu})}{\partial \boldsymbol{\lambda}}=2\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}-2\mathbf{T}-2\mathbf{X}\boldsymbol{\nu}=\mathbf{0}$$
$$\frac{\partial L(\boldsymbol{\lambda},\boldsymbol{\nu})}{\partial \boldsymbol{\nu}}=-2(\mathbf{x}_{0}^{T}-\boldsymbol{\lambda}^{T}\mathbf{X})=0 $$

우선 첫 번째 식으로부터
$$\hat{\boldsymbol{\lambda}}=\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}\boldsymbol{\nu})$$
를 얻는다. 이것을 두 번째 식에 집어넣으면
$$
\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}\boldsymbol{\nu})=\mathbf{x}_{0}\\
\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}+\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X}\boldsymbol{\nu}=\mathbf{x}_{0}\\
\therefore \hat{\boldsymbol{\nu}}=(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})\\
$$
가 된다. 이 식을 다시 첫 번째 식에 집어넣는다. 그러면
$$\hat{\boldsymbol{\lambda}}=\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}))$$
이고

\begin{eqnarray*}
\hat{Z_{0}}&=&\hat{\boldsymbol{\lambda}}^{T}\mathbf{Z}=(\mathbf{T}+\mathbf{X}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}))^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}\\
&=&\mathbf{T}^{2}\boldsymbol{\Sigma}^{-1}\mathbf{Z}+(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})^{T}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}\\
&=&\mathbf{x}_{0}\hat{\boldsymbol{\beta}}_{\text{GLS}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}}_{\text{GLS}})\\
\end{eqnarray*}

이다. 마지막 식을 살펴보면 $(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}}_{\text{GLS}})$는 일종의 residual로 생각할 수 있고 $\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}$은 대응되는 적당한 weight라고 생각할 수 있다. 여기서
$$\hat{\boldsymbol{\beta}}_{\text{GLS}}=(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}$$
이다.

### 조건부분포 방법(conditional distribution approach)

이 방법은 라그랑지 승수법을 이용한 방법과 달리 다변량 분포의 정규성 가정이 필요하다. 즉
$$
\begin{pmatrix}
\mathbf{Z}\\
Z_{0}\\
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
\mathbf{X}\boldsymbol{\beta}\\
\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
\end{pmatrix},
\begin{pmatrix}
\boldsymbol{\Sigma} & \mathbf{T}\\
\mathbf{T}^{T} & \sigma_{0}^{2}\\
\end{pmatrix}
\end{bmatrix}
$$
으로 가정한다. 그러면 $Z_{0}|\mathbf{Z}$의 분포는
$$Z_{0}|\mathbf{Z} \sim \mathcal{N}(\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta}), \sigma_{0}^{2}-\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})$$
가 된다. 조건부 기댓값을 구하는 공식에 의해 $\hat{Z}_{0}$을 정할 수 있다.
$$\hat{Z}_{0}=E(Z_{0}|\mathbf{Z})=\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\mathbf{T}^{T}\Sigma^{-1}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta}).$$
그런데 이 추정량은 알려지지 않은 $\boldsymbol{\beta}$가 들어있어 문제가 된다. 그래서 실제 문제에서 이대로 쓰지 못한다. 따라서
$$\hat{Z}_{0}\Longrightarrow \mathbf{x}_{0}^{T}\hat{\boldsymbol{\beta}}+\mathbf{T}^{T}\Sigma^{-1}(\mathbf{Z}-\mathbf{X}\hat{\boldsymbol{\beta}})$$
로 바꿔 사용한다(참고로 normal 가정시 BP=BLP=BLUP라고 한다).

### 베이지안 방법(Bayesian approach)
여기서도 조건부 분포 방법과 마찬가지로 다변량 분포의 정규성 가정이 필요하다.
$$
\begin{pmatrix}
\mathbf{Z}\\
Z_{0}\\
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
\mathbf{X}\boldsymbol{\beta}\\
\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
\end{pmatrix},
\begin{pmatrix}
\boldsymbol{\Sigma} & \mathbf{T}\\
\mathbf{T}^{T} & \sigma_{0}^{2}\\
\end{pmatrix}
\end{bmatrix}
$$
이때 
$$\Sigma=\sigma^{2}V(\theta), \mathbf{T}=\sigma^{2}W(\theta), \sigma_{0}^{2}=\sigma^{2}$$
으로 놓는다. 그 다음에는 $\boldsymbol{\beta}, \sigma^{2}, \theta$(보통 1,2차원이라 가정한다)에 대한 사전분포를 만들어야 한다. 일반적으로 이 사전분포는
$$\propto \pi(\theta)\cdot \frac{1}{\sigma^{2}}$$
로 준다. $\boldsymbol{\beta}$는 flat prior를 주며 $\pi(\theta)$는 parameter에 따라 바뀐다. 가능하면 사전분포의 contribution을 없애고 싶기 때문에 이렇게 놓는다고 한다.

Bayesian setting에서는 $Z_{0}|\mathbf{Z}$에 $Z_{0}|\mathbf{Z}, \boldsymbol{\beta}, \sigma^{2}, \theta$(이 분포는 알고 있는 분포이다)가 숨어있는 꼴이라고 한다. $\pi(Z_{0}|\mathbf{Z})$를 만들려면 $\boldsymbol{\beta}, \sigma^{2}, \theta$에 대해 적분해야 한다. 먼저 $\boldsymbol{\beta}$에 대해 decompose를 이용해 적분하면

\begin{eqnarray*}
\pi(Z_{0}|\mathbf{Z},\sigma^{2},\theta)&=&\int \pi(Z_{0},\boldsymbol{\beta}|\mathbf{Z},\sigma^{2},\theta)d\mathbf{\beta}\\
&=& \int \pi(Z_{0}|\mathbf{Z},\boldsymbol{\beta},\sigma^{2},\theta)\pi(\boldsymbol{\beta}|\mathbf{Z},\sigma^{2},\theta)d\mathbf{\beta}\\
\end{eqnarray*}

으로 구할 수 있다. 같은 방법으로 $\sigma^{2}$에 대해서도 적분하면
$$\pi(Z_{0}|\mathbf{Z},\theta)=\int \pi(Z_{0},\sigma^{2}|\mathbf{Z},\theta)d\sigma^{2}$$
으로 구할 수 있다. 여기까지는 explicit form이 잘 나온다고 한다. 그리고 $\theta$에 대해서도 마찬가지로 적분할 수 있는데

\begin{eqnarray*}
\pi(Z_{0}|\mathbf{Z})&=&\int\pi(Z_{0},\theta |\mathbf{Z})d\theta\\
&=&\int\pi(Z_{0}|\mathbf{Z}, \theta )\cdot\pi(\theta)d\theta\\
\end{eqnarray*}

이다. 그런데 여기서는 explicit form이 잘 안나와 보통 numerical integration을 한다고 한다.

이 방법의 장점은 앞선 두 방법들과 달리 $\Sigma$, $\sigma_{0}^{2}$을 알 필요가 없다는 것이다. 라그랑지 승수법과 조건부 분포를 이용한 방법은 $\hat{\Sigma}$, $\hat{\sigma}_{0}^{2}$를 통해 uncertainty가 늘어나는데 이 방법은 그렇지 않다.

그리고 $\hat{\phi}=(\hat{\boldsymbol{\beta}}, \hat{\sigma}^{2}, \hat{theta})$로 parameterization했을 때에도 unbiased한지 optimal한지 체크해야 하는데, 어떠한 조건이 주어질 경우 된다고 한다.

### 덩어리 효과가 있는 모형의 크리깅(Kriging for the model with a nugget effect)

지금까지 크리깅은 nugget 효과가 없다는 가정에서 진행하였다. 그렇다면 nugget이 있는 경우에는 어떻게 되는가? $Z(\mathbf{s})$에 대해 다음과 같은 decomposition을 할 수 있다.
$$Z(\mathbf{s})=x(\mathbf{s})^{T}\boldsymbol{\beta}+\eta(\mathbf{s})+\epsilon(\mathbf{s}).$$
이것에 대한 자세한 내용은 Cressie 책 112쪽을 참고하기 바란다. 그러면 $\boldsymbol{\Sigma}$가 $\boldsymbol{\Gamma}$로 대체된다.
$$\boldsymbol{\Gamma}=\boldsymbol{\Sigma}+C_{0}\mathbf{I}$$
여기서 $\boldsymbol{\Sigma}$는 $\eta(\mathbf{s})$의 covariance structure, $C_{0}\mathbf{I}$는 $\epsilon(\mathbf{s})$에 대한 covariance structure이다.

어쨌든 결론은 nugget이 커지면 prediction 결과도 spread out (퍼짐)한다는 것이다.

## 크리깅의 예측오차(prediction error in Kriging)

크리깅에 대해서 다시 살펴보면, 관측하지 않은 $\mathbf{s}_{0}$지점을
$$\hat{Z}_{0}\equiv \hat{Z}(S_{0})$$
으로 예측하는 것이다. 그러나 실제로 $\hat{Z}_{0}=\hat{Z}_{0}(\boldsymbol{\psi})$, $\boldsymbol{\psi}$는 covariance parameter들의 set인데 이 parameter들을 모르므로 $\hat{Z}_{0}(\hat{\boldsymbol{\psi}})$ (이것을 추정하는 것, OLS, WLS, MLE, REML, empirically 하는 것이 저번시간에 했던 내용들)로 해야한다. 즉 $\boldsymbol{\psi} \rightarrow \hat{\boldsymbol{\psi}}$로 할 때 추가 error가 발생하는 것이다. 다시 말하면 $\hat{Z}_{0}(\hat{\boldsymbol{\psi}})$는 MSPE를 overestimate하는 것이다.

그래서 다음과 같은 correction을 한다.
$$
\begin{aligned}
\hat{m}(\boldsymbol{\psi})=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0})^{2}&=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi})+Z_{0}(\boldsymbol{\psi})-Z_{0})^2\\
&=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi}))^{2}+E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2\\
&\geq E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2
\end{aligned}
$$
여기서 $E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi}))^{2}=m'(\boldsymbol{\psi})$라 하고 $E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2=m(\boldsymbol{\psi})$라 하면 $\hat{m}(\boldsymbol{\psi})$에서 $m'(\boldsymbol{\psi})$를 빼내어 $m(\boldsymbol{\psi})$에 가깝게 만든다고 한다(correction).

사실 application이 굉장히 많고 공간통계학 뿐만 아니라 random effect prediction에서도 생기는 문제라고 한다. 그런데 공간통계학자들은 이 예측오차를 무시하고 그냥 할 때가 많고 random effect쪽은 자료가 독립이라 간단하게 된다고 한다. 이 문제를 깊게 다루는 분야는 **small area estimation**이니 관심있으면 찾아보는 것이 좋다.

또한 Bayesian 입장에서는 $\boldsymbol{\psi}$를 integrated out하므로 $\pi(Z_{0]|\mathbf{Z}})$하는 것이 큰 문제가 되지 않는다. 그러나 numerical integration이 문제가 된다.

## 다른 크리깅들(other Krigings)

Universal kriging 이외에 다른 방법들은 간단히 소개만 하고 넘어갈 것이다.

- **Simple kriging**: $\mathbf{x}_{0}^{T}\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}$
$$\hat{Z}_{0}=\hat{\boldsymbol{\mu}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\hat{\boldsymbol{\mu}}\cdot \mathbf{1}), \hat{\mathbf{\mu}}=\bar{\mathbf{Z}}.$$
여전히 covariance parameter들은 안다고 가정한다. 특히 평균은 알려져 있다고 가정하거나 0이라고 놓는다.

- **Ordinary kriging**: $\mathbf{x}_{0}^{T}\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}$
$$\hat{Z}_{0}=\tilde{\boldsymbol{\mu}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\tilde{\boldsymbol{\mu}}\cdot \mathbf{1}), \tilde{\boldsymbol{\mu}}=(\mathbf{1}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{1})^{-1}\mathbf{1}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}.$$
여기서 $\tilde{\boldsymbol{\mu}}$는 $\boldsymbol{\mu}$의 GLS solution이며 ordinary kriging은 universal kriging의 special case이다.

만약 $\hat{Z}_{0}$를 prediction weight ($\boldsymbol{\psi}$의 함수) $\lambda_{i}$와 $Z(\mathbf{s}_{i})$의 linaer combination으로 생각한다면 $\hat{Z}_{0}=\sum_{i=1}^{n}\lambda_{i}Z(\mathbf{s}_{i})$로 쓸 수 있다. Simple kriging의 경우는

\begin{eqnarray*}
\hat{Z}_{0}&=&(1-\sigma_{i}\lambda_{i})\boldsymbol{\mu}+\boldsymbol{\lambda}^{T}\mathbf{Z} \qquad(\boldsymbol{\lambda}=\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1})\\
&=&(1-\sigma_{i}\lambda_{i})\boldsymbol{\mu}+\sum_{i}\lambda_{i}Z(\mathbf{s}_{i})\\
&=&\text{global mean + data로부터 나오는 mean}\\
\end{eqnarray*}

으로 $\hat{\boldsymbol{\mu}}=\bar{\mathbf{Z}}$로 했기 때문에 global mean이 없어지지 않고 남는다. 한편 ordinary kriging의 경우는 $\sum_{i}\lambda_{i}=1$이라는 제약조건 하에 $\hat{Z}_{0}=\sum_{i}\lambda_{i}Z(\mathbf{s}_{i})$로 표현 가능하다. 참고로 universal kriging인 경우에 constraint는 $\boldsymbol{\lambda}^{T}\mathbf{x}=x_{0}$이라는 제약조건이 있다.

참고로 **크리깅 자체는 stationary 가정이 필요 없다.** 다만 나중에  $\Sigma$ 추정시 문제가 될 수 있다고 한다.

## 추가적인 크리깅들(more Krigings)

- **Co-kriging** (kriging의 multivariate case)

예를 들면 weather station에 있는 기온, 습도, 풍속 데이터 등을 크리깅할 경우가 이에 해당한다. 한 장소에 $p$개의 데이터
$$\mathbf{Z}(\mathbf{s})=(Z_{1}(\mathbf{s}), \ldots, Z_{p}(\mathbf{s}_{n}))$$
이 있다고 하자. 여기서 $Z_{1}(\mathbf{s}_{0})$을 예측하고 싶다고 하자. 추가로 $E(\mathbf{Z}(\mathbf{s}))=\boldsymbol{\mu}=(\mu_{1}, \ldots , \mu_{n})^{T}$,
$$
\text{Cov}(\mathbf{Z}(\mathbf{s}),\mathbf{Z}(\mathbf{t}))=C(\mathbf{s},\mathbf{t})=
\begin{bmatrix}
C_{11}(\mathbf{s},\mathbf{t}) & \ldots & C_{1p}(\mathbf{s},\mathbf{t}) \\
\vdots & \ddots & \vdots\\
C_{p1}(\mathbf{s},\mathbf{t}) & \ldots & C_{pp}(\mathbf{s},\mathbf{t})
\end{bmatrix}
$$
이다$(p\times p$ matrix). Cross-covariance에 대한 모델링이 추가로 필요하다.

다음과 같은 linear unbiased predictior $\hat{Z}_{1}(\mathbf{s}_{0})=\sum_{i}^{n}\sum_{j}^{p}\lambda_{ji}Z_{j}(\mathbf{s}_{i})$를 생각하보자. unbiasedness에 의해 constraint가 나온다.
$$E(\hat{Z}_{1}(\mathbf{s}_{0})-Z_{1}(\mathbf{s}_{0}))=0 \rightarrow \sum_{i}^{n}\lambda_{ji}=1, \sum_{i=1}^{n}\lambda_{ji}=0 \qquad \text{for } j=2,\ldots, p (unbiased constraints).$$

그러면
$$\text{MSPE}=E(Z_{1}(\mathbf{s}_{0})-\sum_{i=1}^{n}\sum_{j=1}^{p}\lambda_{ji}Z_{j}(\mathbf{s}_{i})) \qquad{\text{under some constraints.}}$$
로 크리깅을 할 수 있다. 복잡하지만 라그랑지 승수법으로 계산 가능하다고 한다.

- **Trans-Gaussian Kriging**

이 방법은 자료가 가우시안이 아닐 경우 어떡할 것인가로부터 시작했다. $\mathbf{Z}(\mathbf{s})$가 가우시안이 아니라고 하자. 만약 변환을 통해 만들어진 $\mathbf{Y}(\mathbf{s})$
$$\mathbf{Y}(\mathbf{s})=h(\mathbf{Z}(\mathbf{s}))$$
가 가우시안일 경우 모델링 할 수 있다는 것이다.

여기서 잠시 자료가 가우시안인지 아닌지 어떻게 검정할 것인가라는 문제가 있는데, 자료가 독립일 경우 Q-Q plot 또는 Kolmogorov-Sminov 검정 등을 한다고 한다. 공간통계학자들의 경우 검정보다는 그냥 모델링에 관심을 갖는다. $\mathbf{Z}(\mathbf{s_{0}})$으로부터 $E(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Z})$를 예측하고 싶어한다. 이 때 가우시안 가정은 필요치 않으나 가우시안이 아닐 경우 이 predictor가 optimal이 아닐 수도 있다고 한다.

실제로 $E(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Z})$의 추정량 $\hat{E}(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Y})$을 구할 수 있는 $h$는 몇 개 없다고 한다. 구할 수 있는 가장 대표적인 $h$는 log-normal로, 이 때의 $Z$를 log-normal process라고 부른다.

- **Indicator Kriging**

지금까지는 조건부 기댓값을 예측했지만, 이번에는 확률을 예측하고 싶어한다. 예를 들면, $Z(s_{1}), \ldots , Z(s_{n})$이라는 자료를 사용 가능할 때 $s_{0}$라는 장소의 비가 $c$ 이상 올 확률에 대해 알고 싶어한다고 하자. 즉,
$$P(Z(s_{0})>c |\mathbf{Z})=E(I(Z(s_{0})>c)|\mathbf{Z})=P(Y(s_{0})|\mathbf{Z})$$
에 대해 알고싶어한다.

한 가지 방법은 indicator observation $(0,1)$로 크리깅하는 것이다.
