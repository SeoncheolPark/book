# 공간자료에서의 가능도 기반 방법들 {#spatlikelihood}

앞 장에서는 지구통계 모형의 모수 추정을 적률 추정 또는 최소자승법에 기반한 추정으로 하는 방법들을 고려했다. 이러한 방법들을 흔히 **고전 지구통계학(classical geostatistics)**라고 부른다. 이 장에서는 가능도 기반 방법론들에 대해 살펴본다.  [@Gelfand2010]의 45쪽부터의 내용을 참고하였다.

보통 coordinate를 가지고 least square로 mean trend 제거 후($\hat{\beta}$ 모델링) 잔차를 이용해 variogram을 fitting한다. 그리고 variogram 모델링을 통해 $\hat{\Sigma}$를 구한 후 이것을 $\hat{\beta}=(X^{T}\Sigma^{-1}X)^{-1}X^{T}\Sigma^{-1}Z$에 넣어 $\hat{beta}$를 업데이트 한다. 이것을 가지고 다시 $\hat{\Sigma}$를 업데이트 하는 등 iterative한 방법으로 업데이트를 많이 한다.

그러나 가능도를 사용하면 first-order-structure와 second-order-structure를 동시에 업데이트 할 수 있다고 한다. 그렇지만 우도 방법을 쓰려면 데이터의 분포 가정을 해야 한다. Empirical variogram을 사용할 때에는 데이터에 대한 분포 가정이 필요치 않다.

## 가능도 기반 방법론들(likelihood-based methods)

몇 가지 가정을 먼저 하자. 먼저
$$Z(\mathbf{s})=\mathbf{x}^{T}(\mathbf{s})\boldsymbol{\beta}+\boldsymbol{\epsilon}(\mathbf{s}), \qquad{\boldsymbol{\epsilon}(\mathbf{s}) \sim  \text{GP}(0, C(\cdot, \boldsymbol{\theta}))}$$
와 같이 mean-structure는 선형이라고 가정한다. $\epsilon(\mathbf{s})$는 가우스 과정(Gaussian process)이다.

그리고 $\mathbf{s}_{1}, \cdots , \mathbf{s}_{n}$에서 정의된 $Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n})$에 대해
$$\mathbf{Z}=\mathbf{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon}, \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^{2}V(\boldsymbol{\rho}))$$
라고 가정한다. 여기서 $\boldsymbol{\theta}=(\sigma^{2}, \boldsymbol{\rho})$로 놓는다. $\boldsymbol{\rho}$는 벡터일 수도 있다.

<div class="example">

 Exponential variogram model with nugget인 경우 covariance function은 (일반적인 정의와 다른 것 같으니 체크 필요)
$$
C(\mathbf{h})=
\begin{cases}
c_{0}+\sigma^{2} & \text{if $\mathbf{h}$=0}\\
\sigma^{2}(e^{-\frac{\|\mathbf{h}\|}{R}}) & \text{if $\mathbf{h}\neq 0$}\\
\end{cases}
$$
여기서 $\sigma^{2}$이 $c_{1}$에 해당한다. 이 때 $\boldsymbol{\rho}=(c_{0},R)$이다. Covariance matrix를 계산하면
$$
\begin{bmatrix}
c_{0}+\sigma^{2} &  &  \\
\sigma^{2}e^{-\frac{1}{R}} & \ddots &  \\
 &  & c_{0}+\sigma^{2}\\
\end{bmatrix}
$$
식으로 나오는데, nugget 때문에 nice한 convex가 아는 wiggle한 형태가 된다. 이러한 문제를 해결하기 위해 **재모수화(reparametrization)**를 한다.

</div>

## 재모수화(reparametrization)

위 문제를 해결하기 위해 다음과 같이 재모수화을 한다.
$$\phi=\frac{c_{0}}{c_{0}+\sigma^{2}}: \text{ ratio of nugget to sill (일종의 parameter stabilization)}$$
이것은 parameter들 $\boldsymbol{\theta}$를
$$\boldsymbol{\theta}=(\sigma^{2}, c_{0}, R) \rightarrow (\sigma^{2}, \phi , R)$$
다음과 같이 one-to-one mapping하는 것이다. 

재모수화 후의 공분산 행렬은
$$\sigma^{2}V(\boldsymbol{\theta}) = \sigma^{2} 
\begin{bmatrix}
\frac{1}{1-\rho} &  &  \\
e^{-\frac{d_{ij}}{R}} & \ddots &  \\
 &  & \frac{1}{1-\rho}\\
\end{bmatrix}
$$
와 같이 모델링한다. 여기서 $d_{ij}=\|\mathbf{s}_{i}-\mathbf{s}_{j}\|$이다. $\sigma^{2}$이 밖으로 빠져나와 MLE 계산이 쉽다. Exponential variogram model 뿐만 아니라 다른 모델들도 nugget이 있는 경우 재모수화를 많이 한다.

## 공간자료에서의 최대가능도추정(MLE in spatial data)

MLE 추정 방법의 기본 골자는 다음과 같다.
$$
\begin{aligned}
l(\boldsymbol{\beta},\sigma^{2}, \boldsymbol{\rho})=&\log f(\mathbf{Z};\boldsymbol{\beta},\sigma^{2},\boldsymbol{\rho})\\
&\varpropto -\frac{1}{2}\log |\sigma^{2}V(\boldsymbol{\beta})| -\frac{1}{2\sigma^{2}}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta})^{T}V^{-1}(\boldsymbol{\rho})(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta})\\
\end{aligned}
$$
위 식에서 각 모수들에 대해 미분을 하자.
$$\frac{\partial l}{\partial \boldsymbol{\beta}}=0 \rightarrow \hat{\boldsymbol{\beta}}=(\mathbf{X}^{T}V^{-1}(\boldsymbol{\rho})\mathbf{X})^{-1}\mathbf{X}^{T}V^{-1}(\boldsymbol{\rho})\mathbf{Z}$$
$$\frac{\partial l}{\partial \sigma^{2}}=0 \rightarrow \frac{1}{n}(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}})^{T}V^{-1}(\boldsymbol{\rho})(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}})$$
$$\frac{\partial l}{\partial f}=0$$

또는 **profile likelihood**를 정의하여 문제를 풀 수도 있다. Profile likelihood의 idea를 어떤 parameter를 다른 parameter의 함수로 표현하여 모수를 줄여 푸는 것이다.
$$l*(f)=f(\hat{\beta}(\boldsymbol{\rho}), \hat{\sigma}^{2}(\boldsymbol{\rho}),\boldsymbol{\rho}).$$
Profile likelihood와 그냥 likelihood의 차이는 무엇일까? Likelihood인 경우 joint density, joint probability로 나타낼 수 있지만, profile likelihood인 경우 true likelihood가 아닐 수도 있다. 그 얘기는 즉 joint density, joint probability로 표현하지 못할 수도 있다는 뜻이다. 이 경우 추정에는 문제가 없으나 inference, 특히 test시 문제가 된다.

일반적으로 likelihood 사용시 쓰는 test는 LRT test이다. Profile likelihood를 사용할 경우
$$2(l*(\hat{\boldsymbol{\theta}})-l*(\boldsymbol{\theta_{0}})) \sim \chi^{2}$$
이 식에서 $\chi^{2}$로 점근적으로 가는 속도가 그냥 likelihood를 쓸 때보다 느려진다고 한다. 또한 bias가 생기는 문제도 있다. 이를 해결하기 위해 **Bartlett Correction**이라는 것을 사용한다고 한다. [@BarndorfF-Nielsen1983]이나 [@Cox1987] 등은 modified profile likelihood를 제안하기도 하였다. 더 자세한 내용을 알려면 mixed model thoery나 likelihood inference쪽 reference를 찾아보기 바란다.

## 제한된 최대가능도추정(restricted MLE)

다음과 같은 모델
$$\mathbf{Z}=\mathbf{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon} \rightarrow \mathcal{N}(\mathbf{X}^{T}\boldsymbol{\beta}, \sigma^{2}V(\boldsymbol{\rho}))$$
을 생각해보자. Restricted MLE는 mean part가 0이 되도록 변환을 해 주는 것이다. 즉
$$w=\mathbf{A}^{T}\mathbf{Z}=\mathbf{A}^{T}\mathbf{X}^{T}\boldsymbol{\beta}+\mathbf{A}\boldsymbol{\epsilon} \rightarrow \mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{A}^{T}V(\boldsymbol{\rho})\mathbf{A})$$
이 likelihood를 가지고 estimation을 하는 것이다.

그렇다면 $\mathbf{A}$를 어떻게 찾을 것인가? Harville (1974)는
$$
\left(\begin{array}{l}
\mathbf{A}\mathbf{A}^{T}=\mathbf{I}-\mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\\
\mathbf{A}^{T}\mathbf{A}=\mathbf{I}
\end{array}
\right\} \text{ 이면 } \mathbf{A}^{T}\mathbf{X}^{T}\boldsymbol{\beta}=\mathbf{0}
$$
임을 보였다. 즉 projection matrix의 linear independent한 column을 뽑아내면 된다.

또 다른 방법으로는 QR 분해를 하는 것이 있다.
$$\mathbf{X}_{n\times p}=\mathbf{Q}\mathbf{P}=[\mathbf{Q}_{1}, \mathbf{Q}_{2}]_{n\times n}
\begin{bmatrix}
 &  &  \\
 & \ddots &  \\
 &  & \\
\end{bmatrix}
$$
여기서 $\mathbf{Q}$는 orthogonal matrix, $\mathbf{R}$는 upper triangular matrix이다. 이 때 $\mathbf{Q}_{2}=\mathbf{A}$라고 한다.

RMLE을 쓸 경우 $\sigma^{2}=\frac{1}{n-p}( \cdots )$가 되어 불편추정량이 된다고 한다.

## 공간자료 최대우도추정의 점근성(asymptotics of MLE of spatial data)

공간자료에서는 크게 세 가지 asymptotic framework가 있다.

- **Increasing domain asymptotics**

가장 클래식하고 많이 쓰는 방법이다. 이 방법의 가정은 관찰 location 사이의 minimum distance가 sample size가 $\rightarrow \infty$이더라도 계속 $>0$일 것이라는 가정이다. 즉 이 방법은 sample을 계속 뽑을수록 dist $>0$이어야 하므로 domain도 같이 늘어나야 한다.

이 방법은 time series의 asymptotic과 비슷한다. Daily 시계열 자료를 생각해보면 날짜가 계속 늘어날 수 있을 것이다. 그러나 시계열에서는 한쪽방향으로만 증가할 수 있다. 즉 domain의 direction이 존재한다. 시계열 자료의 asymptotic에 주로 쓰는 마팅게일은 방향이 있는 자료밖에 못쓴다고 한다. 그래서 시계열과 비슷하더라도 같은 asymptotic을 쓰지 못한다.

- **Fixed domain asymptotics (infill asymptotics)**

이 방법은 domain이 무한정 증가하는 게 말이 안된다고 생각해서 나온 것이다. Sample size가 $\rightarrow \infty$일 때 minimum distance는 $\rightarrow 0$이며 domain은 고정되어 있다고 본다. 즉 observation의 갯수가 $n$일 때, sample size 사이의 거리는 $O(\frac{1}{n})$이라고 생각하는 것이다. 이 방법은 time series나 nonparametric setting에서 많이 쓰인다. 그러나 다른 방법들보다 보이는 것이 어려워 잘 쓰이지는 않는다.

- **Mixed domain asymptotics (shrinking asymptotics)**

이 방법은 앞 두 방법을 섞은 것이라고 생각하면 된다. Domain은 늘어나고 sample 사이의 distance는 줄어든다. 즉 observation의 갯수가 $n$일 때, sample size 사이의 거리는 $O(\frac{1}{n^{\alpha}}), 0 <\alpha <1$이라고 생각하는 것이다. 교수님께서는 MLE에서는 보지 못했으나 MLE말고 다른 asymptotic에서는 종종 쓰인다고 하였다.

### 몇 가지 결과들(sum results about asymptotics of MLE of spatial data)

- **Increasing domain asymptotics**하의 결과들

Mardia and Marshall (1984)는 linear regression, dependence가 있는 Gaussian distribution을 따르는 모형에서의 MLE asymptotic (weak consistence, asymptotic normal)을 보였다고 한다. General한 결과는 stationary 가정도 필요없다고 한다. 이들의 결과는 Sweeting (1980)의 general CLT 결과를 이용한 것이다.

Cressie and Lahiri (1993, 1996)은 REML에서의 asymptotics를 보였다고 한다.

- **Fixed domain asymptotics (infill asymptotics)**하의 결과들

이 방법은로는 general한 결과는 없고 매우 specific한 경우의 결과들만 있다.

Ying (1991)은 Gaussian이고 exponential covariance model $(\sigma^{2}e^{-\theta h})$이고 $d=1$인 경우에 $\sigma^{2}$과 $\theta$를 따로 estimate 할 수 없음을 보였다고 한다(MLE 포함 모든 방법이 다 단된다. Increasing domain asymptotics인 경우에는 따로 estimate하는 것이 가능하다고 한다). 그러나 곱한 경우, $\sigma^{2}\theta$는 MLE가 consistent함을 보였다고 한다.

Ying (1993)은 $d \geq 2$인 multiplicative covariance model (두 exponential covariance model을 곱한 것, $d=2$일 때 $\sigma^{2}e^{-\theta_{1} h_{1}-\theta_{2}h_{2}}=\sigma^{2}e^{-\theta_{1}h_{1}}e^{-\theta_{2}h_{2}})$이다. Anisotropic model 중 하나지만 computation이 쉬운 편이다.)로 확장하여 $\sigma^{2}, \theta_{1}, \theta_{2}$를 따로 consistently하게 estimate할 수 있음을 보였다고 한다. 참고로 multiplicative covariance model은 공학에서 Gaussian process의 covariance model로 많이 쓴다고 한다.

Zhang (2004)는 $d \leq 3$인 Matern covariance model에 대하여 $(\sigma^{2}, \phi, \alpha)$가 따로 consistently estimated 될 수 없음을 보였다고 한다. 다만 가능한 경우는 $\alpha$의 경우 true를 안다고 가정하고 $\phi$는 true를 모르지만 $\phi=\phi_{1}$와 같이 고정할 경우
$$\hat{\sigma}^{2}\phi_{1}^{2\alpha} \stackrel{a.s.}{\rightarrow} \sigma^{2}\phi^{2\alpha}$$
가 된다고 한다(consistency만 되고 asymptotic normality는 안된다). 

Du et al. (2009) 는 Gaussian distribution, Matern covariance, $d=1$인 경우에 (maybe consistency와) asymptotic normality를 보였다. 또한 MLE의 tapered version에 대해서도 asymptotic normality가 성립함을 보였다. Tapering에 대해서는 뒤에 다시 설명하겠다.

기타 Loh (2005, 2011), Kaufman et al (2008) 등이 있다.

이제 다음과 같은 질문을 해 볼 수 있다. Increasing domain asymptotics와 fixed domain asymptotics (infill asymptotics) 중에 어떤 방법이 더 better한 방법인가? 그리고 왜 increasing domain asymptotics는 잘 되는데 fixed domain asymptotics는 어려울까?

우선 뒤의 질문부터 답하면 모수가 microergodic parameter일 경우 추정이 잘 되나 그렇지 않으면 잘 안된다고 한다. 이것은 spatial prediction part에서 다시 설명할 것이다.

첫번째 질문에 대한 답은 Zhang and Zimmerman (2005)가 Guassian distribution을 따르는 MLE 추정에 대해서 해 본적이 있다. 방법은 asymptotic distribution 중 어떤 것이 finite sample distribution (Monte-carlo로 계산)에 가까운가이다. Exponential covariance model인 경우 $(\sigma^{2}e^{-\theta h})$ parameter가 두 framework에서 다 잘 estimated될 경우에는 둘 다 performance가 좋았으나 parameter가 not estimable할 경우에는 infill asymptotics 근사 결과가 좀 더 좋았다고 한다.

## 공간통계에서의 계산 문제들(computational issues in spatial statistics)

공간통계에서는 주로 $|\Sigma|$ (determinant)와 $\Sigma^{-1} \sim O(n^{3})$, (symmetric이고 positive definite한 경우에는 order를 낮출 수 있다고 한다)을 계산해야 하는 경우가 많다. 그런데 $|\Sigma|$나 $\Sigma^{-1}$이 block diagonal이나 nice한 모양이 아니므로 computation이 문제되는 경우에 있다. 이는 데이터가 커지고 있는 최근에 더 문제가 된다.

MLE에서의 computational issue들은 다음과 같은 것들이 있다.

- Inverse, determinant 계산 문제

- Parameter가 많아 likelihood surface가 smooth (nice)하지 않아 어려운 경우 (보통 re-parametrization을 쓸 수 있다고 한다)

최근에는 아예 MLE가 아닌 다른 inverse-free approach를 찾는 연구 또한 활발하다고 한다.

### 공간통계에서의 계산 문제들의 해결책들(solutions about computational issues in spatial statistics)

- **Classical**

Cholesky decomposition
$$\Sigma=LL^{T}, \text{ L은 lower triangular matrix}$$
를 쓰는 방법이 있다. Lower triangular matrix로 바꿀 경우 linear하게 inverse를 계산할 수 있어 앞서 말했던 계산의 order가 $O(n)$으로 줄어드는 효과가 있다고 한다. 그리고
$$|\Sigma | = \prod_{i=1}^{n}l_{ii}^{2}$$
이 된다고 한다. 여기서 $l_{ii}$는 diagonal entry이다.

- **Tapering**

**Tapering**의 의미는 '잘라내다'라는 뜻으로, 공대나 time series 등에서 많이 사용된다고 한다. Tapering을 지칭하는 경우는 data에 tapering을 하는 경우와 covariance에 tapering을 하는 경우 두 가지가 있는데, 여기서는 covariance matrix에 tapering을 하는 것을 의미한다.

$C_{T}(h)$가 compact support (compact support가 아니면 곱해줘도 0이 안되어 computational gain이 없다)를 갖는 isotropic correlation function이라고 하자. Tapered covariance function $\tilde{C}(h)$는
$$\tilde{C}(h)=C(h)\cdot C_{T}(h)$$
로 정의된다. 여기서 곱은 elementwise product (Hadamard, Schur) 인 것 같다(전미경 교수님 노트 참고).

Tapered covariance matrix는 block-diagonal 형태가 되며 이런 행렬의 경우 inverse나 determinant를 더 빨리 계산하는 알고리즘들이 이미 있다고 한다. 물론 $\tilde{C}(h)$ 또한 nonnegative definite covariance인지와 같은 thoery를 justify해야 한다. 그러나 거의 일반적으로 tapering해도 nonnegative definite covariance가 그대로 유지된다고 한다. 한편 tapering을 쓰면 surface가 original보다 좀 더 wiggle해짐이 알려져 있다(sol 찾을 시 조심해야).

## 근사 가능도(approximate Likelihood)

마지막으로 기타 다른 방법들에 대해서 소개한다.

Vecchina (1988)은 approximate likelihood라는 것을 소개했다. $\beta$를 회귀모수, $\theta$를 covariance parameter, $\mathbf{z}=(z_{1}, \cdots , z_{n})$를 data라고 하자. 그러면 원래 likelihood를 conditional density argument를 통해 decompose할 수 있다는 아이디어에서 비롯되었다.
$$
\begin{eqnarray*}
L(\beta, \sigma, \mathbf{z}) &=& f(\mathbf{z}, \beta, \sigma)\\
&=&f(\mathbf{z}, \beta, \sigma) \prod_{i=2}^{n}f(z_{i}|z_{j}, 1 \leq j \leq i-1, \beta, \theta)\\
\end{eqnarray*}
$$

그 후에 적당한 subset $z_{im}$을 잡는 것이다. 즉
$$
\begin{eqnarray*}
P(X,Y)&=&P(X)P(Y|X)\\
&\approx& f(\mathbf{z},\beta, \theta)\prod_{i=2}^{n}f(z_{i}|z_{j}; z_{im}\text{은 subset of }z_{1}, \cdots ,z_{i-1})\\
\end{eqnarray*}
$$
그렇다면 얼마나 자를 것인가? 가까운 애들만 남기면 된다. Empirically, $n=100$일 때 $m \approx 10$ 정도를 쓴다고 한다. Vecchina는 theory를 안했다고 한다. 물론 80년대라서 이럴 수 있었다고 한다.

Stein et al (2004)는 Vecchina의 접근을 REML에서 시도하였다. 특히 subset을 찾기 위한 몇 가지 시도들을 했고, 근사값을 계산하기 위한 방법을 제공했다. 또한 강한 spatial dependence가 있는 경우, 멀리 있는 observation 또한 고려할 필요가 있음을 보였다.

## 유사가능도와 복합가능도(pseudo-Likelihood and composite Likelihood)

**Pseudo-likelihood**의 기본적인 아이디어는 dependent data를 마치 independent data처럼 쪼개서 생각한다는 것이다. Lattice model의 conditional AR model에서도 다시 등장한다. Pseudo-likelihood는 Besag (1975)에 의해 처음 소개되었다. 그리고 Gurriero and Lele (1999)sms variogram에 대해 pseudo-likelihood를 사용하였다.

$\gamma(h;\sigma)$를 semivariogram이라고 하자. 그리고
$$v_{ij}\equiv Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})$$
라고 하자. $Z$는 instrinsic stationary이며 normal로 assume한다. 여기서 $\mathbf{s}_{i}-\mathbf{s}_{j}=\mathbf{h}$이다. 그러면
$$v_{ij} \sim \mathcal{N}(0, 2\gamma(\mathbf{h};\theta))$$
이다. 만약 다른 index $k$가 존재해 $\mathbf{s}_{i}-\mathbf{s}_{k}=\mathbf{h}$를 만족한다면 $v_{ij}$와 $v_{ik}$는 dependent하다. 그러나 pseudo approach에서는 independent인 것처럼 모델링한다(Handbook of sptaial statistics 52쪽).
$$f(v_{ij})=\frac{1}{2\pi\sqrt{2\gamma(\mathbf{h};\theta)}}\rho^{-\frac{1}{2}v_{ij}^{2}/2\gamma(\mathbf{h};\theta)]}$$
이며 log-likelihood는
$$\text{CL}(\theta)=-\frac{1}{2}\sum_{i=1}^{n}\sum_{j>i}\{ \frac{(Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j}))^{2}}{2\gamma(\mathbf{s}_{i}-\mathbf{s}_{j};\theta)}+\log \gamma(\mathbf{s}_{i}-\mathbf{s}_{j};\theta) \}$$
이다(수식 찾아서 확인해 볼 필요 있음).

이것의 장점은 matrix inverse나 determinant 계산이 필요가 없고 consistent estimator를 준다는 것이다. 그리고 MLE는 원래 분포가정과 실제 분포가 다를 때 misspecification이 일어나는데 여기서는 joint/bivariate distribution assumption을 안했으므로 이것에 대해 robust하다는 것 또한 장점이다. 그러나 dependency를 고려하지 않기 때문에 efficiency가 떨어진다.

비슷한 개념으로 quasi-likelihood가 있는데, GLM이나 GLMM에서 등장한다(찾아보기). Pseudo-likelihood는 generalized mixed effect model에 등장한다.

이것으로 지금까지 geostatstics에서 nonparametric/parameteric/MLE 추정에 대해 간략히 살펴보았다.
