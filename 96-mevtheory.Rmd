# 다변량 극단값 이론 {#mevtheory}

이 부분은 [@Dey2015]의 2장을 참고하였다. 많은 경우 marginal GEV distribution과 코퓰라를 혼합해 사용하는데, 이런 경우 extreme-value copula가 아닌 이상 다변량 극단값 분포가 marginal GEV distribution으로 잘 표현되지 않는다. 이 장에서는 다변량 극단값 분포, 극단값 코퓰라, extremal dependence의 측정, 다변량 일반화 파레토 분포, 준모수 조건부 상관성 모형들 그리고 그것들의 통계적 추론에 대해서 다룬다.

## 극한의 정규화(limiting standardization)

$d$-차원 **다변량 극단값 분포(multivariate extreme value distribution, MEVD)**은 d-차원 다변량 분포로부터 무작위로 생성된 값들의 componentwise maxima의 극한 분포로부터 유래한다. $\{\mathbf{X}_{i} = (X_{i,1},\ldots, X_{i,d}):i=1,\ldots, n\}$은 joint distribution $F$, marginal distribution $F_{1}, \ldots, F_{d}$ 그리고 코퓰라 $C_{F}$를 갖는 $d$-차원 무작위벡터로부터 생성된 무작위 표본이라고 하자. 각각의 margin $j=1,\ldots, d$에 대해 적절한 표준화수열(normalizing sequence) $\mathbf{a}_{n}=(a_{n,1},\ldots, a_{n,d})>0$과 $\mathbf{b}_{n}=(b_{n,1},\ldots, b_{n,d})$가 존재해 limiting marginal distribution
$$Z_{n,j} = \frac{M_{n,j}-b_{n,j}}{a_{n,j}}$$
$n\rightarrow\infty$일 때 non-degenerate generalized extreme value distribution을 갖는다고 정의한다. 여기서 우리들의 관심사는 $\mathbf{Z}_{n}=(Z_{n,1},\ldots, Z_{n,d})$의 limiting joint distribution인 $G$ (즉 MEVD)이다. 특별히, $\mathbf{z}\in\mathbb{R}^{d}$에 대해
$$P(\mathbf{Z}_{n}\leq z)=F^{n}(\mathbf{az} +\mathbf{b}) \rightarrow G(\mathbf{z})$$
이다. 여기서 벡터 계산들은 모두 componentwise하다. 이때 분포 $F$는 $G$의 domain of attraction이라고 부르며 $F\in\mathcal{D}(G)$라고 적는다.

MEVD들은 **최대안정성(max-stability)**라는 것으로 특성화되어진다. $d$차원 분포함수 $G$는 모든 정수 $k\geq 1$에서 벡터 $\boldsymbol{\alpha}_{k}>0$와 $\boldsymbol{\beta}_{k}$가 존재해
\begin{equation}
G^{k}(\boldsymbol{\alpha}_{k}\mathbf{z}+\boldsymbol{\beta}_{k}) = G(\mathbf{z}), \qquad{\forall \mathbf{z}\in\mathbb{R}^{d}}
(\#eq:MEVDmaxstable)
\end{equation}
를 만족할 때 최대안정하다라고 말한다. 최대안정 분포 $G$는 그것만의 domain of attraction을 갖는데 그것이 MEVD이다. 또한 역도 성립한다. 

[@Dey2015]의 16장에 보충설명이 있다. $G$는 $a>0$일 때 $G^{a}$가 분포가 되도록 하며 이러한 성질을 만족하는 분포들의 클래스를 **최대-무한분해가능(max-infnitely divisible, max-id)**라고 부른다. 종 더 구체적으로 $\mathbb{R}^{d}$에서의 분포 $G$가 max-id라면, 어떤 $n\in\mathbb{N}$에 대해 $G=F_{n}^{n}$을 만족하는 분포 $F_{n}$이 존재하게 된다. 이 말은 $G$는 $n$개의 i.i.d. random vector들의 sample maxima들의 분포를 통해 항상 정의될 수 있다는 의미가 된다.

MEVD의 클래스를 특성화하기 위해서는 두 가지를 특성화하는 것이 필요하다.

1. 주변분포의 형태

2. 의존성 구조의 형태

## 코퓰라(copula)

이변량 또는 다변량 분포를 만들 수 있는 방법으로 **코퓰라(copula)**라는 것이 있다. 이것은 보험학, 금융, 경제 모델링, 생존분석, 극단값, 환경 모델링, 시계열과 생물통계 등에서 최근에 주목을 받고 있다. 이 부분은 [@Mikosch2006]를 참고하였다.

연속 분포 $F$를 갖고 그것의 support에서 역 $F^{-1}$을 갖는 실변수 확률변수 $X$를 생각해보자. 만약 $U$가 $(0,1)$에서 정의된 균등분포일 때 $X\stackrel{d}{=}F^{-1}(U)$와 $F(X)\stackrel{d}{=}U$임을 쉽게 체크할 수 있다.

이제 respective marginal distribution functions $F_{i}$ (assumed to be continous and have inverse on the support of $X_{i}$)를 갖는 $\mathbb{R}^{d}$차원 무작위벡터 $\mathbf{X}=(X_{1},\ldots, X_{d})$를 생각하보자. 1차원일 때와 같은 방법으로 다음의 결과를 얻을 수 있다.
$$
\begin{align*} 
P(X_{1}\leq F_{1}^{-1}(x_{1}),\ldots, X_{d}\leq F_{d}^{-1}(x_{d})) &=  P(F_{1}(X_{1})\leq x_{1}, \ldots, F_{d}(X_{d})\leq x_{d}) \\ 
 &= C(x_{1},\ldots, x_{d}).
\end{align*}
$$
식의 오른쪽에서 $d$차원 분포함수 $C$는 support $[0,1]^{d}$를 갖고 uniform marginal distribution functions를 갖는다. 이 때 이 분포함수 $C$를 벡터 $\mathbf{X}$의 **코퓰라(copula)**라고 부른다. 한편 $\mathbf{X}$의 copula $C$가 주어졌을 때 우리는 분포함수들 $F_{i}$들을 이용해 $\mathbf{X}$의 distribution function을 재구성할 수 있다.
$$
\begin{align*} 
P(X_{1}\leq y_{1}, \ldots, X_{d}\leq y_{d}) &=  P(F_{1}(X_{1})\leq F_{1}(y_{1}), \ldots, F_{d}(X_{d})\leq F_{d}(y_{d})) \\ 
 &= C(F_{1}(y_{1}),\ldots,F_{d}(y_{d})).
\end{align*}
$$
좀 더 넓은 의미로 $[0,1]^{d}$에서 support를 갖고 uniform marginal을 갖는 분포함수들을 코퓰라라고 부른다.

주변 분포 함수들 $F_{i}$에 대한 가정이 주어졌다면, 코퓰라 $C$와 $\mathbf{X}$의 분포 사이에는 일대일 대응관계가 존재한다. 즉, $X_{1},\ldots, X_{d}$의 상관관계 구조는 코퓰라와 주변분포 $F_{i}$로 재구성될 수 있다는 것이다. 그러나 코퓰라 $C$를 갖는 벡터 $\mathbf{X}=(X_{1},\ldots, X_{d})$와 분포함수 $C$를 갖는 $\mathbf{Y}=(F_{1}(X_{1}), \ldots, F_{d}(X_{d}))$의 dependence structure가 같다고 얘기하는 것은 아니다.

예를 들어, 벡터 $\mathbf{X}$의 상관관계가 정의될 경우, 그것은 보통 $\mathbf{Y}$의 상관관계 구조와 다르다. $X_{1},\ldots, X_{d}$가 zero autocorrelation들을 갖는 seond order stationary process에서 얻은 표본들이라고 하자. 이것에 대응되는 $F_{1}(X_{1}), \ldots, F_{d}(X_{d})$들을 매우 correlated되어 있고 따라서 일반적으로 second order stationary가 아닐 것이다. 마찬가지로 spectral measure도 $\mathbf{X}$ with unbounded support에서 make sense한 $\mathbf{Y}$에서는 그렇지 않다.


```{theorem, name="누적 분포 함수와 코퓰라"}
Sklar (1959)의 정리는 코퓰라 이론의 시작점을 제시하였다. [@Yee2015] $F$가 무작위 벡터 $(Y_{1}, Y_{2})$의 CDF이고 marginal이 $F_{1}$, $F_{2}$일 때, 대응되는 copula $C$가 존재해 다음을 만족한다.
$$F(y_{1}, y_{2})=C(F_{1}(y_{1}), F_{2}(y_{2});\boldsymbol{\alpha})=C(u_{1},u_{2};\boldsymbol{\alpha}).$$
모든 $(y_{1}, y_{2})\in\mathbb{R}^{2}$에 대해 $F_{j}$가 연속이라면 $C$는 unique하다고 할 수 있다.

```

우리는 $C$를 dependency parameter $\boldsymbol{\alpha}$를 통해 $F$의 dependence structure 정보를 담고 있는 것으로 볼 수 있다. 보통 $\boldsymbol{\alpha}$는 1차원 또는 2차원이다.

```{example, name="이변량 가우스 코퓰라"}
$\boldsymbol{\alpha}=\rho$일 때 **이변량 가우스 코퓰라(bivariate Gaussian copula)**는
$$C(u_{1}, u_{2}, \rho)=\Phi_{2}(\Phi^{-1}(u_{1}), \Phi^{-1}(u_{2});\rho), -1<\rho < 1, 0\leq u_{j} \leq 1$$
이다.
  
```

[@Yee2015]는 이변량 코퓰라들의 흥미로운 성질들을 적어놓았다.

1. $C(u_{1}, u_{2})=0$ if $u_{1}=0$ and/or $u_{2}=0$

2. $C(u_{1}, 1) =u_{1}$ and $C(1,u_{2})=u_{2}$

3. For $j=1,2$, $C(u_{1}, u_{2})$ is nondecreasing as $u_{j}$ increases, keeping the other agument fixed.

4. $F$로부터 random variates를 생성하기  위한 방법으로 다음의 방법을 사용한다. 그 전에 $u_{j}$가 증가함에 따라 $C(u_{1}, u_{2})$또한 증가한다는 강한 가정이 필요하다.

- $U_{1}$, $U_{2}$를 standard uniform distribution으로부터 독립적으로 만든다.

- $Z_{2}=C_{U_{2}|U_{1}}^{-1}(u_{2})$ where $C_{U_{2}|U_{1}}(u_{2})=\partial C(u_{1}, u_{2})/\partial u_{1}= P(U_{2}\leq u_{2}|U_{1}=u_{1})$ is the conditional for $U_{2}$ given $U_{1}=u_{1}$.

- Then $(U_{1}, Z_{2})\sim F$.

5. 연속 확률 변수 $(Y_{1}, Y_{2})\in\mathbb{R}^{2}$에 대응되는 unique copula는 $Y_{1}$ and/or $Y_{2}$에 continuous monotonic transform을 적용해도 변하지 않는다.

## 극단값 코퓰라(extreme-value copula)

이 부분은 [@Dey2015]의 2장을 참고하였다. MEVD $G$의 상관 구조는 그것의 **코퓰라(copula)** $C_{G}$에 의해 특성화된다. MEVD $G$의 코퓰라를 특별히 **극단값 코퓰라(extreme-value copula)**라고 부른다. 어떤 코퓰라 $C$는 다음과 같은 코퓰라 $C_{F}$가 존재해 $n\rightarrow\infty$일 때
$$C_{F}^{n}(\mathbf{u}^{1/n})\rightarrow C(u_{1},\ldots, u_{d}), \qquad{(u_{1}, \ldots, u_{d})\in [0,1]^{d}}$$
를 만족하면 그것을 극단값 코퓰라라고 부르는 것이다. 이 경우에 $C_{F}$는 $C$의 domain of attraction에 속한다고 말한다.

극단값 코퓰라는 또한 최대안정성에 의해서도 특성화된다. 어떤 코퓰라 $C$는 모든 양의정수 $k\geq 1$에 대해 다음
$$C(\mathbf{u})=C^{k}(\mathbf{u}^{1/k}),\qquad{\mathbf{u}\in[0,1]^{d}}$$
을 만족할 때 최대안정이라고 부른다. 최대안정 코퓰라는 자기 자신의 domain of attraction 안에 있다. 그리고 또 한가지 중요한 성질로 **극단값 코퓰라는 최대안정임과 동치**이다 라는 것이 있다. 정의에 의해 극단값 코퓰라 또는 최대안정 코퓰라의 족(family)는 MEVD의 코퓰라의 집합과 일치한다.

극단값 코퓰라 $C$는 **Pickands의 종속함수(Pickands dependence function)**으로 표현할 수 있다. $\Delta_{d-1}$을 $\mathbb{R}^{d}$에서의 단위 심플렉스로
$$\Delta_{d-1} = \{ (w_{1},\ldots, w_{d}: w_{i}>0, i=1,\ldots, d, \sum_{i=1}^{d}w_{i}=1) \}$$
와 같다.

그러면 극단값 코퓰라 $C$는 다음과 같이 쓸 수 있다.
$$C(u_{1},\ldots, u_{d}) = \exp{\Big\{ \Big( \sum_{j=1}^{d}\log u_{j} \Big)  A \Big( \frac{\log u_{1}}{\sum_{j=1}^{d}\log u_{j}},\ldots,  \frac{\log u_{d}}{\sum_{j=1}^{d}\log u_{j}} \Big) \Big\}}.$$
이 때 $A$는 $\Delta_{d-1}\rightarrow [\frac{1}{d},1]$에서 정의되었으며 convex이고 $\max(\mathbf{w})\leq A(\mathbf{w}) \leq q, \forall \mathbf{w} \in \Delta_{d-1}$인 함수로 **Pickands의 종속함수(Pickands dependence function)**이다. 

Marginal GEV 분포에 대해 가우스 코퓰라를 쓸 수도 있다. 그러나 가우스 코퓰라의 꼬리부분은 점근적 독립(asymptotically independent)이 되며 따라서 어떠한 극단값 의존성도 존재하지 않게 딘다.

## 단위 프례세 마진을 갖는 최대안정분포(max-stable distribution with unit Frechet margin)

MEVD의 표현들은 종종 분포함수로 $G_{1}(z) = \exp(-1/z), z>0$을 갖는 **단위 프례세 마진(unit Frechet margin)**으로 설명하기도 한다. 다변량 분포 $G$는 $G$가 다음을 만족할 때 단위 프례세 마진을 갖는 MEVD가 되는 필요충분조건임이 알려져 있다.
$$G(\mathbf{z})=\exp(-V(\mathbf{z})), \qquad{\mathbf{z}\in[0,\infty)^{d}\backslash \{\mathbf{0}\}},$$
이 때
$$V(\mathbf{z})=\int_{\Delta_{d-1}}\max_{j=1,\ldots,d}\Big(\frac{w_{j}}{z_{j}}\Big) dS(w_{1},\ldots, w_{d})$$
이며 $S$는 $\Delta_{d-1}$에서 정의된 양의 측도이며 $\int_{\Delta_{d-1}}w_{j}dS(w_{1},\ldots, w_{d})=1, j=1,\ldots, d$이다. 이 때 측도 $S$를 **스펙트럼 측도(spectral measure)**라고 부르며 $G$의 dependence structure를 특성화한다.

MEVD $G$의 최대안정성 식 \@ref(eq:MEVDmaxstable)은 $G^{-1/k}$가 모든 양의 정수 $k$에 대해 분포함수가 됨을 의미한다. 그것은 $G$가 **최대-무한분해가능한(max-infinitely divisible)** 분포임을 의미한다. 그러면 $[0,\infty)^{d}\backslash\{\mathbf{0}\}$에서 측도 $\mu$가 존재해 다음을 만족한다.
$$V(\mathbf{z})=\mu((0,\mathbf{z}]^{c}), \qquad{\mathbf{z}\in [0,\infty)^{d}\backslash \{\mathbf{0}\}.}$$
이 때 측도 $\mu$를 **지수측도(exponent measure)**라고 하며 함수 $V$는 **지수함수(exponent function)**이라고 부른다. Extreme-value copula의 최대안정성을 프례세 마진을 갖는 MEVD로 변환시키면 모든 $r>0$에 대해 $G^{r}(r\mathbf{z})=G(\mathbf{z})$을 얻는다. 동등하게, $V(r\cdot)=r^{-1}V(\cdot)$이고 $\mu(r\cdot)=r^{-1}\mu(\cdot)$이 된다. 즉, $V$와 $\mu$는 모두 homogeneous of order $-1$이다.

또 이들과 깊게 연관되어있는 개념으로 **안정 꼬리 의존 함수(stable tail dependence function)** $l:[0,\infty]^{d} \rightarrow [0,\infty)$가 있으며 다음과 같이 정의된다.
$$l(\mathbf{v}) = V(1/\mathbf{v}), \qquad{v\in[0,\infty]^{d}.}$$
안정 꼬리 의존 함수는 다음의 성질들을 만족함이 알려져 있다. ([@Beirlant2004]의 257쪽)

1. $l(r\cdot) = r l(\cdot)$ for $r>0$ **(homogeneous of order 1)**

2. $l(e_{j} ) = 1$ for $j=1,\ldots, d$ with $e_{j}$ being the $j$th unit vector in $\mathbb{R}^{d}$

3. $\max_{i=1}^{d}v_{i} \leq l(\mathbf{v})\leq \sum_{i=1}^{d}v_{i}$ for $\mathbf{v} \in [0,\infty)^{d}$

4. $l$ is convex

위의 성질들 중 첫번째 성질 때문에, 안정 꼬리 의존 함수 $l$은 Pickands의 종속함수 $A$와 관련이 있게 된다. 그리고 그것은 $l$을 unit simplex 위에서 제한시킬 수 있게 한다.
$$l(\mathbf{x}) = \Big( \sum_{i=1}^{d}x_{i} \Big) A(w_{1},\ldots, w_{d}), \qquad{\mathbf{x}\in\mathbb{R}_{+}^{d}},$$
이 때 $w_{j} = \frac{x_{j}}{\sum_{i=1}^{d}x_{i}}$이다.

MEVD에 대한 다양한 모수족들이 사전연구에서 제시되었다. [@Gudendorf2010]에서는 extreme value copula들에 대해 정리하였다. 일반적으로 많이 쓰이는 예들은 다음과 같다.

- Logistic model (and its variants)

- H\''{u}sler-Reiss model

- t-extreme-value copula

## 극단 의존성의 측도들 (measures of extremal dependence)

MEVD의 의존성 구조는 다음

- Spectral measure $S$

- Stable tail dependence function $l$

- Pickands dependence function $A$

들로 특성화된다. 이들은 자료로부터 쉽게 추론해낼 수 없는 복잡한 구조들을 말해준다.

때로 의존성에 대한 단순한 요약통계량이 유용할 때가 있다. 이변량일 때에는 의존 측도들로 **켄달의 타우(Kendall's tau)** 또는 **스피어맨의 로우(Spearman's rho)** 들을 쓴다. 이들을 $A$로 표현하면 다음과 같다.
$$\tau=\int_{0}^{1}\frac{t(1-t)}{A(t)}dA'(t), \qquad{\rho = 12\int_{0}^{1}\{A(t)+1\}^{-2}dt-3.}$$
이들은 두 마진의 overall dependence를 잰다. 그러나 때대로 우리는 꼬리에서의 extreme observation들의 의존성을 재는 측도에 좀 더 관심이 있을 것이다.

## 극단 의존성 모형들(extreme dependence models)

이 부분은 [@Dey2015]의 16장을 참고하였다. 이 분야에서는 두 개의 도전적인 과제들이 있다.

첫 번째는 자료의 복잡성, 즉 자료들이 각기 다른 시간 및 공간에서 관찰된다는 것이다.([@DeHaan2007]의 9장) 따라서 이러한 점들을 고려해서 분석해야 한다.

두 번째는 다변량 극단값들의 의존성 구조가 항상 언급된 모형들에서 잘 잡을 수 있는게 아니라는 것이다. [@Ledford1996]은 어떤 상황들 하에서는 의존성 구조를 **점근 독립성(asymptotic independence)**라는 개념을 통해 잘 묘사할 수 있음을 보였다.

## 극단 의존성의 비모수 추정(nonparametric estimation of extremal dependence)

이 부분은 [@Dey2015]의 17장을 참고하였다. 최근 들어 분포의 중심 뿐 아니라 확률벡터의 꼬리부분의 의존성 구조를 이해하는 데이도 많은 관심이 쏠리고 있다. 극단값 이론은 다변량 분포의 결합 꼬리(joint tail) 모델링 문제를 주변 분포(marginal distribution)과 의존성 구조를 독립적으로 모델링하여 문제를 푼다.

High level에서의 의존성을 추정하는 데에는 **안정 꼬리 의존 함수(stable tail dependence function)**과 **스펙트럼 측도(spectral measure)**를 특별히 많이 쓰게 된다.

### 꼬리 의존성(tail dependence)

$\mathbf{X}=(X_{1},\ldots,X_{d})$를 continuous marginal distribution function $F_{j}(x_{j})=\mathbb{P}(X_{j}\leq x_{j})$를 갖고 joint distribution function $F(\mathbf{x})=\mathbb{P}(X_{1}\leq x_{1}, \ldots, X_{d}\leq x_{d})$를 갖도록 하는 확률 벡터라고 하자. 꼬리 의존성을 공부하기 위해 endpoint $(1,\ldots, 1)$의 주변에서의 결합분포함수 $(F_{1}(X_{1}),\ldots, F_{d}(X_{d}))$에 초점을 맞추도록 한다. 즉 여기서는

$$
\begin{align*}
1-\mathbb{P}[F_{1}(X_{1}) &\leq 1-tx_{1},\ldots, F_{d}(X_{d})\leq 1-tx_{d}]\\
&= \mathbb{P}[F_{1}(X_{1})>1-tx_{1} \text{ or } \ldots \text{ or } F_{d}(X_{d})>1-tx_{d}],
\end{align*}
$$
에 초점을 맞춘다. 이 확률은 적어도 한 개의 변수들이 매우 클 확률을 나타낸다. 이 때 $t>0$은 작고 숫자들 $x_{1},\ldots, x_{d} \in [0,\infty)$은 $d$ 변수들의 upper endpoint들의 상대적 거리를 모수화한다. 위의 확률은 $t\rightarrow 0$으로 됨에 따라 $0$으로 수렴한다.

$$
\begin{align*}
\max (tx_{1},\ldots, tx_{d}) &\leq \mathbb{P}[F_{1}(X_{1})>1-tx_{1} \text{ or } \ldots \text{ or } F_{d}(X_{d})>1-tx_{d}]\\
&\leq tx_{1} +\cdots +tx_{d}.
\end{align*}
$$
**안정 꼬리 의존 함수(stable tail dependence function)** $l:[0,\infty)^{d}\rightarrow [0,\infty)$는 $\mathbf{x}\in[0,\infty)^{d}$인 $\mathbf{x}$에 대해 다음과 같이 정의된다.
$$
l(\mathbf{x}):=\lim_{t\downarrow 0 }t^{-1}\mathbb{P}[F_{1}(X_{1})>1-tx_{1} \text{ or }\ldots \text{ or } F_{d}(X_{d})>1-tx_{d}].
$$

지금까지 살펴본 상황과는 반대로, 우리는 모든 변수들이 동시에 클 확률 또한 생각해 볼 수 있다. **꼬리 코퓰라(tail copula)** $R:[0,\infty)^{d}\rightarrow [0,\infty)$는 다음과 같이 정의된다.
$$
R(\mathbf{x}):=\lim_{t\downarrow 0}t^{-1}\mathbb{P}[F_{1}(X_{1})>1-tx_{1},\ldots, F_{d}(X_{d}>1-tx_{d})].
$$
물론 이러한 극한값이 존재한다는 가정 하에 논리를 전개해 나간다.

그렇다면 안정 꼬리 의존 함수와 꼬리 코퓰라 사이에는 어떤 관계가 있을까? $d=2$인 경우에는 특별히 $R(x,y)=x+y-l(x,y)$의 관계를 갖는다. [@Dey2015]의 그림 17.2에 자세한 비교가 있다. 꼬리 코퓰라는 모든 확률변수들이 동시에 큰 값을 가져야 하기 때문에 안정 꼬리 의존함수보다 작은 확률을 갖는다. 

### 스펙트럼 측도(spectral measure)

### 스펙트럼 측도의 추정(estimating the spectral measure)

### 안정 꼬리 의존 함수의 추정(estimating the stable tail dependence function)

### 점근 독립성(asymptotic independence)
