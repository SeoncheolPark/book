<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>통계공부와 관련된 글들</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다.">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="통계공부와 관련된 글들" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  <meta name="github-repo" content="SeoncheolPark/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="통계공부와 관련된 글들" />
  
  <meta name="twitter:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  

<meta name="author" content="Seoncheol Park">

<meta name="date" content="2016-09-23">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="variogramest.html">
<link rel="next" href="kriging.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">통계공부와 관련된 글들</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 일러두기</a></li>
<li class="part"><span><b>Basic Concepts</b></span></li>
<li class="chapter" data-level="2" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>2</b> 기본적인 수학 개념들</a><ul>
<li class="chapter" data-level="2.1" data-path="math.html"><a href="math.html#sequence--limit"><i class="fa fa-check"></i><b>2.1</b> 수열(sequence)과 수열의 극한(limit)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="math.html"><a href="math.html#supremum-infimum"><i class="fa fa-check"></i><b>2.1.1</b> 상한(supremum)과 하한(infimum)</a></li>
<li class="chapter" data-level="2.1.2" data-path="math.html"><a href="math.html#limit-superior-limit-infimum"><i class="fa fa-check"></i><b>2.1.2</b> 상극한(limit superior)과 하극한(limit infimum)</a></li>
<li class="chapter" data-level="2.1.3" data-path="math.html"><a href="math.html#-sequences-of-real-functions"><i class="fa fa-check"></i><b>2.1.3</b> 실함수의 수열들(sequences of real functions)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="math.html"><a href="math.html#-operators-and-norms"><i class="fa fa-check"></i><b>2.2</b> 연산자들과 노름(operators and norms)</a><ul>
<li class="chapter" data-level="2.2.1" data-path="math.html"><a href="math.html#direct-sum"><i class="fa fa-check"></i><b>2.2.1</b> 직합(direct sum)</a></li>
<li class="chapter" data-level="2.2.2" data-path="math.html"><a href="math.html#-kronecker-product"><i class="fa fa-check"></i><b>2.2.2</b> 크로네커 곱(Kronecker product)</a></li>
<li class="chapter" data-level="2.2.3" data-path="math.html"><a href="math.html#tensor-product"><i class="fa fa-check"></i><b>2.2.3</b> 텐서곱(tensor product)</a></li>
<li class="chapter" data-level="2.2.4" data-path="math.html"><a href="math.html#norm"><i class="fa fa-check"></i><b>2.2.4</b> 노름(norm)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="math.html"><a href="math.html#space"><i class="fa fa-check"></i><b>2.3</b> 공간(space)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="math.html"><a href="math.html#space-r"><i class="fa fa-check"></i><b>2.3.1</b> 실수공간(space R)</a></li>
<li class="chapter" data-level="2.3.2" data-path="math.html"><a href="math.html#vector-space"><i class="fa fa-check"></i><b>2.3.2</b> 벡터공간(vector space)</a></li>
<li class="chapter" data-level="2.3.3" data-path="math.html"><a href="math.html#sobolev-sobolev-space"><i class="fa fa-check"></i><b>2.3.3</b> Sobolev 공간(Sobolev space)</a></li>
<li class="chapter" data-level="2.3.4" data-path="math.html"><a href="math.html#besov-besov-space"><i class="fa fa-check"></i><b>2.3.4</b> Besov 공간(Besov space)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basicprob.html"><a href="basicprob.html"><i class="fa fa-check"></i><b>3</b> 기초 확률론</a><ul>
<li class="chapter" data-level="3.1" data-path="basicprob.html"><a href="basicprob.html#-sample-space-and-events"><i class="fa fa-check"></i><b>3.1</b> 표본공간과 사건(sample space and events)</a></li>
<li class="chapter" data-level="3.2" data-path="basicprob.html"><a href="basicprob.html#-sigma-field"><i class="fa fa-check"></i><b>3.2</b> 시그마-체(sigma-field)</a></li>
<li class="chapter" data-level="3.3" data-path="basicprob.html"><a href="basicprob.html#generators"><i class="fa fa-check"></i><b>3.3</b> 생성기들(generators)</a></li>
<li class="chapter" data-level="3.4" data-path="basicprob.html"><a href="basicprob.html#probability-space"><i class="fa fa-check"></i><b>3.4</b> 확률공간(probability space)</a></li>
<li class="chapter" data-level="3.5" data-path="basicprob.html"><a href="basicprob.html#--borel-sigma-field"><i class="fa fa-check"></i><b>3.5</b> 보렐 시그마-체(Borel sigma field)</a><ul>
<li class="chapter" data-level="3.5.1" data-path="basicprob.html"><a href="basicprob.html#-------no-uniform-probablity-of-power-set-on-continous-sample-space"><i class="fa fa-check"></i><b>3.5.1</b> 연속 표본공간에서 시그마-체로 멱집합을 쓰지 않는 이유(no uniform probablity of power set on continous sample space)</a></li>
<li class="chapter" data-level="3.5.2" data-path="basicprob.html"><a href="basicprob.html#---borel-sigma-field-on-r"><i class="fa fa-check"></i><b>3.5.2</b> 실수공간에서 보렐 시그마-체(Borel sigma-field on R)</a></li>
<li class="chapter" data-level="3.5.3" data-path="basicprob.html"><a href="basicprob.html#--construction-of-a-probability-measure-on-r"><i class="fa fa-check"></i><b>3.5.3</b> 실수공간에서 확률측도의 구성(construction of a probability measure on R)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="basicprob.html"><a href="basicprob.html#measure"><i class="fa fa-check"></i><b>3.6</b> 측도(measure)</a><ul>
<li class="chapter" data-level="3.6.1" data-path="basicprob.html"><a href="basicprob.html#lebesgue-lebesgue-measure"><i class="fa fa-check"></i><b>3.6.1</b> Lebesgue 측도(Lebesgue measure)</a></li>
<li class="chapter" data-level="3.6.2" data-path="basicprob.html"><a href="basicprob.html#probability-measure"><i class="fa fa-check"></i><b>3.6.2</b> 확률측도(probability measure)</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="basicprob.html"><a href="basicprob.html#random-variable"><i class="fa fa-check"></i><b>3.7</b> 확률변수(random variable)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="convergencerv.html"><a href="convergencerv.html"><i class="fa fa-check"></i><b>4</b> 확률변수의 수렴</a><ul>
<li class="chapter" data-level="4.1" data-path="convergencerv.html"><a href="convergencerv.html#---almost-sure-convergence"><i class="fa fa-check"></i><b>4.1</b> 거의 확실한 확률 수렴(Almost sure convergence)</a></li>
<li class="chapter" data-level="4.2" data-path="convergencerv.html"><a href="convergencerv.html#convergence-in-probability"><i class="fa fa-check"></i><b>4.2</b> 확률수렴(Convergence in probability)</a></li>
<li class="chapter" data-level="4.3" data-path="convergencerv.html"><a href="convergencerv.html#lp-convergence-in-lp"><i class="fa fa-check"></i><b>4.3</b> Lp 수렴(Convergence in Lp)</a></li>
<li class="chapter" data-level="4.4" data-path="convergencerv.html"><a href="convergencerv.html#convergence-in-distribution"><i class="fa fa-check"></i><b>4.4</b> 분포수렴(Convergence in distribution)</a></li>
<li class="chapter" data-level="4.5" data-path="convergencerv.html"><a href="convergencerv.html#--connections-between-modes-of-convergence"><i class="fa fa-check"></i><b>4.5</b> 수렴 사이들의 관계(Connections between modes of convergence)</a></li>
<li class="chapter" data-level="4.6" data-path="convergencerv.html"><a href="convergencerv.html#convergence-of-moments-uniform-integrability"><i class="fa fa-check"></i><b>4.6</b> Convergence of moments: 일양적분가능성(uniform integrability)</a></li>
<li class="chapter" data-level="4.7" data-path="convergencerv.html"><a href="convergencerv.html#big-o-small-o-big-o-and-small-o"><i class="fa fa-check"></i><b>4.7</b> Big O와 small o (big O and small o)</a></li>
<li class="chapter" data-level="4.8" data-path="convergencerv.html"><a href="convergencerv.html#big-op-small-op-big-op-and-small-op"><i class="fa fa-check"></i><b>4.8</b> Big Op와 small op (big Op and small op)</a></li>
</ul></li>
<li class="part"><span><b>Multiscale Methods in Statistics</b></span></li>
<li class="chapter" data-level="5" data-path="multiscale.html"><a href="multiscale.html"><i class="fa fa-check"></i><b>5</b> 다중척도 방법론</a><ul>
<li class="chapter" data-level="5.1" data-path="multiscale.html"><a href="multiscale.html#-multiscale-transform"><i class="fa fa-check"></i><b>5.1</b> 다중척도 변환(multiscale transform)</a></li>
<li class="chapter" data-level="5.2" data-path="multiscale.html"><a href="multiscale.html#inverse"><i class="fa fa-check"></i><b>5.2</b> 역(inverse)</a></li>
<li class="chapter" data-level="5.3" data-path="multiscale.html"><a href="multiscale.html#sparsity"><i class="fa fa-check"></i><b>5.3</b> 희소성(sparsity)</a></li>
<li class="chapter" data-level="5.4" data-path="multiscale.html"><a href="multiscale.html#-filter-in-signal-processing"><i class="fa fa-check"></i><b>5.4</b> 신호처리에서의 필터(filter in signal processing)</a></li>
<li class="chapter" data-level="5.5" data-path="multiscale.html"><a href="multiscale.html#r-r-multiscale"><i class="fa fa-check"></i><b>5.5</b> R 예제(R-multiscale)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="wavelettransform.html"><a href="wavelettransform.html"><i class="fa fa-check"></i><b>6</b> 웨이블릿 변환</a><ul>
<li class="chapter" data-level="6.1" data-path="wavelettransform.html"><a href="wavelettransform.html#-haar--discrete-haar-wavelet-transform"><i class="fa fa-check"></i><b>6.1</b> 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform)</a></li>
<li class="chapter" data-level="6.2" data-path="wavelettransform.html"><a href="wavelettransform.html#scaling-coefficient-translation-coefficient-"><i class="fa fa-check"></i><b>6.2</b> 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념</a></li>
<li class="chapter" data-level="6.3" data-path="wavelettransform.html"><a href="wavelettransform.html#--fine-scale-approximation"><i class="fa fa-check"></i><b>6.3</b> 섬세한 척도 근사(fine-scale approximation)</a></li>
<li class="chapter" data-level="6.4" data-path="wavelettransform.html"><a href="wavelettransform.html#-----computing-coarser-scale-coefficients-from-fine-scale"><i class="fa fa-check"></i><b>6.4</b> 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale)</a></li>
<li class="chapter" data-level="6.5" data-path="wavelettransform.html"><a href="wavelettransform.html#---defference-between-scale-approximations--"><i class="fa fa-check"></i><b>6.5</b> 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름)</a></li>
<li class="chapter" data-level="6.6" data-path="wavelettransform.html"><a href="wavelettransform.html#-types-of-wavelets"><i class="fa fa-check"></i><b>6.6</b> 웨이블릿의 종류들(types of wavelets)</a><ul>
<li class="chapter" data-level="6.6.1" data-path="wavelettransform.html"><a href="wavelettransform.html#haar-haar-wavelet"><i class="fa fa-check"></i><b>6.6.1</b> Haar 웨이블릿(Haar wavelet)</a></li>
<li class="chapter" data-level="6.6.2" data-path="wavelettransform.html"><a href="wavelettransform.html#shannon-shannon-wavelet"><i class="fa fa-check"></i><b>6.6.2</b> Shannon 웨이블릿(Shannon wavelet)</a></li>
<li class="chapter" data-level="6.6.3" data-path="wavelettransform.html"><a href="wavelettransform.html#meyer-meyer-wavelet"><i class="fa fa-check"></i><b>6.6.3</b> Meyer 웨이블릿(Meyer wavelet)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html"><i class="fa fa-check"></i><b>7</b> 웨이블릿 수축</a><ul>
<li class="chapter" data-level="7.1" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#---main-concept-of-wavelet-shrinkage"><i class="fa fa-check"></i><b>7.1</b> 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage)</a></li>
<li class="chapter" data-level="7.2" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#oracle"><i class="fa fa-check"></i><b>7.2</b> 오라클(oracle)</a></li>
<li class="chapter" data-level="7.3" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#-universal-thresholding"><i class="fa fa-check"></i><b>7.3</b> 만능 임계화(universal thresholding)</a></li>
<li class="chapter" data-level="7.4" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#stein---steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>7.4</b> Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE))</a></li>
<li class="chapter" data-level="7.5" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#r-r-waveletshrinkage"><i class="fa fa-check"></i><b>7.5</b> R 예제(R-waveletshrinkage)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html"><i class="fa fa-check"></i><b>8</b> 웨이블릿 수축의 고등 논제들</a><ul>
<li class="chapter" data-level="8.1" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#cross-validation"><i class="fa fa-check"></i><b>8.1</b> 교차타당성(cross-validation)</a></li>
<li class="chapter" data-level="8.2" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#-multiple-testing"><i class="fa fa-check"></i><b>8.2</b> 다중 비교(multiple testing)</a></li>
<li class="chapter" data-level="8.3" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#--bayesian-wavelet-shrinkage"><i class="fa fa-check"></i><b>8.3</b> 베이지안 웨이블릿 축소(Bayesian wavelet shrinkage)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#prior-mixture-of-gaussian"><i class="fa fa-check"></i><b>8.3.1</b> Prior mixture of Gaussian</a></li>
<li class="chapter" data-level="8.3.2" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#prior-mixture-of-point-mass-and-gaussian"><i class="fa fa-check"></i><b>8.3.2</b> Prior mixture of point mass and Gaussian</a></li>
<li class="chapter" data-level="8.3.3" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#mixture-of-point-mass-and-heavy-tail-distribution"><i class="fa fa-check"></i><b>8.3.3</b> Mixture of point mass and heavy-tail distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#--linear-wavelet-smoothing"><i class="fa fa-check"></i><b>8.4</b> 선형 웨이블릿 평활화(linear wavelet smoothing)</a></li>
<li class="chapter" data-level="8.5" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#-block-thresholding"><i class="fa fa-check"></i><b>8.5</b> 블록 임계화(block thresholding)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiscalets.html"><a href="multiscalets.html"><i class="fa fa-check"></i><b>9</b> 다중척도 시계열분석</a><ul>
<li class="chapter" data-level="9.1" data-path="multiscalets.html"><a href="multiscalets.html#--stationary-time-series"><i class="fa fa-check"></i><b>9.1</b> 시계열 자료의 정상성(stationary time series)</a></li>
<li class="chapter" data-level="9.2" data-path="multiscalets.html"><a href="multiscalets.html#-whitening-of-stationary-process"><i class="fa fa-check"></i><b>9.2</b> 정상과정의 백색화(whitening of stationary process)</a></li>
<li class="chapter" data-level="9.3" data-path="multiscalets.html"><a href="multiscalets.html#--spectral-representation-of-stationary-process"><i class="fa fa-check"></i><b>9.3</b> 정상과정의 스펙트럼 표현(spectral representation of stationary process)</a></li>
<li class="chapter" data-level="9.4" data-path="multiscalets.html"><a href="multiscalets.html#----non-decimated-discrete-wavelets"><i class="fa fa-check"></i><b>9.4</b> 압축 표본화되지 않은 이산 웨이블릿(non-decimated discrete wavelets)</a></li>
<li class="chapter" data-level="9.5" data-path="multiscalets.html"><a href="multiscalets.html#---locally-stationary-wavelet-process"><i class="fa fa-check"></i><b>9.5</b> 국소 정상 웨이블릿 과정(locally stationary wavelet process)</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="admultiscale.html"><a href="admultiscale.html"><i class="fa fa-check"></i><b>10</b> 고급 다중척도 방법론</a><ul>
<li class="chapter" data-level="10.1" data-path="admultiscale.html"><a href="admultiscale.html#--second-generation-wavelet-transform"><i class="fa fa-check"></i><b>10.1</b> 2세대 웨이블릿 변환(second-generation wavelet transform)</a></li>
<li class="chapter" data-level="10.2" data-path="admultiscale.html"><a href="admultiscale.html#-lifting-scheme"><i class="fa fa-check"></i><b>10.2</b> 리프팅 스킴(lifting scheme)</a></li>
<li class="chapter" data-level="10.3" data-path="admultiscale.html"><a href="admultiscale.html#---lifting-in-two-dimensions"><i class="fa fa-check"></i><b>10.3</b> 2차원 자료의 리프팅 스킴(lifting in two dimensions)</a></li>
</ul></li>
<li class="part"><span><b>Function Data Analysis</b></span></li>
<li class="chapter" data-level="11" data-path="functionestimation.html"><a href="functionestimation.html"><i class="fa fa-check"></i><b>11</b> 함수추정</a></li>
<li class="part"><span><b>Spatial Statistics</b></span></li>
<li class="chapter" data-level="12" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>12</b> 공간통계학</a><ul>
<li class="chapter" data-level="12.1" data-path="spatial.html"><a href="spatial.html#-classes-of-spatial-data"><i class="fa fa-check"></i><b>12.1</b> 공간자료의 종류(classes of spatial data)</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="spatialprocess.html"><a href="spatialprocess.html"><i class="fa fa-check"></i><b>13</b> 공간과정</a><ul>
<li class="chapter" data-level="13.1" data-path="spatialprocess.html"><a href="spatialprocess.html#-stationary-in-spatial-data"><i class="fa fa-check"></i><b>13.1</b> 공간자료의 정상성(stationary in spatial data)</a><ul>
<li class="chapter" data-level="13.1.1" data-path="spatialprocess.html"><a href="spatialprocess.html#strictly-stationary"><i class="fa fa-check"></i><b>13.1.1</b> 순정상성(strictly stationary)</a></li>
<li class="chapter" data-level="13.1.2" data-path="spatialprocess.html"><a href="spatialprocess.html#second-order-stationary-weakly-stationary"><i class="fa fa-check"></i><b>13.1.2</b> 약정상성(second order stationary, weakly stationary)</a></li>
<li class="chapter" data-level="13.1.3" data-path="spatialprocess.html"><a href="spatialprocess.html#intrinsic-stationary"><i class="fa fa-check"></i><b>13.1.3</b> 내재정상성(intrinsic stationary)</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-stationarity"><i class="fa fa-check"></i><b>13.2</b> 정상성들 사이의 관계(relationship between stationarity)</a><ul>
<li class="chapter" data-level="13.2.1" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-strong-and-weak-stationary"><i class="fa fa-check"></i><b>13.2.1</b> 순정상성과 약정상성간의 관계(relationship between strong and weak stationary)</a></li>
<li class="chapter" data-level="13.2.2" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-weak-and-intrinsic-stationary"><i class="fa fa-check"></i><b>13.2.2</b> 약정상성와 내재정상성간의 관계(relationship between weak and intrinsic stationary)</a></li>
<li class="chapter" data-level="13.2.3" data-path="spatialprocess.html"><a href="spatialprocess.html#----counterexample-of-intrinsic-stationary-but-not-weak-stationary"><i class="fa fa-check"></i><b>13.2.3</b> 내재정상성이나 약정상성이 안 되는 예(counterexample of intrinsic stationary but not weak stationary)</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="spatialprocess.html"><a href="spatialprocess.html#-ergodic-process"><i class="fa fa-check"></i><b>13.3</b> 에르고딕 과정(ergodic process)</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="covfct.html"><a href="covfct.html"><i class="fa fa-check"></i><b>14</b> 공분산함수</a><ul>
<li class="chapter" data-level="14.1" data-path="covfct.html"><a href="covfct.html#--spectral-representation-theorem"><i class="fa fa-check"></i><b>14.1</b> 스펙트럴 표현 정리(spectral representation theorem)</a></li>
<li class="chapter" data-level="14.2" data-path="covfct.html"><a href="covfct.html#--kolmogorovs-existence-theorem"><i class="fa fa-check"></i><b>14.2</b> 콜모고로프 존재 정리(Kolmogorov’s existence theorem)</a></li>
<li class="chapter" data-level="14.3" data-path="covfct.html"><a href="covfct.html#-properties-of-covariance-functions"><i class="fa fa-check"></i><b>14.3</b> 공분산함수의 성질(properties of covariance functions)</a><ul>
<li class="chapter" data-level="14.3.1" data-path="covfct.html"><a href="covfct.html#isotropy"><i class="fa fa-check"></i><b>14.3.1</b> 등방성(isotropy)</a></li>
<li class="chapter" data-level="14.3.2" data-path="covfct.html"><a href="covfct.html#homogeneous"><i class="fa fa-check"></i><b>14.3.2</b> 동질성(homogeneous)</a></li>
<li class="chapter" data-level="14.3.3" data-path="covfct.html"><a href="covfct.html#anisotropy"><i class="fa fa-check"></i><b>14.3.3</b> 이등방성(anisotropy)</a></li>
<li class="chapter" data-level="14.3.4" data-path="covfct.html"><a href="covfct.html#z3--geometric-anisotropy"><i class="fa fa-check"></i><b>14.3.4</b> z3 기하학적 이등방성(Geometric anisotropy)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="covfct.html"><a href="covfct.html#---continuity-and-differentiabiliy-of-spatial-stochastic-process"><i class="fa fa-check"></i><b>14.4</b> 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process</a><ul>
<li class="chapter" data-level="14.4.1" data-path="covfct.html"><a href="covfct.html#path-continuity---path-differentiability"><i class="fa fa-check"></i><b>14.4.1</b> 경로연속(path-continuity) 또는 경로 미분가능성(path-differentiability)</a></li>
<li class="chapter" data-level="14.4.2" data-path="covfct.html"><a href="covfct.html#mean-square-continuity---mean-square-differentiability"><i class="fa fa-check"></i><b>14.4.2</b> 평균제곱연속(mean-square continuity) 또는 평균제곱 미분가능성(mean-square differentiability)</a></li>
<li class="chapter" data-level="14.4.3" data-path="covfct.html"><a href="covfct.html#bartlett-bartletts-theorem"><i class="fa fa-check"></i><b>14.4.3</b> Bartlett의 정리(Bartlett’s theorem)</a></li>
<li class="chapter" data-level="14.4.4" data-path="covfct.html"><a href="covfct.html#kent---kents-sufficient-condition-for-path-continuity-2-d-ver"><i class="fa fa-check"></i><b>14.4.4</b> Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="covmodel.html"><a href="covmodel.html"><i class="fa fa-check"></i><b>15</b> 공분산모형</a><ul>
<li class="chapter" data-level="15.1" data-path="covmodel.html"><a href="covmodel.html#-nugget-effect"><i class="fa fa-check"></i><b>15.1</b> 덩어리 효과(nugget effect)</a></li>
<li class="chapter" data-level="15.2" data-path="covmodel.html"><a href="covmodel.html#--idealized-shape-of-variogram-isotropic-case"><i class="fa fa-check"></i><b>15.2</b> 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case))</a></li>
<li class="chapter" data-level="15.3" data-path="covmodel.html"><a href="covmodel.html#-effective-range"><i class="fa fa-check"></i><b>15.3</b> 유효 범위(effective range)</a></li>
<li class="chapter" data-level="15.4" data-path="covmodel.html"><a href="covmodel.html#----classical-parametric-isotropic-variogram-models"><i class="fa fa-check"></i><b>15.4</b> 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models)</a><ul>
<li class="chapter" data-level="15.4.1" data-path="covmodel.html"><a href="covmodel.html#-----addtional-explanation-for-k_alpha"><i class="fa fa-check"></i><b>15.4.1</b> 변형된 이형 베셀에 대한 보충 설명(addtional explanation for K_alpha)</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="covmodel.html"><a href="covmodel.html#---variograms-in-other-situation"><i class="fa fa-check"></i><b>15.5</b> 기타 다른 상황에서의 변동도들(variograms in other situation)</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="variogramest.html"><a href="variogramest.html"><i class="fa fa-check"></i><b>16</b> 변동도의 추정</a><ul>
<li class="chapter" data-level="16.1" data-path="variogramest.html"><a href="variogramest.html#empirical-variogram"><i class="fa fa-check"></i><b>16.1</b> 경험변동도(empirical variogram)</a></li>
<li class="chapter" data-level="16.2" data-path="variogramest.html"><a href="variogramest.html#----fitting-parametric-models-to-empirical-variogram"><i class="fa fa-check"></i><b>16.2</b> 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram)</a><ul>
<li class="chapter" data-level="16.2.1" data-path="variogramest.html"><a href="variogramest.html#ls-method"><i class="fa fa-check"></i><b>16.2.1</b> LS method</a></li>
<li class="chapter" data-level="16.2.2" data-path="variogramest.html"><a href="variogramest.html#gls-method"><i class="fa fa-check"></i><b>16.2.2</b> GLS method</a></li>
<li class="chapter" data-level="16.2.3" data-path="variogramest.html"><a href="variogramest.html#wls-method"><i class="fa fa-check"></i><b>16.2.3</b> WLS method</a></li>
<li class="chapter" data-level="16.2.4" data-path="variogramest.html"><a href="variogramest.html#-wls-approximated-wls"><i class="fa fa-check"></i><b>16.2.4</b> 근사 WLS (approximated WLS)</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="variogramest.html"><a href="variogramest.html#r-r-variogramest"><i class="fa fa-check"></i><b>16.3</b> R 예제(R-variogramest)</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="spatlikelihood.html"><a href="spatlikelihood.html"><i class="fa fa-check"></i><b>17</b> 공간자료에서의 가능도 기반 방법들</a><ul>
<li class="chapter" data-level="17.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--likelihood-based-methods"><i class="fa fa-check"></i><b>17.1</b> 가능도 기반 방법론들(likelihood-based methods)</a></li>
<li class="chapter" data-level="17.2" data-path="spatlikelihood.html"><a href="spatlikelihood.html#reparametrization"><i class="fa fa-check"></i><b>17.2</b> 재모수화(reparametrization)</a></li>
<li class="chapter" data-level="17.3" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-mle-in-spatial-data"><i class="fa fa-check"></i><b>17.3</b> 공간자료에서의 최대가능도추정(MLE in spatial data)</a></li>
<li class="chapter" data-level="17.4" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-restricted-mle"><i class="fa fa-check"></i><b>17.4</b> 제한된 최대가능도추정(restricted MLE)</a></li>
<li class="chapter" data-level="17.5" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--asymptotics-of-mle-of-spatial-data"><i class="fa fa-check"></i><b>17.5</b> 공간자료 최대우도추정의 점근성(asymptotics of MLE of spatial data)</a><ul>
<li class="chapter" data-level="17.5.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--sum-results-about-asymptotics-of-mle-of-spatial-data"><i class="fa fa-check"></i><b>17.5.1</b> 몇 가지 결과들(sum results about asymptotics of MLE of spatial data)</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--computational-issues-in-spatial-statistics"><i class="fa fa-check"></i><b>17.6</b> 공간통계에서의 계산 문제들(computational issues in spatial statistics)</a><ul>
<li class="chapter" data-level="17.6.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#---solutions-about-computational-issues-in-spatial-statistics"><i class="fa fa-check"></i><b>17.6.1</b> 공간통계에서의 계산 문제들의 해결책들(solutions about computational issues in spatial statistics)</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-approximate-likelihood"><i class="fa fa-check"></i><b>17.7</b> 근사 가능도(approximate Likelihood)</a></li>
<li class="chapter" data-level="17.8" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-pseudo-likelihood-and-composite-likelihood"><i class="fa fa-check"></i><b>17.8</b> 유사가능도와 복합가능도(pseudo-Likelihood and composite Likelihood)</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="kriging.html"><a href="kriging.html"><i class="fa fa-check"></i><b>18</b> 크리깅</a><ul>
<li class="chapter" data-level="18.1" data-path="kriging.html"><a href="kriging.html#-spatial-prediction"><i class="fa fa-check"></i><b>18.1</b> 공간 예측(spatial prediction)</a></li>
<li class="chapter" data-level="18.2" data-path="kriging.html"><a href="kriging.html#-universal-kriging"><i class="fa fa-check"></i><b>18.2</b> 일반 크리깅(universal Kriging)</a><ul>
<li class="chapter" data-level="18.2.1" data-path="kriging.html"><a href="kriging.html#--lagrange-multiplier-approach"><i class="fa fa-check"></i><b>18.2.1</b> 라그랑즈 승수 접근법(Lagrange multiplier approach)</a></li>
<li class="chapter" data-level="18.2.2" data-path="kriging.html"><a href="kriging.html#-conditional-distribution-approach"><i class="fa fa-check"></i><b>18.2.2</b> 조건부분포 방법(conditional distribution approach)</a></li>
<li class="chapter" data-level="18.2.3" data-path="kriging.html"><a href="kriging.html#-bayesian-approach"><i class="fa fa-check"></i><b>18.2.3</b> 베이지안 방법(Bayesian approach)</a></li>
<li class="chapter" data-level="18.2.4" data-path="kriging.html"><a href="kriging.html#----kriging-for-the-model-with-a-nugget-effect"><i class="fa fa-check"></i><b>18.2.4</b> 덩어리 효과가 있는 모형의 크리깅(Kriging for the model with a nugget effect)</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="kriging.html"><a href="kriging.html#-prediction-error-in-kriging"><i class="fa fa-check"></i><b>18.3</b> 크리깅의 예측오차(prediction error in Kriging)</a></li>
<li class="chapter" data-level="18.4" data-path="kriging.html"><a href="kriging.html#-other-krigings"><i class="fa fa-check"></i><b>18.4</b> 다른 크리깅들(other Krigings)</a></li>
<li class="chapter" data-level="18.5" data-path="kriging.html"><a href="kriging.html#-more-krigings"><i class="fa fa-check"></i><b>18.5</b> 추가적인 크리깅들(more Krigings)</a></li>
</ul></li>
<li class="part"><span><b>Spatial Point Processes</b></span></li>
<li class="chapter" data-level="19" data-path="pointpattern.html"><a href="pointpattern.html"><i class="fa fa-check"></i><b>19</b> 공간점과정</a><ul>
<li class="chapter" data-level="19.1" data-path="pointpattern.html"><a href="pointpattern.html#--examples-of-spatial-point-patterns"><i class="fa fa-check"></i><b>19.1</b> 공간점패턴 자료의 예(examples of spatial point patterns)</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="csr.html"><a href="csr.html"><i class="fa fa-check"></i><b>20</b> 완전공간임의성</a><ul>
<li class="chapter" data-level="20.1" data-path="csr.html"><a href="csr.html#------general-monte-carlo-methods-for-csr-test"><i class="fa fa-check"></i><b>20.1</b> 완전공간임의성 검정을 위한 일반적인 몬테카를로 방법 (general Monte Carlo methods for CSR test)</a><ul>
<li class="chapter" data-level="20.1.1" data-path="csr.html"><a href="csr.html#-----mc-tests-using-edf"><i class="fa fa-check"></i><b>20.1.1</b> 경험적 분포함수를 이용한 몬테칼로 검정 방법들(MC tests using EDF)</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="csr.html"><a href="csr.html#--methods-based-on-nearest-neighbor-distance"><i class="fa fa-check"></i><b>20.2</b> 최근접이웃거리 기반 방법들(methods based on nearest neighbor distance)</a><ul>
<li class="chapter" data-level="20.2.1" data-path="csr.html"><a href="csr.html#----mc-tests-based-on-nearest-neighbor-distance"><i class="fa fa-check"></i><b>20.2.1</b> 최근접이웃거리 기반 몬테칼로 검정 방법들(MC tests based on nearest neighbor distance)</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="csr.html"><a href="csr.html#r-r-edf"><i class="fa fa-check"></i><b>20.3</b> R 예제(R-edf)</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="sparsesampling.html"><a href="sparsesampling.html"><i class="fa fa-check"></i><b>21</b> 희박한 샘플링 분석</a><ul>
<li class="chapter" data-level="21.1" data-path="sparsesampling.html"><a href="sparsesampling.html#-----quadrat-counts-for-sparse-sampled-data"><i class="fa fa-check"></i><b>21.1</b> 희박한 샘플링 자료를 위한 정방구역 계산(quadrat counts for sparse sampled data)</a></li>
<li class="chapter" data-level="21.2" data-path="sparsesampling.html"><a href="sparsesampling.html#-----distance-methods-for-sparsely-sampled-data"><i class="fa fa-check"></i><b>21.2</b> 희박한 샘플링 자료를 위한 거리 방법들(distance methods for sparsely sampled data)</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="pointprocess.html"><a href="pointprocess.html"><i class="fa fa-check"></i><b>22</b> 점과정</a><ul>
<li class="chapter" data-level="22.1" data-path="pointprocess.html"><a href="pointprocess.html#-definition-of-point-processes"><i class="fa fa-check"></i><b>22.1</b> 점과정의 정의(definition of point processes)</a><ul>
<li class="chapter" data-level="22.1.1" data-path="pointprocess.html"><a href="pointprocess.html#-marked-point-process"><i class="fa fa-check"></i><b>22.1.1</b> 표시된 점과정(marked point process)</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="pointprocess.html"><a href="pointprocess.html#-poisson-point-process"><i class="fa fa-check"></i><b>22.2</b> 포아송 점과정(Poisson point process)</a></li>
<li class="chapter" data-level="22.3" data-path="pointprocess.html"><a href="pointprocess.html#--properties-of-poisson-point-process"><i class="fa fa-check"></i><b>22.3</b> 포아송 점과정의 성질들(properties of Poisson point process)</a><ul>
<li class="chapter" data-level="22.3.1" data-path="pointprocess.html"><a href="pointprocess.html#-superpositioning-and-thinning"><i class="fa fa-check"></i><b>22.3.1</b> 중첩과 세선화(superpositioning and thinning)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Quantile Regression</b></span></li>
<li class="chapter" data-level="23" data-path="qr.html"><a href="qr.html"><i class="fa fa-check"></i><b>23</b> 분위수 회귀분석</a><ul>
<li class="chapter" data-level="23.1" data-path="qr.html"><a href="qr.html#-quantile"><i class="fa fa-check"></i><b>23.1</b> 분위수 (quantile)</a></li>
<li class="chapter" data-level="23.2" data-path="qr.html"><a href="qr.html#--linear-quantile-regression"><i class="fa fa-check"></i><b>23.2</b> 선형 분위수 회귀분석(linear quantile regression)</a></li>
</ul></li>
<li class="part"><span><b>Extreme Value Statistics</b></span></li>
<li class="chapter" data-level="24" data-path="extremevaluestat.html"><a href="extremevaluestat.html"><i class="fa fa-check"></i><b>24</b> 극단값 통계학</a><ul>
<li class="chapter" data-level="24.1" data-path="extremevaluestat.html"><a href="extremevaluestat.html#----annual-maximum-sea-levels-at-port-pirie-south-australia"><i class="fa fa-check"></i><b>24.1</b> 연 최대 해수면 높이 자료(annual maximum sea levels at Port Pirie, South Australia)</a></li>
<li class="chapter" data-level="24.2" data-path="extremevaluestat.html"><a href="extremevaluestat.html#----danish-reinsurance-claim-dataset"><i class="fa fa-check"></i><b>24.2</b> 코펜하겐 재보험 화재 손실액 자료(Danish reinsurance claim dataset)</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="uGEVtheory.html"><a href="uGEVtheory.html"><i class="fa fa-check"></i><b>25</b> 일변량 극단값 이론</a><ul>
<li class="chapter" data-level="25.1" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--generalized-extreme-value-distribution"><i class="fa fa-check"></i><b>25.1</b> 일반화 극단값 분포(generalized extreme value distribution)</a></li>
<li class="chapter" data-level="25.2" data-path="uGEVtheory.html"><a href="uGEVtheory.html#max-stablity"><i class="fa fa-check"></i><b>25.2</b> 최대안정성(max-stablity)</a></li>
<li class="chapter" data-level="25.3" data-path="uGEVtheory.html"><a href="uGEVtheory.html#-return-level"><i class="fa fa-check"></i><b>25.3</b> 복귀 수준(return level)</a></li>
<li class="chapter" data-level="25.4" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--inference-in-extreme-value-statistics"><i class="fa fa-check"></i><b>25.4</b> 극단값 분포에서의 추론(inference in extreme value statistics)</a></li>
<li class="chapter" data-level="25.5" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--mle-in-extreme-value-statistics"><i class="fa fa-check"></i><b>25.5</b> 극단값 분포에서의 최대가능도추정(mle in extreme value statistics)</a></li>
<li class="chapter" data-level="25.6" data-path="uGEVtheory.html"><a href="uGEVtheory.html#---profile-likelihood-in-extreme-value-statistics"><i class="fa fa-check"></i><b>25.6</b> 극단값 분포에서의 프로파일 가능도(profile likelihood in extreme value statistics)</a></li>
<li class="chapter" data-level="25.7" data-path="uGEVtheory.html"><a href="uGEVtheory.html#r--r-uevtheory"><i class="fa fa-check"></i><b>25.7</b> R-예제 (r-uevtheory)</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="pot.html"><a href="pot.html"><i class="fa fa-check"></i><b>26</b> 분계점 방법들</a><ul>
<li class="chapter" data-level="26.1" data-path="pot.html"><a href="pot.html#--generalized-pareto-distribution"><i class="fa fa-check"></i><b>26.1</b> 일반화 파레토 분포(generalized pareto distribution)</a></li>
<li class="chapter" data-level="26.2" data-path="pot.html"><a href="pot.html#r-r-pot"><i class="fa fa-check"></i><b>26.2</b> R-예제(r-pot)</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="spatextremes.html"><a href="spatextremes.html"><i class="fa fa-check"></i><b>27</b> 공간 극단값 이론과 최대안정과정</a><ul>
<li class="chapter" data-level="27.1" data-path="spatextremes.html"><a href="spatextremes.html#max-stable-process"><i class="fa fa-check"></i><b>27.1</b> 최대안정과정(max-stable process)</a><ul>
<li class="chapter" data-level="27.1.1" data-path="spatextremes.html"><a href="spatextremes.html#smith-smith-model"><i class="fa fa-check"></i><b>27.1.1</b> Smith 모형(Smith model)</a></li>
<li class="chapter" data-level="27.1.2" data-path="spatextremes.html"><a href="spatextremes.html#schlather-schlather-model"><i class="fa fa-check"></i><b>27.1.2</b> Schlather 모형(Schlather model)</a></li>
<li class="chapter" data-level="27.1.3" data-path="spatextremes.html"><a href="spatextremes.html#brown-resnick-brown-resnick-model"><i class="fa fa-check"></i><b>27.1.3</b> Brown-Resnick 모형(Brown-Resnick model)</a></li>
<li class="chapter" data-level="27.1.4" data-path="spatextremes.html"><a href="spatextremes.html#-t-extremal-t-model"><i class="fa fa-check"></i><b>27.1.4</b> 극단-t 모형(extremal-t model)</a></li>
</ul></li>
<li class="chapter" data-level="27.2" data-path="spatextremes.html"><a href="spatextremes.html#--spatial-dependence-of-extremes"><i class="fa fa-check"></i><b>27.2</b> 극단값의 공간 종속성(spatial dependence of extremes)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>28</b> References</a></li>
<li class="divider"></li>
<li><a href="https://seoncheolpark.github.io/" target="blank">Return to Park's Github Page</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">통계공부와 관련된 글들</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="spatlikelihood" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> 공간자료에서의 가능도 기반 방법들</h1>
<p>앞 장에서는 지구통계 모형의 모수 추정을 적률 추정 또는 최소자승법에 기반한 추정으로 하는 방법들을 고려했다. 이러한 방법들을 흔히 <strong>고전 지구통계학(classical geostatistics)</strong>라고 부른다. 이 장에서는 가능도 기반 방법론들에 대해 살펴본다. <span class="citation">(Gelfand et al. <a href="#ref-Gelfand2010">2010</a>)</span>의 45쪽부터의 내용을 참고하였다.</p>
<p>보통 coordinate를 가지고 least square로 mean trend 제거 후(<span class="math inline">\(\hat{\beta}\)</span> 모델링) 잔차를 이용해 variogram을 fitting한다. 그리고 variogram 모델링을 통해 <span class="math inline">\(\hat{\Sigma}\)</span>를 구한 후 이것을 <span class="math inline">\(\hat{\beta}=(X^{T}\Sigma^{-1}X)^{-1}X^{T}\Sigma^{-1}Z\)</span>에 넣어 <span class="math inline">\(\hat{beta}\)</span>를 업데이트 한다. 이것을 가지고 다시 <span class="math inline">\(\hat{\Sigma}\)</span>를 업데이트 하는 등 iterative한 방법으로 업데이트를 많이 한다.</p>
<p>그러나 가능도를 사용하면 first-order-structure와 second-order-structure를 동시에 업데이트 할 수 있다고 한다. 그렇지만 우도 방법을 쓰려면 데이터의 분포 가정을 해야 한다. Empirical variogram을 사용할 때에는 데이터에 대한 분포 가정이 필요치 않다.</p>
<div id="--likelihood-based-methods" class="section level2">
<h2><span class="header-section-number">17.1</span> 가능도 기반 방법론들(likelihood-based methods)</h2>
<p>몇 가지 가정을 먼저 하자. 먼저 <span class="math display">\[Z(\mathbf{s})=\mathbf{x}^{T}(\mathbf{s})\boldsymbol{\beta}+\boldsymbol{\epsilon}(\mathbf{s}), \qquad{\boldsymbol{\epsilon}(\mathbf{s}) \sim  \text{GP}(0, C(\cdot, \boldsymbol{\theta}))}\]</span> 와 같이 mean-structure는 선형이라고 가정한다. <span class="math inline">\(\epsilon(\mathbf{s})\)</span>는 가우스 과정(Gaussian process)이다.</p>
<p>그리고 <span class="math inline">\(\mathbf{s}_{1}, \cdots , \mathbf{s}_{n}\)</span>에서 정의된 <span class="math inline">\(Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n})\)</span>에 대해 <span class="math display">\[\mathbf{Z}=\mathbf{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon}, \boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^{2}V(\boldsymbol{\rho}))\]</span> 라고 가정한다. 여기서 <span class="math inline">\(\boldsymbol{\theta}=(\sigma^{2}, \boldsymbol{\rho})\)</span>로 놓는다. <span class="math inline">\(\boldsymbol{\rho}\)</span>는 벡터일 수도 있다.</p>
<div class="example">
<p>Exponential variogram model with nugget인 경우 covariance function은 (일반적인 정의와 다른 것 같으니 체크 필요) <span class="math display">\[
C(\mathbf{h})=
\begin{cases}
c_{0}+\sigma^{2} &amp; \text{if $\mathbf{h}$=0}\\
\sigma^{2}(e^{-\frac{\|\mathbf{h}\|}{R}}) &amp; \text{if $\mathbf{h}\neq 0$}\\
\end{cases}
\]</span> 여기서 <span class="math inline">\(\sigma^{2}\)</span>이 <span class="math inline">\(c_{1}\)</span>에 해당한다. 이 때 <span class="math inline">\(\boldsymbol{\rho}=(c_{0},R)\)</span>이다. Covariance matrix를 계산하면 <span class="math display">\[
\begin{bmatrix}
c_{0}+\sigma^{2} &amp;  &amp;  \\
\sigma^{2}e^{-\frac{1}{R}} &amp; \ddots &amp;  \\
 &amp;  &amp; c_{0}+\sigma^{2}\\
\end{bmatrix}
\]</span> 식으로 나오는데, nugget 때문에 nice한 convex가 아는 wiggle한 형태가 된다. 이러한 문제를 해결하기 위해 <strong>재모수화(reparametrization)</strong>를 한다.</p>
</div>
</div>
<div id="reparametrization" class="section level2">
<h2><span class="header-section-number">17.2</span> 재모수화(reparametrization)</h2>
<p>위 문제를 해결하기 위해 다음과 같이 재모수화을 한다. <span class="math display">\[\phi=\frac{c_{0}}{c_{0}+\sigma^{2}}: \text{ ratio of nugget to sill (일종의 parameter stabilization)}\]</span> 이것은 parameter들 <span class="math inline">\(\boldsymbol{\theta}\)</span>를 <span class="math display">\[\boldsymbol{\theta}=(\sigma^{2}, c_{0}, R) \rightarrow (\sigma^{2}, \phi , R)\]</span> 다음과 같이 one-to-one mapping하는 것이다.</p>
<p>재모수화 후의 공분산 행렬은 <span class="math display">\[\sigma^{2}V(\boldsymbol{\theta}) = \sigma^{2} 
\begin{bmatrix}
\frac{1}{1-\rho} &amp;  &amp;  \\
e^{-\frac{d_{ij}}{R}} &amp; \ddots &amp;  \\
 &amp;  &amp; \frac{1}{1-\rho}\\
\end{bmatrix}
\]</span> 와 같이 모델링한다. 여기서 <span class="math inline">\(d_{ij}=\|\mathbf{s}_{i}-\mathbf{s}_{j}\|\)</span>이다. <span class="math inline">\(\sigma^{2}\)</span>이 밖으로 빠져나와 MLE 계산이 쉽다. Exponential variogram model 뿐만 아니라 다른 모델들도 nugget이 있는 경우 재모수화를 많이 한다.</p>
</div>
<div id="-mle-in-spatial-data" class="section level2">
<h2><span class="header-section-number">17.3</span> 공간자료에서의 최대가능도추정(MLE in spatial data)</h2>
<p>MLE 추정 방법의 기본 골자는 다음과 같다. <span class="math display">\[
\begin{aligned}
l(\boldsymbol{\beta},\sigma^{2}, \boldsymbol{\rho})=&amp;\log f(\mathbf{Z};\boldsymbol{\beta},\sigma^{2},\boldsymbol{\rho})\\
&amp;\varpropto -\frac{1}{2}\log |\sigma^{2}V(\boldsymbol{\beta})| -\frac{1}{2\sigma^{2}}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta})^{T}V^{-1}(\boldsymbol{\rho})(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta})\\
\end{aligned}
\]</span> 위 식에서 각 모수들에 대해 미분을 하자. <span class="math display">\[\frac{\partial l}{\partial \boldsymbol{\beta}}=0 \rightarrow \hat{\boldsymbol{\beta}}=(\mathbf{X}^{T}V^{-1}(\boldsymbol{\rho})\mathbf{X})^{-1}\mathbf{X}^{T}V^{-1}(\boldsymbol{\rho})\mathbf{Z}\]</span> <span class="math display">\[\frac{\partial l}{\partial \sigma^{2}}=0 \rightarrow \frac{1}{n}(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}})^{T}V^{-1}(\boldsymbol{\rho})(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}})\]</span> <span class="math display">\[\frac{\partial l}{\partial f}=0\]</span></p>
<p>또는 <strong>profile likelihood</strong>를 정의하여 문제를 풀 수도 있다. Profile likelihood의 idea를 어떤 parameter를 다른 parameter의 함수로 표현하여 모수를 줄여 푸는 것이다. <span class="math display">\[l*(f)=f(\hat{\beta}(\boldsymbol{\rho}), \hat{\sigma}^{2}(\boldsymbol{\rho}),\boldsymbol{\rho}).\]</span> Profile likelihood와 그냥 likelihood의 차이는 무엇일까? Likelihood인 경우 joint density, joint probability로 나타낼 수 있지만, profile likelihood인 경우 true likelihood가 아닐 수도 있다. 그 얘기는 즉 joint density, joint probability로 표현하지 못할 수도 있다는 뜻이다. 이 경우 추정에는 문제가 없으나 inference, 특히 test시 문제가 된다.</p>
<p>일반적으로 likelihood 사용시 쓰는 test는 LRT test이다. Profile likelihood를 사용할 경우 <span class="math display">\[2(l*(\hat{\boldsymbol{\theta}})-l*(\boldsymbol{\theta_{0}})) \sim \chi^{2}\]</span> 이 식에서 <span class="math inline">\(\chi^{2}\)</span>로 점근적으로 가는 속도가 그냥 likelihood를 쓸 때보다 느려진다고 한다. 또한 bias가 생기는 문제도 있다. 이를 해결하기 위해 <strong>Bartlett Correction</strong>이라는 것을 사용한다고 한다. <span class="citation">(BarndorfF-Nielsen <a href="#ref-BarndorfF-Nielsen1983">1983</a>)</span>이나 <span class="citation">(Cox and Reid <a href="#ref-Cox1987">1987</a>)</span> 등은 modified profile likelihood를 제안하기도 하였다. 더 자세한 내용을 알려면 mixed model thoery나 likelihood inference쪽 reference를 찾아보기 바란다.</p>
</div>
<div id="-restricted-mle" class="section level2">
<h2><span class="header-section-number">17.4</span> 제한된 최대가능도추정(restricted MLE)</h2>
<p>다음과 같은 모델 <span class="math display">\[\mathbf{Z}=\mathbf{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon} \rightarrow \mathcal{N}(\mathbf{X}^{T}\boldsymbol{\beta}, \sigma^{2}V(\boldsymbol{\rho}))\]</span> 을 생각해보자. Restricted MLE는 mean part가 0이 되도록 변환을 해 주는 것이다. 즉 <span class="math display">\[w=\mathbf{A}^{T}\mathbf{Z}=\mathbf{A}^{T}\mathbf{X}^{T}\boldsymbol{\beta}+\mathbf{A}\boldsymbol{\epsilon} \rightarrow \mathcal{N}(\mathbf{0},\sigma^{2}\mathbf{A}^{T}V(\boldsymbol{\rho})\mathbf{A})\]</span> 이 likelihood를 가지고 estimation을 하는 것이다.</p>
<p>그렇다면 <span class="math inline">\(\mathbf{A}\)</span>를 어떻게 찾을 것인가? Harville (1974)는 <span class="math display">\[
\left(\begin{array}{l}
\mathbf{A}\mathbf{A}^{T}=\mathbf{I}-\mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\\
\mathbf{A}^{T}\mathbf{A}=\mathbf{I}
\end{array}
\right\} \text{ 이면 } \mathbf{A}^{T}\mathbf{X}^{T}\boldsymbol{\beta}=\mathbf{0}
\]</span> 임을 보였다. 즉 projection matrix의 linear independent한 column을 뽑아내면 된다.</p>
<p>또 다른 방법으로는 QR 분해를 하는 것이 있다. <span class="math display">\[\mathbf{X}_{n\times p}=\mathbf{Q}\mathbf{P}=[\mathbf{Q}_{1}, \mathbf{Q}_{2}]_{n\times n}
\begin{bmatrix}
 &amp;  &amp;  \\
 &amp; \ddots &amp;  \\
 &amp;  &amp; \\
\end{bmatrix}
\]</span> 여기서 <span class="math inline">\(\mathbf{Q}\)</span>는 orthogonal matrix, <span class="math inline">\(\mathbf{R}\)</span>는 upper triangular matrix이다. 이 때 <span class="math inline">\(\mathbf{Q}_{2}=\mathbf{A}\)</span>라고 한다.</p>
<p>RMLE을 쓸 경우 <span class="math inline">\(\sigma^{2}=\frac{1}{n-p}( \cdots )\)</span>가 되어 불편추정량이 된다고 한다.</p>
</div>
<div id="--asymptotics-of-mle-of-spatial-data" class="section level2">
<h2><span class="header-section-number">17.5</span> 공간자료 최대우도추정의 점근성(asymptotics of MLE of spatial data)</h2>
<p>공간자료에서는 크게 세 가지 asymptotic framework가 있다.</p>
<ul>
<li><strong>Increasing domain asymptotics</strong></li>
</ul>
<p>가장 클래식하고 많이 쓰는 방법이다. 이 방법의 가정은 관찰 location 사이의 minimum distance가 sample size가 <span class="math inline">\(\rightarrow \infty\)</span>이더라도 계속 <span class="math inline">\(&gt;0\)</span>일 것이라는 가정이다. 즉 이 방법은 sample을 계속 뽑을수록 dist <span class="math inline">\(&gt;0\)</span>이어야 하므로 domain도 같이 늘어나야 한다.</p>
<p>이 방법은 time series의 asymptotic과 비슷한다. Daily 시계열 자료를 생각해보면 날짜가 계속 늘어날 수 있을 것이다. 그러나 시계열에서는 한쪽방향으로만 증가할 수 있다. 즉 domain의 direction이 존재한다. 시계열 자료의 asymptotic에 주로 쓰는 마팅게일은 방향이 있는 자료밖에 못쓴다고 한다. 그래서 시계열과 비슷하더라도 같은 asymptotic을 쓰지 못한다.</p>
<ul>
<li><strong>Fixed domain asymptotics (infill asymptotics)</strong></li>
</ul>
<p>이 방법은 domain이 무한정 증가하는 게 말이 안된다고 생각해서 나온 것이다. Sample size가 <span class="math inline">\(\rightarrow \infty\)</span>일 때 minimum distance는 <span class="math inline">\(\rightarrow 0\)</span>이며 domain은 고정되어 있다고 본다. 즉 observation의 갯수가 <span class="math inline">\(n\)</span>일 때, sample size 사이의 거리는 <span class="math inline">\(O(\frac{1}{n})\)</span>이라고 생각하는 것이다. 이 방법은 time series나 nonparametric setting에서 많이 쓰인다. 그러나 다른 방법들보다 보이는 것이 어려워 잘 쓰이지는 않는다.</p>
<ul>
<li><strong>Mixed domain asymptotics (shrinking asymptotics)</strong></li>
</ul>
<p>이 방법은 앞 두 방법을 섞은 것이라고 생각하면 된다. Domain은 늘어나고 sample 사이의 distance는 줄어든다. 즉 observation의 갯수가 <span class="math inline">\(n\)</span>일 때, sample size 사이의 거리는 <span class="math inline">\(O(\frac{1}{n^{\alpha}}), 0 &lt;\alpha &lt;1\)</span>이라고 생각하는 것이다. 교수님께서는 MLE에서는 보지 못했으나 MLE말고 다른 asymptotic에서는 종종 쓰인다고 하였다.</p>
<div id="--sum-results-about-asymptotics-of-mle-of-spatial-data" class="section level3">
<h3><span class="header-section-number">17.5.1</span> 몇 가지 결과들(sum results about asymptotics of MLE of spatial data)</h3>
<ul>
<li><strong>Increasing domain asymptotics</strong>하의 결과들</li>
</ul>
<p>Mardia and Marshall (1984)는 linear regression, dependence가 있는 Gaussian distribution을 따르는 모형에서의 MLE asymptotic (weak consistence, asymptotic normal)을 보였다고 한다. General한 결과는 stationary 가정도 필요없다고 한다. 이들의 결과는 Sweeting (1980)의 general CLT 결과를 이용한 것이다.</p>
<p>Cressie and Lahiri (1993, 1996)은 REML에서의 asymptotics를 보였다고 한다.</p>
<ul>
<li><strong>Fixed domain asymptotics (infill asymptotics)</strong>하의 결과들</li>
</ul>
<p>이 방법은로는 general한 결과는 없고 매우 specific한 경우의 결과들만 있다.</p>
<p>Ying (1991)은 Gaussian이고 exponential covariance model <span class="math inline">\((\sigma^{2}e^{-\theta h})\)</span>이고 <span class="math inline">\(d=1\)</span>인 경우에 <span class="math inline">\(\sigma^{2}\)</span>과 <span class="math inline">\(\theta\)</span>를 따로 estimate 할 수 없음을 보였다고 한다(MLE 포함 모든 방법이 다 단된다. Increasing domain asymptotics인 경우에는 따로 estimate하는 것이 가능하다고 한다). 그러나 곱한 경우, <span class="math inline">\(\sigma^{2}\theta\)</span>는 MLE가 consistent함을 보였다고 한다.</p>
<p>Ying (1993)은 <span class="math inline">\(d \geq 2\)</span>인 multiplicative covariance model (두 exponential covariance model을 곱한 것, <span class="math inline">\(d=2\)</span>일 때 <span class="math inline">\(\sigma^{2}e^{-\theta_{1} h_{1}-\theta_{2}h_{2}}=\sigma^{2}e^{-\theta_{1}h_{1}}e^{-\theta_{2}h_{2}})\)</span>이다. Anisotropic model 중 하나지만 computation이 쉬운 편이다.)로 확장하여 <span class="math inline">\(\sigma^{2}, \theta_{1}, \theta_{2}\)</span>를 따로 consistently하게 estimate할 수 있음을 보였다고 한다. 참고로 multiplicative covariance model은 공학에서 Gaussian process의 covariance model로 많이 쓴다고 한다.</p>
<p>Zhang (2004)는 <span class="math inline">\(d \leq 3\)</span>인 Matern covariance model에 대하여 <span class="math inline">\((\sigma^{2}, \phi, \alpha)\)</span>가 따로 consistently estimated 될 수 없음을 보였다고 한다. 다만 가능한 경우는 <span class="math inline">\(\alpha\)</span>의 경우 true를 안다고 가정하고 <span class="math inline">\(\phi\)</span>는 true를 모르지만 <span class="math inline">\(\phi=\phi_{1}\)</span>와 같이 고정할 경우 <span class="math display">\[\hat{\sigma}^{2}\phi_{1}^{2\alpha} \stackrel{a.s.}{\rightarrow} \sigma^{2}\phi^{2\alpha}\]</span> 가 된다고 한다(consistency만 되고 asymptotic normality는 안된다).</p>
<p>Du et al. (2009) 는 Gaussian distribution, Matern covariance, <span class="math inline">\(d=1\)</span>인 경우에 (maybe consistency와) asymptotic normality를 보였다. 또한 MLE의 tapered version에 대해서도 asymptotic normality가 성립함을 보였다. Tapering에 대해서는 뒤에 다시 설명하겠다.</p>
<p>기타 Loh (2005, 2011), Kaufman et al (2008) 등이 있다.</p>
<p>이제 다음과 같은 질문을 해 볼 수 있다. Increasing domain asymptotics와 fixed domain asymptotics (infill asymptotics) 중에 어떤 방법이 더 better한 방법인가? 그리고 왜 increasing domain asymptotics는 잘 되는데 fixed domain asymptotics는 어려울까?</p>
<p>우선 뒤의 질문부터 답하면 모수가 microergodic parameter일 경우 추정이 잘 되나 그렇지 않으면 잘 안된다고 한다. 이것은 spatial prediction part에서 다시 설명할 것이다.</p>
<p>첫번째 질문에 대한 답은 Zhang and Zimmerman (2005)가 Guassian distribution을 따르는 MLE 추정에 대해서 해 본적이 있다. 방법은 asymptotic distribution 중 어떤 것이 finite sample distribution (Monte-carlo로 계산)에 가까운가이다. Exponential covariance model인 경우 <span class="math inline">\((\sigma^{2}e^{-\theta h})\)</span> parameter가 두 framework에서 다 잘 estimated될 경우에는 둘 다 performance가 좋았으나 parameter가 not estimable할 경우에는 infill asymptotics 근사 결과가 좀 더 좋았다고 한다.</p>
</div>
</div>
<div id="--computational-issues-in-spatial-statistics" class="section level2">
<h2><span class="header-section-number">17.6</span> 공간통계에서의 계산 문제들(computational issues in spatial statistics)</h2>
<p>공간통계에서는 주로 <span class="math inline">\(|\Sigma|\)</span> (determinant)와 <span class="math inline">\(\Sigma^{-1} \sim O(n^{3})\)</span>, (symmetric이고 positive definite한 경우에는 order를 낮출 수 있다고 한다)을 계산해야 하는 경우가 많다. 그런데 <span class="math inline">\(|\Sigma|\)</span>나 <span class="math inline">\(\Sigma^{-1}\)</span>이 block diagonal이나 nice한 모양이 아니므로 computation이 문제되는 경우에 있다. 이는 데이터가 커지고 있는 최근에 더 문제가 된다.</p>
<p>MLE에서의 computational issue들은 다음과 같은 것들이 있다.</p>
<ul>
<li><p>Inverse, determinant 계산 문제</p></li>
<li><p>Parameter가 많아 likelihood surface가 smooth (nice)하지 않아 어려운 경우 (보통 re-parametrization을 쓸 수 있다고 한다)</p></li>
</ul>
<p>최근에는 아예 MLE가 아닌 다른 inverse-free approach를 찾는 연구 또한 활발하다고 한다.</p>
<div id="---solutions-about-computational-issues-in-spatial-statistics" class="section level3">
<h3><span class="header-section-number">17.6.1</span> 공간통계에서의 계산 문제들의 해결책들(solutions about computational issues in spatial statistics)</h3>
<ul>
<li><strong>Classical</strong></li>
</ul>
<p>Cholesky decomposition <span class="math display">\[\Sigma=LL^{T}, \text{ L은 lower triangular matrix}\]</span> 를 쓰는 방법이 있다. Lower triangular matrix로 바꿀 경우 linear하게 inverse를 계산할 수 있어 앞서 말했던 계산의 order가 <span class="math inline">\(O(n)\)</span>으로 줄어드는 효과가 있다고 한다. 그리고 <span class="math display">\[|\Sigma | = \prod_{i=1}^{n}l_{ii}^{2}\]</span> 이 된다고 한다. 여기서 <span class="math inline">\(l_{ii}\)</span>는 diagonal entry이다.</p>
<ul>
<li><strong>Tapering</strong></li>
</ul>
<p><strong>Tapering</strong>의 의미는 ’잘라내다’라는 뜻으로, 공대나 time series 등에서 많이 사용된다고 한다. Tapering을 지칭하는 경우는 data에 tapering을 하는 경우와 covariance에 tapering을 하는 경우 두 가지가 있는데, 여기서는 covariance matrix에 tapering을 하는 것을 의미한다.</p>
<p><span class="math inline">\(C_{T}(h)\)</span>가 compact support (compact support가 아니면 곱해줘도 0이 안되어 computational gain이 없다)를 갖는 isotropic correlation function이라고 하자. Tapered covariance function <span class="math inline">\(\tilde{C}(h)\)</span>는 <span class="math display">\[\tilde{C}(h)=C(h)\cdot C_{T}(h)\]</span> 로 정의된다. 여기서 곱은 elementwise product (Hadamard, Schur) 인 것 같다(전미경 교수님 노트 참고).</p>
<p>Tapered covariance matrix는 block-diagonal 형태가 되며 이런 행렬의 경우 inverse나 determinant를 더 빨리 계산하는 알고리즘들이 이미 있다고 한다. 물론 <span class="math inline">\(\tilde{C}(h)\)</span> 또한 nonnegative definite covariance인지와 같은 thoery를 justify해야 한다. 그러나 거의 일반적으로 tapering해도 nonnegative definite covariance가 그대로 유지된다고 한다. 한편 tapering을 쓰면 surface가 original보다 좀 더 wiggle해짐이 알려져 있다(sol 찾을 시 조심해야).</p>
</div>
</div>
<div id="-approximate-likelihood" class="section level2">
<h2><span class="header-section-number">17.7</span> 근사 가능도(approximate Likelihood)</h2>
<p>마지막으로 기타 다른 방법들에 대해서 소개한다.</p>
<p>Vecchina (1988)은 approximate likelihood라는 것을 소개했다. <span class="math inline">\(\beta\)</span>를 회귀모수, <span class="math inline">\(\theta\)</span>를 covariance parameter, <span class="math inline">\(\mathbf{z}=(z_{1}, \cdots , z_{n})\)</span>를 data라고 하자. 그러면 원래 likelihood를 conditional density argument를 통해 decompose할 수 있다는 아이디어에서 비롯되었다. <span class="math display">\[
\begin{eqnarray*}
L(\beta, \sigma, \mathbf{z}) &amp;=&amp; f(\mathbf{z}, \beta, \sigma)\\
&amp;=&amp;f(\mathbf{z}, \beta, \sigma) \prod_{i=2}^{n}f(z_{i}|z_{j}, 1 \leq j \leq i-1, \beta, \theta)\\
\end{eqnarray*}
\]</span></p>
<p>그 후에 적당한 subset <span class="math inline">\(z_{im}\)</span>을 잡는 것이다. 즉 <span class="math display">\[
\begin{eqnarray*}
P(X,Y)&amp;=&amp;P(X)P(Y|X)\\
&amp;\approx&amp; f(\mathbf{z},\beta, \theta)\prod_{i=2}^{n}f(z_{i}|z_{j}; z_{im}\text{은 subset of }z_{1}, \cdots ,z_{i-1})\\
\end{eqnarray*}
\]</span> 그렇다면 얼마나 자를 것인가? 가까운 애들만 남기면 된다. Empirically, <span class="math inline">\(n=100\)</span>일 때 <span class="math inline">\(m \approx 10\)</span> 정도를 쓴다고 한다. Vecchina는 theory를 안했다고 한다. 물론 80년대라서 이럴 수 있었다고 한다.</p>
<p>Stein et al (2004)는 Vecchina의 접근을 REML에서 시도하였다. 특히 subset을 찾기 위한 몇 가지 시도들을 했고, 근사값을 계산하기 위한 방법을 제공했다. 또한 강한 spatial dependence가 있는 경우, 멀리 있는 observation 또한 고려할 필요가 있음을 보였다.</p>
</div>
<div id="-pseudo-likelihood-and-composite-likelihood" class="section level2">
<h2><span class="header-section-number">17.8</span> 유사가능도와 복합가능도(pseudo-Likelihood and composite Likelihood)</h2>
<p><strong>Pseudo-likelihood</strong>의 기본적인 아이디어는 dependent data를 마치 independent data처럼 쪼개서 생각한다는 것이다. Lattice model의 conditional AR model에서도 다시 등장한다. Pseudo-likelihood는 Besag (1975)에 의해 처음 소개되었다. 그리고 Gurriero and Lele (1999)sms variogram에 대해 pseudo-likelihood를 사용하였다.</p>
<p><span class="math inline">\(\gamma(h;\sigma)\)</span>를 semivariogram이라고 하자. 그리고 <span class="math display">\[v_{ij}\equiv Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})\]</span> 라고 하자. <span class="math inline">\(Z\)</span>는 instrinsic stationary이며 normal로 assume한다. 여기서 <span class="math inline">\(\mathbf{s}_{i}-\mathbf{s}_{j}=\mathbf{h}\)</span>이다. 그러면 <span class="math display">\[v_{ij} \sim \mathcal{N}(0, 2\gamma(\mathbf{h};\theta))\]</span> 이다. 만약 다른 index <span class="math inline">\(k\)</span>가 존재해 <span class="math inline">\(\mathbf{s}_{i}-\mathbf{s}_{k}=\mathbf{h}\)</span>를 만족한다면 <span class="math inline">\(v_{ij}\)</span>와 <span class="math inline">\(v_{ik}\)</span>는 dependent하다. 그러나 pseudo approach에서는 independent인 것처럼 모델링한다(Handbook of sptaial statistics 52쪽). <span class="math display">\[f(v_{ij})=\frac{1}{2\pi\sqrt{2\gamma(\mathbf{h};\theta)}}\rho^{-\frac{1}{2}v_{ij}^{2}/2\gamma(\mathbf{h};\theta)]}\]</span> 이며 log-likelihood는 <span class="math display">\[\text{CL}(\theta)=-\frac{1}{2}\sum_{i=1}^{n}\sum_{j&gt;i}\{ \frac{(Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j}))^{2}}{2\gamma(\mathbf{s}_{i}-\mathbf{s}_{j};\theta)}+\log \gamma(\mathbf{s}_{i}-\mathbf{s}_{j};\theta) \}\]</span> 이다(수식 찾아서 확인해 볼 필요 있음).</p>
<p>이것의 장점은 matrix inverse나 determinant 계산이 필요가 없고 consistent estimator를 준다는 것이다. 그리고 MLE는 원래 분포가정과 실제 분포가 다를 때 misspecification이 일어나는데 여기서는 joint/bivariate distribution assumption을 안했으므로 이것에 대해 robust하다는 것 또한 장점이다. 그러나 dependency를 고려하지 않기 때문에 efficiency가 떨어진다.</p>
<p>비슷한 개념으로 quasi-likelihood가 있는데, GLM이나 GLMM에서 등장한다(찾아보기). Pseudo-likelihood는 generalized mixed effect model에 등장한다.</p>
<p>이것으로 지금까지 geostatstics에서 nonparametric/parameteric/MLE 추정에 대해 간략히 살펴보았다.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Gelfand2010">
<p>Gelfand, Alan E., Peter Diggle, Peter Guttorp, and Montserrat Fuentes. 2010. <em>Handbook of Spatial Statistics</em>. CRC Press.</p>
</div>
<div id="ref-BarndorfF-Nielsen1983">
<p>BarndorfF-Nielsen. 1983. “On a Formula For the Distribution of the Maximum Likelihood Estimator.” <em>Biometrika</em> 70 (2): 343–65. doi:<a href="https://doi.org/10.1093/biomet/70.2.343">10.1093/biomet/70.2.343</a>.</p>
</div>
<div id="ref-Cox1987">
<p>Cox, D.R., and N. Reid. 1987. “Parameter Orthogonality and Approximate Conditional Inference.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 49 (1): 1–39.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="variogramest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kriging.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://seoncheolpark.github.io/book/_book/36-spatlikelihood.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
