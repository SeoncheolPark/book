<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>통계공부와 관련된 글들</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다.">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="통계공부와 관련된 글들" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  <meta name="github-repo" content="SeoncheolPark/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="통계공부와 관련된 글들" />
  
  <meta name="twitter:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  

<meta name="author" content="Seoncheol Park">

<meta name="date" content="2016-09-19">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="spatlikelihood.html">
<link rel="next" href="pointpattern.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">통계공부와 관련된 글들</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 일러두기</a></li>
<li class="part"><span><b>Basic Concepts</b></span></li>
<li class="chapter" data-level="2" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>2</b> 기본적인 수학 개념들</a><ul>
<li class="chapter" data-level="2.1" data-path="math.html"><a href="math.html#sequence--limit"><i class="fa fa-check"></i><b>2.1</b> 수열(sequence)과 수열의 극한(limit)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="math.html"><a href="math.html#supremum-infimum"><i class="fa fa-check"></i><b>2.1.1</b> 상한(supremum)과 하한(infimum)</a></li>
<li class="chapter" data-level="2.1.2" data-path="math.html"><a href="math.html#limit-superior-limit-infimum"><i class="fa fa-check"></i><b>2.1.2</b> 상극한(limit superior)과 하극한(limit infimum)</a></li>
<li class="chapter" data-level="2.1.3" data-path="math.html"><a href="math.html#-sequences-of-real-functions"><i class="fa fa-check"></i><b>2.1.3</b> 실함수의 수열들(sequences of real functions)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="math.html"><a href="math.html#-operators-and-norms"><i class="fa fa-check"></i><b>2.2</b> 연산자들과 노름(operators and norms)</a><ul>
<li class="chapter" data-level="2.2.1" data-path="math.html"><a href="math.html#direct-sum"><i class="fa fa-check"></i><b>2.2.1</b> 직합(direct sum)</a></li>
<li class="chapter" data-level="2.2.2" data-path="math.html"><a href="math.html#-kronecker-product"><i class="fa fa-check"></i><b>2.2.2</b> 크로네커 곱(Kronecker product)</a></li>
<li class="chapter" data-level="2.2.3" data-path="math.html"><a href="math.html#tensor-product"><i class="fa fa-check"></i><b>2.2.3</b> 텐서곱(tensor product)</a></li>
<li class="chapter" data-level="2.2.4" data-path="math.html"><a href="math.html#norm"><i class="fa fa-check"></i><b>2.2.4</b> 노름(norm)</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="math.html"><a href="math.html#space"><i class="fa fa-check"></i><b>2.3</b> 공간(space)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="math.html"><a href="math.html#vector-space"><i class="fa fa-check"></i><b>2.3.1</b> 벡터공간(vector space)</a></li>
<li class="chapter" data-level="2.3.2" data-path="math.html"><a href="math.html#sobolev-sobolev-space"><i class="fa fa-check"></i><b>2.3.2</b> Sobolev 공간(Sobolev space)</a></li>
<li class="chapter" data-level="2.3.3" data-path="math.html"><a href="math.html#besov-besov-space"><i class="fa fa-check"></i><b>2.3.3</b> Besov 공간(Besov space)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basicprob.html"><a href="basicprob.html"><i class="fa fa-check"></i><b>3</b> 기초 확률론</a><ul>
<li class="chapter" data-level="3.1" data-path="basicprob.html"><a href="basicprob.html#-sample-space-and-events"><i class="fa fa-check"></i><b>3.1</b> 표본공간과 사건(sample space and events)</a></li>
<li class="chapter" data-level="3.2" data-path="basicprob.html"><a href="basicprob.html#-sigma-field"><i class="fa fa-check"></i><b>3.2</b> 시그마-체(sigma-field)</a></li>
<li class="chapter" data-level="3.3" data-path="basicprob.html"><a href="basicprob.html#generators"><i class="fa fa-check"></i><b>3.3</b> 생성기들(generators)</a></li>
<li class="chapter" data-level="3.4" data-path="basicprob.html"><a href="basicprob.html#--borel-sigma-field"><i class="fa fa-check"></i><b>3.4</b> 보렐 시그마-체(Borel sigma field)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="basicprob.html"><a href="basicprob.html#-------no-uniform-probablity-of-power-set-on-continous-sample-space"><i class="fa fa-check"></i><b>3.4.1</b> 연속 표본공간에서 시그마-체로 멱집합을 쓰지 않는 이유(no uniform probablity of power set on continous sample space)</a></li>
<li class="chapter" data-level="3.4.2" data-path="basicprob.html"><a href="basicprob.html#---borel-sigma-field-on-r"><i class="fa fa-check"></i><b>3.4.2</b> 실수공간에서 보렐 시그마-체(Borel sigma-field on R)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="basicprob.html"><a href="basicprob.html#measure"><i class="fa fa-check"></i><b>3.5</b> 측도(measure)</a><ul>
<li class="chapter" data-level="3.5.1" data-path="basicprob.html"><a href="basicprob.html#lebesgue-lebesgue-measure"><i class="fa fa-check"></i><b>3.5.1</b> Lebesgue 측도(Lebesgue measure)</a></li>
<li class="chapter" data-level="3.5.2" data-path="basicprob.html"><a href="basicprob.html#probability-measure"><i class="fa fa-check"></i><b>3.5.2</b> 확률측도(probability measure)</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="basicprob.html"><a href="basicprob.html#random-variable"><i class="fa fa-check"></i><b>3.6</b> 확률변수(random variable)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="convergencerv.html"><a href="convergencerv.html"><i class="fa fa-check"></i><b>4</b> 확률변수의 수렴</a><ul>
<li class="chapter" data-level="4.1" data-path="convergencerv.html"><a href="convergencerv.html#---almost-sure-convergence"><i class="fa fa-check"></i><b>4.1</b> 거의 확실한 확률 수렴(Almost sure convergence)</a></li>
<li class="chapter" data-level="4.2" data-path="convergencerv.html"><a href="convergencerv.html#convergence-in-probability"><i class="fa fa-check"></i><b>4.2</b> 확률수렴(Convergence in probability)</a></li>
<li class="chapter" data-level="4.3" data-path="convergencerv.html"><a href="convergencerv.html#lp-convergence-in-lp"><i class="fa fa-check"></i><b>4.3</b> Lp 수렴(Convergence in Lp)</a></li>
<li class="chapter" data-level="4.4" data-path="convergencerv.html"><a href="convergencerv.html#convergence-in-distribution"><i class="fa fa-check"></i><b>4.4</b> 분포수렴(Convergence in distribution)</a></li>
<li class="chapter" data-level="4.5" data-path="convergencerv.html"><a href="convergencerv.html#--connections-between-modes-of-convergence"><i class="fa fa-check"></i><b>4.5</b> 수렴 사이들의 관계(Connections between modes of convergence)</a></li>
<li class="chapter" data-level="4.6" data-path="convergencerv.html"><a href="convergencerv.html#convergence-of-moments-uniform-integrability"><i class="fa fa-check"></i><b>4.6</b> Convergence of moments: 일양적분가능성(uniform integrability)</a></li>
<li class="chapter" data-level="4.7" data-path="convergencerv.html"><a href="convergencerv.html#big-o-small-o-big-o-and-small-o"><i class="fa fa-check"></i><b>4.7</b> Big O와 small o (big O and small o)</a></li>
<li class="chapter" data-level="4.8" data-path="convergencerv.html"><a href="convergencerv.html#big-op-small-op-big-op-and-small-op"><i class="fa fa-check"></i><b>4.8</b> Big Op와 small op (big Op and small op)</a></li>
</ul></li>
<li class="part"><span><b>Multiscale Methods in Statistics</b></span></li>
<li class="chapter" data-level="5" data-path="multiscale.html"><a href="multiscale.html"><i class="fa fa-check"></i><b>5</b> 다중척도 방법론</a><ul>
<li class="chapter" data-level="5.1" data-path="multiscale.html"><a href="multiscale.html#-multiscale-transform"><i class="fa fa-check"></i><b>5.1</b> 다중척도 변환(multiscale transform)</a></li>
<li class="chapter" data-level="5.2" data-path="multiscale.html"><a href="multiscale.html#inverse"><i class="fa fa-check"></i><b>5.2</b> 역(inverse)</a></li>
<li class="chapter" data-level="5.3" data-path="multiscale.html"><a href="multiscale.html#sparsity"><i class="fa fa-check"></i><b>5.3</b> 희소성(sparsity)</a></li>
<li class="chapter" data-level="5.4" data-path="multiscale.html"><a href="multiscale.html#-filter-in-signal-processing"><i class="fa fa-check"></i><b>5.4</b> 신호처리에서의 필터(filter in signal processing)</a></li>
<li class="chapter" data-level="5.5" data-path="multiscale.html"><a href="multiscale.html#r-r-multiscale"><i class="fa fa-check"></i><b>5.5</b> R 예제(R-multiscale)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="wavelettransform.html"><a href="wavelettransform.html"><i class="fa fa-check"></i><b>6</b> 웨이블릿 변환</a><ul>
<li class="chapter" data-level="6.1" data-path="wavelettransform.html"><a href="wavelettransform.html#-haar--discrete-haar-wavelet-transform"><i class="fa fa-check"></i><b>6.1</b> 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform)</a></li>
<li class="chapter" data-level="6.2" data-path="wavelettransform.html"><a href="wavelettransform.html#scaling-coefficient-translation-coefficient-"><i class="fa fa-check"></i><b>6.2</b> 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념</a></li>
<li class="chapter" data-level="6.3" data-path="wavelettransform.html"><a href="wavelettransform.html#--fine-scale-approximation"><i class="fa fa-check"></i><b>6.3</b> 섬세한 척도 근사(fine-scale approximation)</a></li>
<li class="chapter" data-level="6.4" data-path="wavelettransform.html"><a href="wavelettransform.html#-----computing-coarser-scale-coefficients-from-fine-scale"><i class="fa fa-check"></i><b>6.4</b> 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale)</a></li>
<li class="chapter" data-level="6.5" data-path="wavelettransform.html"><a href="wavelettransform.html#---defference-between-scale-approximations--"><i class="fa fa-check"></i><b>6.5</b> 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름)</a></li>
<li class="chapter" data-level="6.6" data-path="wavelettransform.html"><a href="wavelettransform.html#-types-of-wavelets"><i class="fa fa-check"></i><b>6.6</b> 웨이블릿의 종류들(types of wavelets)</a><ul>
<li class="chapter" data-level="6.6.1" data-path="wavelettransform.html"><a href="wavelettransform.html#haar-haar-wavelet"><i class="fa fa-check"></i><b>6.6.1</b> Haar 웨이블릿(Haar wavelet)</a></li>
<li class="chapter" data-level="6.6.2" data-path="wavelettransform.html"><a href="wavelettransform.html#shannon-shannon-wavelet"><i class="fa fa-check"></i><b>6.6.2</b> Shannon 웨이블릿(Shannon wavelet)</a></li>
<li class="chapter" data-level="6.6.3" data-path="wavelettransform.html"><a href="wavelettransform.html#meyer-meyer-wavelet"><i class="fa fa-check"></i><b>6.6.3</b> Meyer 웨이블릿(Meyer wavelet)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html"><i class="fa fa-check"></i><b>7</b> 웨이블릿 수축</a><ul>
<li class="chapter" data-level="7.1" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#---main-concept-of-wavelet-shrinkage"><i class="fa fa-check"></i><b>7.1</b> 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage)</a></li>
<li class="chapter" data-level="7.2" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#oracle"><i class="fa fa-check"></i><b>7.2</b> 오라클(oracle)</a></li>
<li class="chapter" data-level="7.3" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#-universal-thresholding"><i class="fa fa-check"></i><b>7.3</b> 만능 임계화(universal thresholding)</a></li>
<li class="chapter" data-level="7.4" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#stein---steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>7.4</b> Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE))</a></li>
<li class="chapter" data-level="7.5" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#r-r-waveletshrinkage"><i class="fa fa-check"></i><b>7.5</b> R 예제(R-waveletshrinkage)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html"><i class="fa fa-check"></i><b>8</b> 웨이블릿 수축의 고등 논제들</a><ul>
<li class="chapter" data-level="8.1" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#cross-validation"><i class="fa fa-check"></i><b>8.1</b> 교차타당성(cross-validation)</a></li>
<li class="chapter" data-level="8.2" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#-multiple-testing"><i class="fa fa-check"></i><b>8.2</b> 다중 비교(multiple testing)</a></li>
<li class="chapter" data-level="8.3" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#--bayesian-wavelet-shrinkage"><i class="fa fa-check"></i><b>8.3</b> 베이지안 웨이블릿 축소(Bayesian wavelet shrinkage)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#prior-mixture-of-gaussian"><i class="fa fa-check"></i><b>8.3.1</b> Prior mixture of Gaussian</a></li>
<li class="chapter" data-level="8.3.2" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#prior-mixture-of-point-mass-and-gaussian"><i class="fa fa-check"></i><b>8.3.2</b> Prior mixture of point mass and Gaussian</a></li>
<li class="chapter" data-level="8.3.3" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#mixture-of-point-mass-and-heavy-tail-distribution"><i class="fa fa-check"></i><b>8.3.3</b> Mixture of point mass and heavy-tail distribution</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#--linear-wavelet-smoothing"><i class="fa fa-check"></i><b>8.4</b> 선형 웨이블릿 평활화(linear wavelet smoothing)</a></li>
<li class="chapter" data-level="8.5" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#-block-thresholding"><i class="fa fa-check"></i><b>8.5</b> 블록 임계화(block thresholding)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multiscalets.html"><a href="multiscalets.html"><i class="fa fa-check"></i><b>9</b> 다중척도 시계열분석</a><ul>
<li class="chapter" data-level="9.1" data-path="multiscalets.html"><a href="multiscalets.html#--stationary-time-series"><i class="fa fa-check"></i><b>9.1</b> 시계열 자료의 정상성(stationary time series)</a></li>
<li class="chapter" data-level="9.2" data-path="multiscalets.html"><a href="multiscalets.html#-whitening-of-stationary-process"><i class="fa fa-check"></i><b>9.2</b> 정상과정의 백색화(whitening of stationary process)</a></li>
<li class="chapter" data-level="9.3" data-path="multiscalets.html"><a href="multiscalets.html#--spectral-representation-of-stationary-process"><i class="fa fa-check"></i><b>9.3</b> 정상과정의 스펙트럼 표현(spectral representation of stationary process)</a></li>
<li class="chapter" data-level="9.4" data-path="multiscalets.html"><a href="multiscalets.html#----non-decimated-discrete-wavelets"><i class="fa fa-check"></i><b>9.4</b> 압축 표본화되지 않은 이산 웨이블릿(non-decimated discrete wavelets)</a></li>
<li class="chapter" data-level="9.5" data-path="multiscalets.html"><a href="multiscalets.html#---locally-stationary-wavelet-process"><i class="fa fa-check"></i><b>9.5</b> 국소 정상 웨이블릿 과정(locally stationary wavelet process)</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="admultiscale.html"><a href="admultiscale.html"><i class="fa fa-check"></i><b>10</b> 고급 다중척도 방법론</a><ul>
<li class="chapter" data-level="10.1" data-path="admultiscale.html"><a href="admultiscale.html#--second-generation-wavelet-transform"><i class="fa fa-check"></i><b>10.1</b> 2세대 웨이블릿 변환(second-generation wavelet transform)</a></li>
<li class="chapter" data-level="10.2" data-path="admultiscale.html"><a href="admultiscale.html#-lifting-scheme"><i class="fa fa-check"></i><b>10.2</b> 리프팅 스킴(lifting scheme)</a></li>
<li class="chapter" data-level="10.3" data-path="admultiscale.html"><a href="admultiscale.html#---lifting-in-two-dimensions"><i class="fa fa-check"></i><b>10.3</b> 2차원 자료의 리프팅 스킴(lifting in two dimensions)</a></li>
</ul></li>
<li class="part"><span><b>Spatial Statistics</b></span></li>
<li class="chapter" data-level="11" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>11</b> 공간통계학</a><ul>
<li class="chapter" data-level="11.1" data-path="spatial.html"><a href="spatial.html#-classes-of-spatial-data"><i class="fa fa-check"></i><b>11.1</b> 공간자료의 종류(classes of spatial data)</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="spatialprocess.html"><a href="spatialprocess.html"><i class="fa fa-check"></i><b>12</b> 공간과정</a><ul>
<li class="chapter" data-level="12.1" data-path="spatialprocess.html"><a href="spatialprocess.html#-stationary-in-spatial-data"><i class="fa fa-check"></i><b>12.1</b> 공간자료의 정상성(stationary in spatial data)</a><ul>
<li class="chapter" data-level="12.1.1" data-path="spatialprocess.html"><a href="spatialprocess.html#strictly-stationary"><i class="fa fa-check"></i><b>12.1.1</b> 순정상성(strictly stationary)</a></li>
<li class="chapter" data-level="12.1.2" data-path="spatialprocess.html"><a href="spatialprocess.html#second-order-stationary-weakly-stationary"><i class="fa fa-check"></i><b>12.1.2</b> 약정상성(second order stationary, weakly stationary)</a></li>
<li class="chapter" data-level="12.1.3" data-path="spatialprocess.html"><a href="spatialprocess.html#intrinsic-stationary"><i class="fa fa-check"></i><b>12.1.3</b> 내재정상성(intrinsic stationary)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-stationarity"><i class="fa fa-check"></i><b>12.2</b> 정상성들 사이의 관계(relationship between stationarity)</a><ul>
<li class="chapter" data-level="12.2.1" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-strong-and-weak-stationary"><i class="fa fa-check"></i><b>12.2.1</b> 순정상성과 약정상성간의 관계(relationship between strong and weak stationary)</a></li>
<li class="chapter" data-level="12.2.2" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-weak-and-intrinsic-stationary"><i class="fa fa-check"></i><b>12.2.2</b> 약정상성와 내재정상성간의 관계(relationship between weak and intrinsic stationary)</a></li>
<li class="chapter" data-level="12.2.3" data-path="spatialprocess.html"><a href="spatialprocess.html#----counterexample-of-intrinsic-stationary-but-not-weak-stationary"><i class="fa fa-check"></i><b>12.2.3</b> 내재정상성이나 약정상성이 안 되는 예(counterexample of intrinsic stationary but not weak stationary)</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="spatialprocess.html"><a href="spatialprocess.html#-ergodic-process"><i class="fa fa-check"></i><b>12.3</b> 에르고딕 과정(ergodic process)</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="covfct.html"><a href="covfct.html"><i class="fa fa-check"></i><b>13</b> 공분산함수</a><ul>
<li class="chapter" data-level="13.1" data-path="covfct.html"><a href="covfct.html#--spectral-representation-theorem"><i class="fa fa-check"></i><b>13.1</b> 스펙트럴 표현 정리(spectral representation theorem)</a></li>
<li class="chapter" data-level="13.2" data-path="covfct.html"><a href="covfct.html#--kolmogorovs-existence-theorem"><i class="fa fa-check"></i><b>13.2</b> 콜모고로프 존재 정리(Kolmogorov’s existence theorem)</a></li>
<li class="chapter" data-level="13.3" data-path="covfct.html"><a href="covfct.html#-properties-of-covariance-functions"><i class="fa fa-check"></i><b>13.3</b> 공분산함수의 성질(properties of covariance functions)</a><ul>
<li class="chapter" data-level="13.3.1" data-path="covfct.html"><a href="covfct.html#isotropy"><i class="fa fa-check"></i><b>13.3.1</b> 등방성(isotropy)</a></li>
<li class="chapter" data-level="13.3.2" data-path="covfct.html"><a href="covfct.html#homogeneous"><i class="fa fa-check"></i><b>13.3.2</b> 동질성(homogeneous)</a></li>
<li class="chapter" data-level="13.3.3" data-path="covfct.html"><a href="covfct.html#anisotropy"><i class="fa fa-check"></i><b>13.3.3</b> 이등방성(anisotropy)</a></li>
<li class="chapter" data-level="13.3.4" data-path="covfct.html"><a href="covfct.html#z3--geometric-anisotropy"><i class="fa fa-check"></i><b>13.3.4</b> z3 기하학적 이등방성(Geometric anisotropy)</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="covfct.html"><a href="covfct.html#---continuity-and-differentiabiliy-of-spatial-stochastic-process"><i class="fa fa-check"></i><b>13.4</b> 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process</a><ul>
<li class="chapter" data-level="13.4.1" data-path="covfct.html"><a href="covfct.html#path-continuity---path-differentiability"><i class="fa fa-check"></i><b>13.4.1</b> 경로연속(path-continuity) 또는 경로 미분가능성(path-differentiability)</a></li>
<li class="chapter" data-level="13.4.2" data-path="covfct.html"><a href="covfct.html#mean-square-continuity---mean-square-differentiability"><i class="fa fa-check"></i><b>13.4.2</b> 평균제곱연속(mean-square continuity) 또는 평균제곱 미분가능성(mean-square differentiability)</a></li>
<li class="chapter" data-level="13.4.3" data-path="covfct.html"><a href="covfct.html#bartlett-bartletts-theorem"><i class="fa fa-check"></i><b>13.4.3</b> Bartlett의 정리(Bartlett’s theorem)</a></li>
<li class="chapter" data-level="13.4.4" data-path="covfct.html"><a href="covfct.html#kent---kents-sufficient-condition-for-path-continuity-2-d-ver"><i class="fa fa-check"></i><b>13.4.4</b> Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="covmodel.html"><a href="covmodel.html"><i class="fa fa-check"></i><b>14</b> 공분산모형</a><ul>
<li class="chapter" data-level="14.1" data-path="covmodel.html"><a href="covmodel.html#-nugget-effect"><i class="fa fa-check"></i><b>14.1</b> 덩어리 효과(nugget effect)</a></li>
<li class="chapter" data-level="14.2" data-path="covmodel.html"><a href="covmodel.html#--idealized-shape-of-variogram-isotropic-case"><i class="fa fa-check"></i><b>14.2</b> 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case))</a></li>
<li class="chapter" data-level="14.3" data-path="covmodel.html"><a href="covmodel.html#-effective-range"><i class="fa fa-check"></i><b>14.3</b> 유효 범위(effective range)</a></li>
<li class="chapter" data-level="14.4" data-path="covmodel.html"><a href="covmodel.html#----classical-parametric-isotropic-variogram-models"><i class="fa fa-check"></i><b>14.4</b> 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models)</a><ul>
<li class="chapter" data-level="14.4.1" data-path="covmodel.html"><a href="covmodel.html#-----addtional-explanation-for-k_alpha"><i class="fa fa-check"></i><b>14.4.1</b> 변형된 이형 베셀에 대한 보충 설명(addtional explanation for K_alpha)</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="covmodel.html"><a href="covmodel.html#---variograms-in-other-situation"><i class="fa fa-check"></i><b>14.5</b> 기타 다른 상황에서의 변동도들(variograms in other situation)</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="variogramest.html"><a href="variogramest.html"><i class="fa fa-check"></i><b>15</b> 변동도의 추정</a><ul>
<li class="chapter" data-level="15.1" data-path="variogramest.html"><a href="variogramest.html#empirical-variogram"><i class="fa fa-check"></i><b>15.1</b> 경험변동도(empirical variogram)</a></li>
<li class="chapter" data-level="15.2" data-path="variogramest.html"><a href="variogramest.html#----fitting-parametric-models-to-empirical-variogram"><i class="fa fa-check"></i><b>15.2</b> 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram)</a><ul>
<li class="chapter" data-level="15.2.1" data-path="variogramest.html"><a href="variogramest.html#ls-method"><i class="fa fa-check"></i><b>15.2.1</b> LS method</a></li>
<li class="chapter" data-level="15.2.2" data-path="variogramest.html"><a href="variogramest.html#gls-method"><i class="fa fa-check"></i><b>15.2.2</b> GLS method</a></li>
<li class="chapter" data-level="15.2.3" data-path="variogramest.html"><a href="variogramest.html#wls-method"><i class="fa fa-check"></i><b>15.2.3</b> WLS method</a></li>
<li class="chapter" data-level="15.2.4" data-path="variogramest.html"><a href="variogramest.html#-wls-approximated-wls"><i class="fa fa-check"></i><b>15.2.4</b> 근사 WLS (approximated WLS)</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="variogramest.html"><a href="variogramest.html#r-r-variogramest"><i class="fa fa-check"></i><b>15.3</b> R 예제(R-variogramest)</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="spatlikelihood.html"><a href="spatlikelihood.html"><i class="fa fa-check"></i><b>16</b> 공간자료에서의 가능도 기반 방법들</a><ul>
<li class="chapter" data-level="16.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--likelihood-based-methods"><i class="fa fa-check"></i><b>16.1</b> 가능도 기반 방법론들(likelihood-based methods)</a></li>
<li class="chapter" data-level="16.2" data-path="spatlikelihood.html"><a href="spatlikelihood.html#reparametrization"><i class="fa fa-check"></i><b>16.2</b> 재모수화(reparametrization)</a></li>
<li class="chapter" data-level="16.3" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-mle-in-spatial-data"><i class="fa fa-check"></i><b>16.3</b> 공간자료에서의 최대가능도추정(MLE in spatial data)</a></li>
<li class="chapter" data-level="16.4" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-restricted-mle"><i class="fa fa-check"></i><b>16.4</b> 제한된 최대가능도추정(restricted MLE)</a></li>
<li class="chapter" data-level="16.5" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--asymptotics-of-mle-of-spatial-data"><i class="fa fa-check"></i><b>16.5</b> 공간자료 최대우도추정의 점근성(asymptotics of MLE of spatial data)</a><ul>
<li class="chapter" data-level="16.5.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--sum-results-about-asymptotics-of-mle-of-spatial-data"><i class="fa fa-check"></i><b>16.5.1</b> 몇 가지 결과들(sum results about asymptotics of MLE of spatial data)</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="spatlikelihood.html"><a href="spatlikelihood.html#--computational-issues-in-spatial-statistics"><i class="fa fa-check"></i><b>16.6</b> 공간통계에서의 계산 문제들(computational issues in spatial statistics)</a><ul>
<li class="chapter" data-level="16.6.1" data-path="spatlikelihood.html"><a href="spatlikelihood.html#---solutions-about-computational-issues-in-spatial-statistics"><i class="fa fa-check"></i><b>16.6.1</b> 공간통계에서의 계산 문제들의 해결책들(solutions about computational issues in spatial statistics)</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-approximate-likelihood"><i class="fa fa-check"></i><b>16.7</b> 근사 가능도(approximate Likelihood)</a></li>
<li class="chapter" data-level="16.8" data-path="spatlikelihood.html"><a href="spatlikelihood.html#-pseudo-likelihood-and-composite-likelihood"><i class="fa fa-check"></i><b>16.8</b> 유사가능도와 복합가능도(pseudo-Likelihood and composite Likelihood)</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="kriging.html"><a href="kriging.html"><i class="fa fa-check"></i><b>17</b> 크리깅</a><ul>
<li class="chapter" data-level="17.1" data-path="kriging.html"><a href="kriging.html#-spatial-prediction"><i class="fa fa-check"></i><b>17.1</b> 공간 예측(spatial prediction)</a></li>
<li class="chapter" data-level="17.2" data-path="kriging.html"><a href="kriging.html#-universal-kriging"><i class="fa fa-check"></i><b>17.2</b> 일반 크리깅(universal Kriging)</a><ul>
<li class="chapter" data-level="17.2.1" data-path="kriging.html"><a href="kriging.html#--lagrange-multiplier-approach"><i class="fa fa-check"></i><b>17.2.1</b> 라그랑즈 승수 접근법(Lagrange multiplier approach)</a></li>
<li class="chapter" data-level="17.2.2" data-path="kriging.html"><a href="kriging.html#-conditional-distribution-approach"><i class="fa fa-check"></i><b>17.2.2</b> 조건부분포 방법(conditional distribution approach)</a></li>
<li class="chapter" data-level="17.2.3" data-path="kriging.html"><a href="kriging.html#-bayesian-approach"><i class="fa fa-check"></i><b>17.2.3</b> 베이지안 방법(Bayesian approach)</a></li>
<li class="chapter" data-level="17.2.4" data-path="kriging.html"><a href="kriging.html#----kriging-for-the-model-with-a-nugget-effect"><i class="fa fa-check"></i><b>17.2.4</b> 덩어리 효과가 있는 모형의 크리깅(Kriging for the model with a nugget effect)</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="kriging.html"><a href="kriging.html#-prediction-error-in-kriging"><i class="fa fa-check"></i><b>17.3</b> 크리깅의 예측오차(prediction error in Kriging)</a></li>
<li class="chapter" data-level="17.4" data-path="kriging.html"><a href="kriging.html#-other-krigings"><i class="fa fa-check"></i><b>17.4</b> 다른 크리깅들(other Krigings)</a></li>
<li class="chapter" data-level="17.5" data-path="kriging.html"><a href="kriging.html#-more-krigings"><i class="fa fa-check"></i><b>17.5</b> 추가적인 크리깅들(more Krigings)</a></li>
</ul></li>
<li class="part"><span><b>Spatial Point Processes</b></span></li>
<li class="chapter" data-level="18" data-path="pointpattern.html"><a href="pointpattern.html"><i class="fa fa-check"></i><b>18</b> 공간점과정</a><ul>
<li class="chapter" data-level="18.1" data-path="pointpattern.html"><a href="pointpattern.html#--examples-of-spatial-point-patterns"><i class="fa fa-check"></i><b>18.1</b> 공간점패턴 자료의 예(examples of spatial point patterns)</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="csr.html"><a href="csr.html"><i class="fa fa-check"></i><b>19</b> 완전공간임의성</a><ul>
<li class="chapter" data-level="19.1" data-path="csr.html"><a href="csr.html#------general-monte-carlo-methods-for-csr-test"><i class="fa fa-check"></i><b>19.1</b> 완전공간임의성 검정을 위한 일반적인 몬테카를로 방법 (general Monte Carlo methods for CSR test)</a><ul>
<li class="chapter" data-level="19.1.1" data-path="csr.html"><a href="csr.html#-----mc-tests-using-edf"><i class="fa fa-check"></i><b>19.1.1</b> 경험적 분포함수를 이용한 몬테칼로 검정 방법들(MC tests using EDF)</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="csr.html"><a href="csr.html#--methods-based-on-nearest-neighbor-distance"><i class="fa fa-check"></i><b>19.2</b> 최근접이웃거리 기반 방법들(methods based on nearest neighbor distance)</a><ul>
<li class="chapter" data-level="19.2.1" data-path="csr.html"><a href="csr.html#----mc-tests-based-on-nearest-neighbor-distance"><i class="fa fa-check"></i><b>19.2.1</b> 최근접이웃거리 기반 몬테칼로 검정 방법들(MC tests based on nearest neighbor distance)</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="csr.html"><a href="csr.html#r-r-edf"><i class="fa fa-check"></i><b>19.3</b> R 예제(R-edf)</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="sparsesampling.html"><a href="sparsesampling.html"><i class="fa fa-check"></i><b>20</b> 희박한 샘플링 분석</a><ul>
<li class="chapter" data-level="20.1" data-path="sparsesampling.html"><a href="sparsesampling.html#-----quadrat-counts-for-sparse-sampled-data"><i class="fa fa-check"></i><b>20.1</b> 희박한 샘플링 자료를 위한 정방구역 계산(quadrat counts for sparse sampled data)</a></li>
<li class="chapter" data-level="20.2" data-path="sparsesampling.html"><a href="sparsesampling.html#-----distance-methods-for-sparsely-sampled-data"><i class="fa fa-check"></i><b>20.2</b> 희박한 샘플링 자료를 위한 거리 방법들(distance methods for sparsely sampled data)</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="pointprocess.html"><a href="pointprocess.html"><i class="fa fa-check"></i><b>21</b> 점과정</a><ul>
<li class="chapter" data-level="21.1" data-path="pointprocess.html"><a href="pointprocess.html#-definition-of-point-processes"><i class="fa fa-check"></i><b>21.1</b> 점과정의 정의(definition of point processes)</a></li>
</ul></li>
<li class="part"><span><b>Quantile Regression</b></span></li>
<li class="chapter" data-level="22" data-path="qr.html"><a href="qr.html"><i class="fa fa-check"></i><b>22</b> 분위수 회귀분석</a><ul>
<li class="chapter" data-level="22.1" data-path="qr.html"><a href="qr.html#-quantile"><i class="fa fa-check"></i><b>22.1</b> 분위수 (quantile)</a></li>
<li class="chapter" data-level="22.2" data-path="qr.html"><a href="qr.html#--linear-quantile-regression"><i class="fa fa-check"></i><b>22.2</b> 선형 분위수 회귀분석(linear quantile regression)</a></li>
</ul></li>
<li class="part"><span><b>Extreme Value Statistics</b></span></li>
<li class="chapter" data-level="23" data-path="extremevaluestat.html"><a href="extremevaluestat.html"><i class="fa fa-check"></i><b>23</b> 극단값 통계학</a></li>
<li class="chapter" data-level="24" data-path="uGEVtheory.html"><a href="uGEVtheory.html"><i class="fa fa-check"></i><b>24</b> 일변량 극단값 이론</a><ul>
<li class="chapter" data-level="24.1" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--generalized-extreme-value-distribution"><i class="fa fa-check"></i><b>24.1</b> 일반화 극단값 분포(generalized extreme value distribution)</a></li>
<li class="chapter" data-level="24.2" data-path="uGEVtheory.html"><a href="uGEVtheory.html#max-stablity"><i class="fa fa-check"></i><b>24.2</b> 최대안정성(max-stablity)</a></li>
<li class="chapter" data-level="24.3" data-path="uGEVtheory.html"><a href="uGEVtheory.html#-return-level"><i class="fa fa-check"></i><b>24.3</b> 복귀 수준(return level)</a></li>
<li class="chapter" data-level="24.4" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--inference-in-extreme-value-statistics"><i class="fa fa-check"></i><b>24.4</b> 극단값 분포에서의 추론(inference in extreme value statistics)</a></li>
<li class="chapter" data-level="24.5" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--mle-in-extreme-value-statistics"><i class="fa fa-check"></i><b>24.5</b> 극단값 분포에서의 최대가능도추정(mle in extreme value statistics)</a></li>
<li class="chapter" data-level="24.6" data-path="uGEVtheory.html"><a href="uGEVtheory.html#---profile-likelihood-in-extreme-value-statistics"><i class="fa fa-check"></i><b>24.6</b> 극단값 분포에서의 프로파일 가능도(profile likelihood in extreme value statistics)</a></li>
<li class="chapter" data-level="24.7" data-path="uGEVtheory.html"><a href="uGEVtheory.html#gev----maximum-likelihood-estimation-application-to-gev-distribution"><i class="fa fa-check"></i><b>24.7</b> GEV 분포애서의 가능도 추정 (maximum likelihood estimation: application to GEV distribution)</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spatextremes.html"><a href="spatextremes.html"><i class="fa fa-check"></i><b>25</b> 공간 극단값 이론과 최대안정과정</a><ul>
<li class="chapter" data-level="25.1" data-path="spatextremes.html"><a href="spatextremes.html#max-stable-process"><i class="fa fa-check"></i><b>25.1</b> 최대안정과정(max-stable process)</a><ul>
<li class="chapter" data-level="25.1.1" data-path="spatextremes.html"><a href="spatextremes.html#smith-smith-model"><i class="fa fa-check"></i><b>25.1.1</b> Smith 모형(Smith model)</a></li>
<li class="chapter" data-level="25.1.2" data-path="spatextremes.html"><a href="spatextremes.html#schlather-schlather-model"><i class="fa fa-check"></i><b>25.1.2</b> Schlather 모형(Schlather model)</a></li>
<li class="chapter" data-level="25.1.3" data-path="spatextremes.html"><a href="spatextremes.html#brown-resnick-brown-resnick-model"><i class="fa fa-check"></i><b>25.1.3</b> Brown-Resnick 모형(Brown-Resnick model)</a></li>
<li class="chapter" data-level="25.1.4" data-path="spatextremes.html"><a href="spatextremes.html#-t-extremal-t-model"><i class="fa fa-check"></i><b>25.1.4</b> 극단-t 모형(extremal-t model)</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="spatextremes.html"><a href="spatextremes.html#--spatial-dependence-of-extremes"><i class="fa fa-check"></i><b>25.2</b> 극단값의 공간 종속성(spatial dependence of extremes)</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>26</b> References</a></li>
<li class="divider"></li>
<li><a href="https://seoncheolpark.github.io/" target="blank">Return to Park's Github Page</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">통계공부와 관련된 글들</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="kriging" class="section level1">
<h1><span class="header-section-number">Chapter 17</span> 크리깅</h1>
<p>이 문서에서는 지금까지 다뤘던 추정 방법들을 가지고 <strong>공간 예측(spatial prediction)</strong>을 하는 방법들에 대해 다루겠다. 공간 예측을 다른 말로 <strong>크리깅(Kriging)</strong>이라 부른다.</p>
<div id="-spatial-prediction" class="section level2">
<h2><span class="header-section-number">17.1</span> 공간 예측(spatial prediction)</h2>
<p><span class="math inline">\(Z(\mathbf{s})\)</span>를 spatial process라고 하자. 목표는 <span class="math inline">\(n\)</span>개의 관측값(data) <span class="math inline">\(\{ Z(\mathbf{s}_{1}), \cdots Z(\mathbf{s}_{n}) \}\)</span>을 이용해 관측되어지지 않은 장소 <span class="math inline">\(\mathbf{s}_{0}\)</span>의 <span class="math inline">\(Z(\mathbf{s}_{0})\)</span>을 predict하는 것이다. 그런데 <strong>문제는 <span class="math inline">\(Z(\mathbf{s}_{0})\)</span>또한 확률변수라는 것이다</strong>(그래서 predict라는 말을 쓴다고 한다).</p>
<p>그렇다면 어떤 기준을 가지고 prediction할 것인가? 가장 일반적인 기준으로는 <strong>mean-square prediction error (MSPE)</strong>를 계산하는 것이다. 우선 몇 가지 notation들을 정리해보자.</p>
<p><span class="math display">\[\mathbf{Z}=(Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n}))^{T}: \text{ data vector}\]</span> <span class="math display">\[Z_{0}\equiv Z(\mathbf{s}_{0}) \text{ (간단히 쓰기 위함)}\]</span> <span class="math display">\[T=Z_{0}: \text{ predict하고싶은 것}\]</span> <span class="math display">\[\hat{T}=\hat{Z}_{0}: \text{ &quot;prediction&quot;}=t(\mathbf{Z}) \text{ ( 데이터들에 대한 함수)}\]</span></p>
<p>Prediction error를 <span class="math inline">\((T-\hat{T})^{2}\)</span>이라고 하면 MSPE는 <span class="math inline">\(E(T-\hat{T})^{2}\)</span>라고 하며 <span class="math display">\[\hat{T}=\text{argmin}_{\tilde{T}\in t(\mathbf{Z})}E(T-\tilde{T})^{2}\]</span> 을 만족하는 <span class="math inline">\(\hat{T}\)</span>를 <strong>best predictior (BP)</strong>라 부른다.</p>
<p>Restriction을 걸어 다른 predction을 할 수도 있다. 예를 들면 <strong>best linear predictor (BLP)</strong>, <strong>best linear unbiased predictor (BLUP)</strong>등이다. BLUP를 많이 쓴다고 한다. Random effect에서 등장하는 개념과 똑같으나 계산할 때만 spatial covariance로 넣어 계산하는 것이라고 생각할 수도 있다.</p>
<p>여기서 말하는 <strong>결론: <span class="math inline">\(\hat{T}=E(Z_{0}|\mathbf{Z})\)</span></strong></p>
<p>Geostatistics에서 spation prediction은 1950년대 남아공 마이닝 엔지니어 D.G. Krige의 이름을 따 <strong>크리깅(Kriging)</strong>이라 부른다. 이 크리깅의 종류는 여러가지가 있다.</p>
</div>
<div id="-universal-kriging" class="section level2">
<h2><span class="header-section-number">17.2</span> 일반 크리깅(universal Kriging)</h2>
<p><span class="math display">\[\text{Model: } \mathbf{Z}(\mathbf{s})=\mathbf{x}^{T}(\mathbf{s})\boldsymbol{\beta}+\boldsymbol{\epsilon}(\mathbf{s}) \text{ (spatial dependence는 } \boldsymbol{\epsilon}(\mathbf{s})\text{에서 나온다)}\]</span> <span class="math display">\[\text{Data: } \mathbf{Z}=(Z(\mathbf{s}_{1}), \cdots , Z(\mathbf{s}_{n}))^{T}\]</span> <span class="math display">\[\text{Want to predict } Z_{0}=Z(\mathbf{s}_{0})=\mathbf{x}^{T}(\mathbf{s}_{0})\boldsymbol{\beta}+\mathbf+\boldsymbol{\epsilon}(\mathbf{s}_{0})\stackrel{let}{=}\mathbf{x}_{0}\boldsymbol{\beta}+\boldsymbol{\epsilon}_{0}\]</span></p>
<p>일단 covariance structure가 필요하다. <span class="math display">\[\text{Cov}\left[\begin{array}
{r}
\mathbf{Z}\\
Z_{0}\\
\end{array}\right]
=
\left[\begin{array}
{rr}
\boldsymbol{\Sigma}_{n\times n} &amp; \mathbf{T}_{n \times 1}\\
\mathbf{T}_{1\times n}^{T} &amp; \sigma_{0}^{2}\\
\end{array}\right]
\]</span> 로 놓는다. 여기서 <span class="math inline">\(\boldsymbol{\Sigma}=\text{Cov}(\mathbf{Z})\)</span>, <span class="math inline">\(\mathbf{T}_{j}=\text{Cov}(Z(\mathbf{s}_{j}), Z_{0})\)</span>, <span class="math inline">\(\sigma_{0}^{2}=\text{Var}(Z_{0})\)</span>이다.</p>
<p>지금부터 MSPE를 minimize하는 predictor들을 찾는 방법을 다룰 것이다. 이 방법들은 공간통계에 국한된 것이 아니고 mixed effect model 등에도 해당되는 것이다.</p>
<div id="--lagrange-multiplier-approach" class="section level3">
<h3><span class="header-section-number">17.2.1</span> 라그랑즈 승수 접근법(Lagrange multiplier approach)</h3>
<p>Linear unbiased predictor <span class="math display">\[\hat{Z}_{0}=\sum_{i=1}^{n}\lambda_{i}z_{i}=\boldsymbol{\lambda}^{T}\mathbf{Z} \text{ and } E(\hat{Z}_{0}-Z_{0})\]</span> 를 가정하자. 여기서 주의하여야 할 점은 <span class="math inline">\(Z_{0}\)</span>자체도 random이므로 <span class="math inline">\(E(\hat{Z}_{0})=Z_{0}\)</span>라 할 수 없다는 것이다. <span class="math inline">\(\mathbf{Z}=\boldsymbol{X}^{T}\boldsymbol{\beta}+\boldsymbol{\epsilon}\)</span>을 이용하면 <span class="math display">\[
\begin{eqnarray*}
E(\hat{Z}_{0})&amp;=&amp;E(\boldsymbol{\lambda}^{T}\mathbf{Z})\\
&amp;\Longrightarrow&amp; E(\boldsymbol{\lambda}^{T}(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}))=\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
&amp;\Longleftrightarrow&amp; \boldsymbol{\lambda}^{T}\mathbf{X}\boldsymbol{\beta}=\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
&amp;\Longleftrightarrow&amp; \boldsymbol{\lambda}^{T}\mathbf{X}\mathbf{x}_{0}^{T} \textbf{ (unbiasedness 조건)}\\
\end{eqnarray*}
\]</span> 마지막 줄이 성립하는 이유는 <span class="math inline">\(\boldsymbol{\lambda}^{T}\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{x}_{0}^{T}\)</span>는 행렬이 아닌 벡터지만 임의의 <span class="math inline">\(\boldsymbol{\beta}\)</span>에 대해 세번째 줄 식이 모두 만족해야 하기 때문이다.</p>
<p><span class="math inline">\(E(\hat{Z}_{0}-Z_{0})^{2}\)</span>을 minimize하기 위해 우선 <span class="math display">\[
\begin{eqnarray*}
\hat{Z}_{0}-Z_{0} &amp;=&amp; \boldsymbol{\lambda}^{T}(\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}-(\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\epsilon_{0}))\\
&amp;=&amp;\boldsymbol{\lambda}^{T}\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\mathbf{x}_{0}^{T}\boldsymbol{\beta}-\epsilon_{0}\\
&amp;=&amp;\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\mathbf{x}_{0}^{T}\boldsymbol{\beta}-\epsilon_{0}\text{ (unbiased constraint를 쓴다)}\\
&amp;=&amp;\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0}
\end{eqnarray*}
\]</span> 마지막 식을 보면, unbiasedness 조건을 줬더니 놀랍게도 <span class="math inline">\(\boldsymbol{\beta}\)</span>가 없어졌음을 알 수 있다. 이는 <span class="math inline">\(\boldsymbol{\beta}\)</span>를 뭘 쓸지 고민할 필요 없이 prediction이 가능하다는 의미라고 한다.</p>
<p>그러면 <span class="math inline">\(E(\hat{Z}_{0}-Z_{0})^{2}\)</span>의 계산은 <span class="math display">\[
\begin{eqnarray*}
E(\hat{Z}_{0}-Z_{0})^{2}&amp;=&amp;E(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0})^{2}\\
&amp;=&amp;E((\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{2})+E(\epsilon_{0}^{2})-2E(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}-\epsilon_{0})\\
&amp;=&amp;\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}\boldsymbol{\lambda}+\sigma_{0}^{2}-2\boldsymbol{\lambda}^{T}\mathbf{T}\tag{$\star$}\\
\end{eqnarray*}
\]</span> <span class="math inline">\(E((\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{2})\)</span>를 계산하기 위해 <span class="math inline">\(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}\)</span>이 스칼라라는 것을 이용, <span class="math inline">\((\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})(\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon})^{T}=\boldsymbol{\lambda}^{T}\boldsymbol{\epsilon}\boldsymbol{\epsilon}^{T}\boldsymbol{\lambda}\)</span>임를 이용하였다.</p>
<p>이제 <span class="math inline">\(\boldsymbol{\lambda}^{T}\mathbf{X}\mathbf{x}_{0}^{T}\)</span> 제한이 걸린 (<span class="math inline">\(\star\)</span>)를 최소화하는 문제 (constraint optimization problem)을 풀기 위해 <strong>Lagrange multiplier</strong>를 쓰도록 하자. 참고로 나중에 추정을 해야 하지만 지금은 <span class="math inline">\(\boldsymbol{\Sigma}, \sigma_{0}^{2}\)</span>을 안다고 가정한다. 다음 <span class="math display">\[
L(\boldsymbol{\lambda},\boldsymbol{\nu})=\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}\boldsymbol{\lambda}-2\boldsymbol{\lambda}^{T}\mathbf{T}+\sigma_{0}^{2}-2(\boldsymbol{\lambda}^{T}\mathbf{X}-\mathbf{x}_{0}^{T})\boldsymbol{\nu} \text{ (계산을 편하게 하기 위해 constraint에 2를 곱합)}\\
\]</span> 식을 미분하면 minimizer를 구할 수 있다. <span class="math display">\[\frac{\partial L(\boldsymbol{\lambda},\boldsymbol{\nu})}{\partial \boldsymbol{\lambda}}=2\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma}-2\mathbf{T}-2\mathbf{X}\boldsymbol{\nu}=\mathbf{0}\tag{1}\]</span> <span class="math display">\[\frac{\partial L(\boldsymbol{\lambda},\boldsymbol{\nu})}{\partial \boldsymbol{\nu}}=-2(\mathbf{x}_{0}^{T}-\boldsymbol{\lambda}^{T}\mathbf{X})=0 \tag{2}\]</span></p>
<p>우선 (1)식으로부터 <span class="math display">\[\hat{\boldsymbol{\lambda}}=\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}\boldsymbol{\nu})\]</span> 를 얻는다. 이것을 (2)식에 집어넣으면 <span class="math display">\[
\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}\boldsymbol{\nu})=\mathbf{x}_{0}\\
\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}+\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X}\boldsymbol{\nu}=\mathbf{x}_{0}\\
\therefore \hat{\boldsymbol{\nu}}=(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})\\
\]</span> 가 된다. 이 식을 다시 (1)에 집어넣는다. 그러면 <span class="math display">\[\hat{\boldsymbol{\lambda}}=\boldsymbol{\Sigma}^{-1}(\mathbf{T}+\mathbf{X}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}))\]</span> 이고 <span class="math display">\[
\begin{eqnarray*}
\hat{Z_{0}}&amp;=&amp;\hat{\boldsymbol{\lambda}}^{T}\mathbf{Z}=(\mathbf{T}+\mathbf{X}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T}))^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}\\
&amp;=&amp;\mathbf{T}^{2}\boldsymbol{\Sigma}^{-1}\mathbf{Z}+(\mathbf{x}_{0}-\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})^{T}(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}\\
&amp;=&amp;\mathbf{x}_{0}\hat{\boldsymbol{\beta}}_{\text{GLS}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}}_{\text{GLS}})\\
\end{eqnarray*}
\]</span> 이다. 마지막 식을 살펴보면 <span class="math inline">\((\mathbf{Z}-\mathbf{X}^{T}\hat{\boldsymbol{\beta}}_{\text{GLS}})\)</span>는 일종의 residual로 생각할 수 있고 <span class="math inline">\(\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}\)</span>은 대응되는 적당한 weight라고 생각할 수 있다. 여기서 <span class="math display">\[\hat{\boldsymbol{\beta}}_{\text{GLS}}=(\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{X})^{-1}\mathbf{X}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}\]</span> 이다.</p>
</div>
<div id="-conditional-distribution-approach" class="section level3">
<h3><span class="header-section-number">17.2.2</span> 조건부분포 방법(conditional distribution approach)</h3>
<p>이 방법은 라그랑지 승수법을 이용한 방법과 달리 다변량 분포의 정규성 가정이 필요하다. 즉 <span class="math display">\[
\begin{pmatrix}
\mathbf{Z}\\
Z_{0}\\
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
\mathbf{X}\boldsymbol{\beta}\\
\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
\end{pmatrix},
\begin{pmatrix}
\boldsymbol{\Sigma} &amp; \mathbf{T}\\
\mathbf{T}^{T} &amp; \sigma_{0}^{2}\\
\end{pmatrix}
\end{bmatrix}
\]</span> 으로 가정한다. 그러면 <span class="math inline">\(Z_{0}|\mathbf{Z}\)</span>의 분포는 <span class="math display">\[Z_{0}|\mathbf{Z} \sim \mathcal{N}(\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta}), \sigma_{0}^{2}-\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{T})\]</span> 가 된다. 조건부 기댓값을 구하는 공식에 의해 <span class="math inline">\(\hat{Z}_{0}\)</span>을 정할 수 있다. <span class="math display">\[\hat{Z}_{0}=E(Z_{0}|\mathbf{Z})=\mathbf{x}_{0}^{T}\boldsymbol{\beta}+\mathbf{T}^{T}\Sigma^{-1}(\mathbf{Z}-\mathbf{X}\boldsymbol{\beta}).\]</span> 그런데 이 추정량은 알려지지 않은 <span class="math inline">\(\boldsymbol{\beta}\)</span>가 들어있어 문제가 된다. 그래서 실제 문제에서 이대로 쓰지 못한다. 따라서 <span class="math display">\[\hat{Z}_{0}\Longrightarrow \mathbf{x}_{0}^{T}\hat{\boldsymbol{\beta}}+\mathbf{T}^{T}\Sigma^{-1}(\mathbf{Z}-\mathbf{X}\hat{\boldsymbol{\beta}})\]</span> 로 바꿔 사용한다(참고로 normal 가정시 BP=BLP=BLUP라고 한다).</p>
</div>
<div id="-bayesian-approach" class="section level3">
<h3><span class="header-section-number">17.2.3</span> 베이지안 방법(Bayesian approach)</h3>
<p>여기서도 조건부 분포 방법과 마찬가지로 다변량 분포의 정규성 가정이 필요하다. <span class="math display">\[
\begin{pmatrix}
\mathbf{Z}\\
Z_{0}\\
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
\mathbf{X}\boldsymbol{\beta}\\
\mathbf{x}_{0}^{T}\boldsymbol{\beta}\\
\end{pmatrix},
\begin{pmatrix}
\boldsymbol{\Sigma} &amp; \mathbf{T}\\
\mathbf{T}^{T} &amp; \sigma_{0}^{2}\\
\end{pmatrix}
\end{bmatrix}
\]</span> 이때 <span class="math display">\[\Sigma=\sigma^{2}V(\theta), \mathbf{T}=\sigma^{2}W(\theta), \sigma_{0}^{2}=\sigma^{2}\]</span> 으로 놓는다. 그 다음에는 <span class="math inline">\(\boldsymbol{\beta}, \sigma^{2}, \theta\)</span>(보통 1,2차원이라 가정한다)에 대한 사전분포를 만들어야 한다. 일반적으로 이 사전분포는 <span class="math display">\[\propto \pi(\theta)\cdot \frac{1}{\sigma^{2}}\]</span> 로 준다. <span class="math inline">\(\boldsymbol{\beta}\)</span>는 flat prior를 주며 <span class="math inline">\(\pi(\theta)\)</span>는 parameter에 따라 바뀐다. 가능하면 사전분포의 contribution을 없애고 싶기 때문에 이렇게 놓는다고 한다.</p>
<p>Bayesian setting에서는 <span class="math inline">\(Z_{0}|\mathbf{Z}\)</span>에 <span class="math inline">\(Z_{0}|\mathbf{Z}, \boldsymbol{\beta}, \sigma^{2}, \theta\)</span>(이 분포는 알고 있는 분포이다)가 숨어있는 꼴이라고 한다. <span class="math inline">\(\pi(Z_{0}|\mathbf{Z})\)</span>를 만들려면 <span class="math inline">\(\boldsymbol{\beta}, \sigma^{2}, \theta\)</span>에 대해 적분해야 한다. 먼저 <span class="math inline">\(\boldsymbol{\beta}\)</span>에 대해 decompose를 이용해 적분하면 <span class="math display">\[
\begin{eqnarray*}
\pi(Z_{0}|\mathbf{Z},\sigma^{2},\theta)&amp;=&amp;\int \pi(Z_{0},\boldsymbol{\beta}|\mathbf{Z},\sigma^{2},\theta)d\mathbf{\beta}\\
&amp;=&amp; \int \pi(Z_{0}|\mathbf{Z},\boldsymbol{\beta},\sigma^{2},\theta)\pi(\boldsymbol{\beta}|\mathbf{Z},\sigma^{2},\theta)d\mathbf{\beta}\\
\end{eqnarray*}
\]</span> 으로 구할 수 있다. 같은 방법으로 <span class="math inline">\(\sigma^{2}\)</span>에 대해서도 적분하면 <span class="math display">\[\pi(Z_{0}|\mathbf{Z},\theta)=\int \pi(Z_{0},\sigma^{2}|\mathbf{Z},\theta)d\sigma^{2}\]</span> 으로 구할 수 있다. 여기까지는 explicit form이 잘 나온다고 한다. 그리고 <span class="math inline">\(\theta\)</span>에 대해서도 마찬가지로 적분할 수 있는데 <span class="math display">\[
\begin{eqnarray*}
\pi(Z_{0}|\mathbf{Z})&amp;=&amp;\int\pi(Z_{0},\theta |\mathbf{Z})d\theta\\
&amp;=&amp;\int\pi(Z_{0}|\mathbf{Z}, \theta )\cdot\pi(\theta)d\theta\\
\end{eqnarray*}
\]</span> 이다. 그런데 여기서는 explicit form이 잘 안나와 보통 numerical integration을 한다고 한다.</p>
<p>이 방법의 장점은 앞선 두 방법들과 달리 <span class="math inline">\(\Sigma\)</span>, <span class="math inline">\(\sigma_{0}^{2}\)</span>을 알 필요가 없다는 것이다. 라그랑지 승수법과 조건부 분포를 이용한 방법은 <span class="math inline">\(\hat{\Sigma}\)</span>, <span class="math inline">\(\hat{\sigma}_{0}^{2}\)</span>를 통해 uncertainty가 늘어나는데 이 방법은 그렇지 않다.</p>
<p>그리고 <span class="math inline">\(\hat{\phi}=(\hat{\boldsymbol{\beta}}, \hat{\sigma}^{2}, \hat{theta})\)</span>로 parameterization했을 때에도 unbiased한지 optimal한지 체크해야 하는데, 어떠한 조건이 주어질 경우 된다고 한다.</p>
</div>
<div id="----kriging-for-the-model-with-a-nugget-effect" class="section level3">
<h3><span class="header-section-number">17.2.4</span> 덩어리 효과가 있는 모형의 크리깅(Kriging for the model with a nugget effect)</h3>
<p>지금까지 크리깅은 nugget 효과가 없다는 가정에서 진행하였다. 그렇다면 nugget이 있는 경우에는 어떻게 되는가? <span class="math inline">\(Z(\mathbf{s})\)</span>에 대해 다음과 같은 decomposition을 할 수 있다. <span class="math display">\[Z(\mathbf{s})=x(\mathbf{s})^{T}\boldsymbol{\beta}+\eta(\mathbf{s})+\epsilon(\mathbf{s}).\]</span> 이것에 대한 자세한 내용은 Cressie 책 112쪽을 참고하기 바란다. 그러면 <span class="math inline">\(\boldsymbol{\Sigma}\)</span>가 <span class="math inline">\(\boldsymbol{\Gamma}\)</span>로 대체된다. <span class="math display">\[\boldsymbol{\Gamma}=\boldsymbol{\Sigma}+C_{0}\mathbf{I}\]</span> 여기서 <span class="math inline">\(\boldsymbol{\Sigma}\)</span>는 <span class="math inline">\(\eta(\mathbf{s})\)</span>의 covariance structure, <span class="math inline">\(C_{0}\mathbf{I}\)</span>는 <span class="math inline">\(\epsilon(\mathbf{s})\)</span>에 대한 covariance structure이다.</p>
<p>어쨌든 결론은 nugget이 커지면 prediction 결과도 spread out (퍼짐)한다는 것이다.</p>
</div>
</div>
<div id="-prediction-error-in-kriging" class="section level2">
<h2><span class="header-section-number">17.3</span> 크리깅의 예측오차(prediction error in Kriging)</h2>
<p>크리깅에 대해서 다시 살펴보면, 관측하지 않은 <span class="math inline">\(\mathbf{s}_{0}\)</span>지점을 <span class="math display">\[\hat{Z}_{0}\equiv \hat{Z}(S_{0})\]</span> 으로 예측하는 것이다. 그러나 실제로 <span class="math inline">\(\hat{Z}_{0}=\hat{Z}_{0}(\boldsymbol{\psi})\)</span>, <span class="math inline">\(\boldsymbol{\psi}\)</span>는 covariance parameter들의 set인데 이 parameter들을 모르므로 <span class="math inline">\(\hat{Z}_{0}(\hat{\boldsymbol{\psi}})\)</span> (이것을 추정하는 것, OLS, WLS, MLE, REML, empirically 하는 것이 저번시간에 했던 내용들)로 해야한다. 즉 <span class="math inline">\(\boldsymbol{\psi} \rightarrow \hat{\boldsymbol{\psi}}\)</span>로 할 때 추가 error가 발생하는 것이다. 다시 말하면 <span class="math inline">\(\hat{Z}_{0}(\hat{\boldsymbol{\psi}})\)</span>는 MSPE를 overestimate하는 것이다.</p>
<p>그래서 다음과 같은 correction을 한다. <span class="math display">\[
\begin{aligned}
\hat{m}(\boldsymbol{\psi})=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0})^{2}&amp;=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi})+Z_{0}(\boldsymbol{\psi})-Z_{0})^2\\
&amp;=E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi}))^{2}+E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2\\
&amp;\geq E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2
\end{aligned}
\]</span> 여기서 <span class="math inline">\(E(Z_{0}(\hat{\boldsymbol{\psi}})-Z_{0}(\boldsymbol{\psi}))^{2}=m&#39;(\boldsymbol{\psi})\)</span>라 하고 <span class="math inline">\(E(Z_{0}(\boldsymbol{\psi})-Z_{0})^2=m(\boldsymbol{\psi})\)</span>라 하면 <span class="math inline">\(\hat{m}(\boldsymbol{\psi})\)</span>에서 <span class="math inline">\(m&#39;(\boldsymbol{\psi})\)</span>를 빼내어 <span class="math inline">\(m(\boldsymbol{\psi})\)</span>에 가깝게 만든다고 한다(correction).</p>
<p>사실 application이 굉장히 많고 공간통계학 뿐만 아니라 random effect prediction에서도 생기는 문제라고 한다. 그런데 공간통계학자들은 이 예측오차를 무시하고 그냥 할 때가 많고 random effect쪽은 자료가 독립이라 간단하게 된다고 한다. 이 문제를 깊게 다루는 분야는 <strong>small area estimation</strong>이니 관심있으면 찾아보는 것이 좋다.</p>
<p>또한 Bayesian 입장에서는 <span class="math inline">\(\boldsymbol{\psi}\)</span>를 integrated out하므로 <span class="math inline">\(\pi(Z_{0]|\mathbf{Z}})\)</span>하는 것이 큰 문제가 되지 않는다. 그러나 numerical integration이 문제가 된다.</p>
</div>
<div id="-other-krigings" class="section level2">
<h2><span class="header-section-number">17.4</span> 다른 크리깅들(other Krigings)</h2>
<p>Universal kriging 이외에 다른 방법들은 간단히 소개만 하고 넘어갈 것이다.</p>
<ul>
<li><p><strong>Simple kriging</strong>: <span class="math inline">\(\mathbf{x}_{0}^{T}\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}\)</span> <span class="math display">\[\hat{Z}_{0}=\hat{\boldsymbol{\mu}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\hat{\boldsymbol{\mu}}\cdot \mathbf{1}), \hat{\mathbf{\mu}}=\bar{\mathbf{Z}}.\]</span> 여전히 covariance parameter들은 안다고 가정한다.</p></li>
<li><p><strong>Ordinary kriging</strong>: <span class="math inline">\(\mathbf{x}_{0}^{T}\boldsymbol{\beta} \rightarrow \boldsymbol{\mu}\)</span> <span class="math display">\[\hat{Z}_{0}=\tilde{\boldsymbol{\mu}}+\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1}(\mathbf{Z}-\tilde{\boldsymbol{\mu}}\cdot \mathbf{1}), \tilde{\boldsymbol{\mu}}=(\mathbf{1}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{1})^{-1}\mathbf{1}^{T}\boldsymbol{\Sigma}^{-1}\mathbf{Z}.\]</span> 여기서 <span class="math inline">\(\tilde{\boldsymbol{\mu}}\)</span>는 <span class="math inline">\(\boldsymbol{\mu}\)</span>의 GLS solution이며 ordinary kriging은 universal kriging의 special case이다.</p></li>
</ul>
<p>만약 <span class="math inline">\(\hat{Z}_{0}\)</span>를 prediction weight (<span class="math inline">\(\boldsymbol{\psi}\)</span>의 함수) <span class="math inline">\(\lambda_{i}\)</span>와 <span class="math inline">\(Z(\mathbf{s}_{i})\)</span>의 linaer combination으로 생각한다면 <span class="math inline">\(\hat{Z}_{0}=\sum_{i=1}^{n}\lambda_{i}Z(\mathbf{s}_{i})\)</span>로 쓸 수 있다. Simple kriging의 경우는 <span class="math display">\[
\begin{eqnarray*}
\hat{Z}_{0}&amp;=&amp;(1-\sigma_{i}\lambda_{i})\boldsymbol{\mu}+\boldsymbol{\lambda}^{T}\mathbf{Z} \qquad(\boldsymbol{\lambda}=\mathbf{T}^{T}\boldsymbol{\Sigma}^{-1})\\
&amp;=&amp;(1-\sigma_{i}\lambda_{i})\boldsymbol{\mu}+\sum_{i}\lambda_{i}Z(\mathbf{s}_{i})\\
&amp;=&amp;\text{global mean + data로부터 나오는 mean}\\
\end{eqnarray*}
\]</span> 으로 <span class="math inline">\(\hat{\boldsymbol{\mu}}=\bar{\mathbf{Z}}\)</span>로 했기 때문에 global mean이 없어지지 않고 남는다. 한편 ordinary kriging의 경우는 <span class="math inline">\(\sum_{i}\lambda_{i}=1\)</span>이라는 제약조건 하에 <span class="math inline">\(\hat{Z}_{0}=\sum_{i}\lambda_{i}Z(\mathbf{s}_{i})\)</span>로 표현 가능하다. 참고로 universal kriging인 경우에 constraint는 <span class="math inline">\(\boldsymbol{\lambda}^{T}\mathbf{x}=x_{0}\)</span>이라는 제약조건이 있다.</p>
<p>참고로 <strong>크리깅 자체는 stationary 가정이 필요 없다.</strong> 다만 나중에 <span class="math inline">\(\Sigma\)</span> 추정시 문제가 될 수 있다고 한다.</p>
</div>
<div id="-more-krigings" class="section level2">
<h2><span class="header-section-number">17.5</span> 추가적인 크리깅들(more Krigings)</h2>
<ul>
<li><strong>Co-kriging</strong> (kriging의 multivariate case)</li>
</ul>
<p>예를 들면 weather station에 있는 기온, 습도, 풍속 데이터 등을 크리깅할 경우가 이에 해당한다. 한 장소에 <span class="math inline">\(p\)</span>개의 데이터 <span class="math display">\[\mathbf{Z}(\mathbf{s})=(Z_{1}(\mathbf{s}), \ldots, Z_{p}(\mathbf{s}_{n}))\]</span> 이 있다고 하자. 여기서 <span class="math inline">\(Z_{1}(\mathbf{s}_{0})\)</span>을 예측하고 싶다고 하자. 추가로 <span class="math inline">\(E(\mathbf{Z}(\mathbf{s}))=\boldsymbol{\mu}=(\mu_{1}, \ldots , \mu_{n})^{T}\)</span>, <span class="math display">\[\text{Cov}(\mathbf{Z}(\mathbf{s}),\mathbf{Z}(\mathbf{t}))=C(\mathbf{s},\mathbf{t})=
\begin{bmatrix}
C_{11}(\mathbf{s},\mathbf{t}) &amp; \ldots &amp; C_{1p}(\mathbf{s},\mathbf{t}) \\
\vdots &amp; \ddots &amp; \vdots\\
C_{p1}(\mathbf{s},\mathbf{t}) &amp; \ldots &amp; C_{pp}(\mathbf{s},\mathbf{t})
\end{bmatrix}
\]</span> 이다<span class="math inline">\((p\times p\)</span> matrix). Cross-covariance에 대한 모델링이 추가로 필요하다.</p>
<p>다음과 같은 linear unbiased predictior <span class="math inline">\(\hat{Z}_{1}(\mathbf{s}_{0})=\sum_{i}^{n}\sum_{j}^{p}\lambda_{ji}Z_{j}(\mathbf{s}_{i})\)</span>를 생각하보자. unbiasedness에 의해 constraint가 나온다. <span class="math display">\[E(\hat{Z}_{1}(\mathbf{s}_{0})-Z_{1}(\mathbf{s}_{0}))=0 \rightarrow \sum_{i}^{n}\lambda_{ji}=1, \sum_{i=1}^{n}\lambda_{ji}=0 \qquad \text{for } j=2,\ldots, p (unbiased constraints).\]</span></p>
<p>그러면 <span class="math display">\[\text{MSPE}=E(Z_{1}(\mathbf{s}_{0})-\sum_{i=1}^{n}\sum_{j=1}^{p}\lambda_{ji}Z_{j}(\mathbf{s}_{i})) \qquad{\text{under some constraints.}}\]</span> 로 크리깅을 할 수 있다. 복잡하지만 라그랑지 승수법으로 계산 가능하다고 한다.</p>
<ul>
<li><strong>Trans-Gaussian Kriging</strong></li>
</ul>
<p>이 방법은 자료가 가우시안이 아닐 경우 어떡할 것인가로부터 시작했다. <span class="math inline">\(\mathbf{Z}(\mathbf{s})\)</span>가 가우시안이 아니라고 하자. 만약 변환을 통해 만들어진 <span class="math inline">\(\mathbf{Y}(\mathbf{s})\)</span> <span class="math display">\[\mathbf{Y}(\mathbf{s})=h(\mathbf{Z}(\mathbf{s}))\]</span> 가 가우시안일 경우 모델링 할 수 있다는 것이다.</p>
<p>여기서 잠시 자료가 가우시안인지 아닌지 어떻게 검정할 것인가라는 문제가 있는데, 자료가 독립일 경우 Q-Q plot 또는 Kolmogorov-Sminov 검정 등을 한다고 한다. 공간통계학자들의 경우 검정보다는 그냥 모델링에 관심을 갖는다. <span class="math inline">\(\mathbf{Z}(\mathbf{s_{0}})\)</span>으로부터 <span class="math inline">\(E(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Z})\)</span>를 예측하고 싶어한다. 이 때 가우시안 가정은 필요치 않으나 가우시안이 아닐 경우 이 predictor가 optimal이 아닐 수도 있다고 한다.</p>
<p>실제로 <span class="math inline">\(E(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Z})\)</span>의 추정량 <span class="math inline">\(\hat{E}(\mathbf{Z}(\mathbf{s_{0}})|\mathbf{Y})\)</span>을 구할 수 있는 <span class="math inline">\(h\)</span>는 몇 개 없다고 한다. 구할 수 있는 가장 대표적인 <span class="math inline">\(h\)</span>는 log-normal로, 이 때의 <span class="math inline">\(Z\)</span>를 log-normal process라고 부른다.</p>
<ul>
<li><strong>Indicator Kriging</strong></li>
</ul>
<p>지금까지는 조건부 기댓값을 예측했지만, 이번에는 확률을 예측하고 싶어한다. 예를 들면, <span class="math inline">\(Z(s_{1}), \ldots , Z(s_{n})\)</span>이라는 자료를 사용 가능할 때 <span class="math inline">\(s_{0}\)</span>라는 장소의 비가 <span class="math inline">\(c\)</span> 이상 올 확률에 대해 알고 싶어한다고 하자. 즉, <span class="math display">\[P(Z(s_{0})&gt;c |\mathbf{Z})=E(I(Z(s_{0})&gt;c)|\mathbf{Z})=P(Y(s_{0})|\mathbf{Z})\]</span> 에 대해 알고싶어한다.</p>
<p>한 가지 방법은 indicator observation <span class="math inline">\((0,1)\)</span>로 크리깅하는 것이다.</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="spatlikelihood.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pointpattern.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://seoncheolpark.github.io/book/_book/27-kriging.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
