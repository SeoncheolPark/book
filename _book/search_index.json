[
["index.html", "통계공부와 관련된 글들 Chapter 1 일러두기", " 통계공부와 관련된 글들 Seoncheol Park 2016-07-14 Chapter 1 일러두기 각 장은 독립된 구성으로 되어 있으며, 한 권 이상의 책들을 참고문헌으로 하여 그들의 정의 및 표현을 따라가는 방식으로 구성되어 있다. 따라서 각 장마다 표현 및 한국어 용어 번역이 상이할 수 있다. "],
["multiscale.html", "Chapter 2 다중척도 방법론 2.1 다중척도 변환(multiscale transform) 2.2 역(inverse) 2.3 희소성(sparsity) 2.4 신호처리에서의 필터(filter in signal processing) 2.5 R 예제(R-multiscale)", " Chapter 2 다중척도 방법론 이 장에서는 통계학에서의 다중척도 방법론을 다룬다. 주된 내용은 2015년 지도교수님의 특강 수업 내용이다. 이 문서에 담겨있는 그림들은 (G. Nason 2010)을 참고하였다. 2.1 다중척도 변환(multiscale transform) 다음과 같은 형태의 벡터 자료를 생각해보자. \\[ \\mathbf{y}=(y_{1},\\ldots,y_{n}), n=2^{J}\\] 여기서 \\(n=2^{J}\\)는 굉장히 강하고 불편한 조건이다. 예를 들어, 자료의 길이가 800개 또는 900개 정도라면 자료의 길이가 2의 배수라는 조건에 맞게 데이터를 일부를 버려야 한다. 또한 \\(\\mathbf{y}\\)는 등간격 자료(equally spaced data)여야 한다. 예를 들어, 시계열 자료의 경우 오늘 10시, 내일 10시에 관측된 값이 자료에 있으면 그 다음 값은 모레 10시에 관측된 값이여야 하며, 11시에 관측된 값이 와서는 안 된다는 것이다. 다중척도 방법론에서 알고 싶어하는 가장 중요한 정보는 각기 다른 척도(scale)와 위치(location)에서의 \\(\\mathbf{y}\\)의 상세(detail)이다. 여기서 척도는 수준(level), 해상도(resolution) 등으로 불리기도 하며 통계학 용어로 번역하자면 분산, 파워(power), 도수(frequency) 등으로 말할 수 있다. 장소라는 것은 관측값을 관찰한 정의역(domain)을 의미하며, 시간 자료면 시간, 공간 자료면 공간이 로케이션이 된다. 한편 상세의 정의는 다음과 같다. 주어진 자료 \\(\\mathbf{y}\\)의 상세(detail) \\(d_{k}\\)는 \\[d_{k}=y_{2k}-y_{2k-1},\\qquad{k=1,2,\\ldots,\\frac{n}{2}}\\] 이다. 다음과 같이 길이 8인 자료 \\(\\textbf{y}=(y_{1},y_{2},\\ldots,y_{8})\\)가 있다고 하자. 그러면 이 자료의 상세는 \\[d_{1}=y_{2}-y_{1}, d_{2}=y_{4}-y_{3}, d_{3}=y_{6}-y_{5}, d_{4}=y_{8}-y_{7}\\] 와 같이 4개가 존재한다. 여기서 특이한 점은 \\(y_{3}-y_{2}\\)와 같은 값들은 고려하지 않는다는 것이다. 이는 어떻게 관측하느냐에 따라 \\(d_{k}\\)가 완전히 달라질 수도 있다는 말이다. 즉 상세는 평행 이동 불변하지 않다(not translation invariant). 한편, 상세와 유사한 개념으로 성김(coarser)을 정의한다. 주어진 자료 \\(\\mathbf{y}\\)의 성김(coarser) \\(c_{k}\\)은 \\[c_{k}=y_{2k}+y_{2k-1},\\qquad{k=1,2,\\ldots,\\frac{n}{2}}\\] 이다. 성김은 매끄러움(smooth)으로 불리기도 하며, +의 개념이다. 상세는 차이(difference)로 불리기도 하며, -의 개념이다. \\(c_{k}\\)와 \\(d_{k}\\)를 알고 있으면 원래 자료들의 원소 \\(y_{i}\\)들도 다 알아낼 수 있다. 이렇게 다중척도 변환은 원래 신호를 재구성(reconstruction)할 수 있어야 한다. 한편, 앞선 예제에서처럼 자료의 길이가 8일 때, (특정 수준에서) 얻을 수 있는 최대 상세는 4개이다. 이것을 가장 섬세한 상세(finest-detail)라고 한다. 그런데 우리가 \\(c_{k}\\)를 이용해서 \\(d_{k}(=d_{J-1})\\)보다 좀 더 엉성한 상세를 얻고 싶을 수 있다. 그러면 그 것보다 낮은 수준, 즉 \\(J-2\\) 수준을 생각하면 된다. \\(J-2\\) 수준에서의 상세는 \\begin{eqnarray*} d_{J-2,l}&amp;=&amp;c_{J-1,2l}-c_{J-1,2l-1},l=1,2,\\ldots,\\frac{n}{4}\\\\ &amp;=&amp;(y_{4l}+y_{4l-1})-(y_{4l-2}+y_{4l-3})\\\\ \\end{eqnarray*} 로 정의된다. 예를 들어, \\(d_{J-2,1}=(y_{4}+y_{3})-(y_{2}+y_{1})\\)이다. 마찬가지로 \\(J-2\\) 레벨에서의 성김은 \\[ c_{J-2,l}=c_{J-1,2l}+c_{J-1,2l-1},l=1,2,\\ldots,\\frac{n}{4} \\] 이다. 앞서 말한 다중척도 과정을 그림으로 요약하면 다음과 같다. Figure 2.1: Generic step in multiscale transform. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)를 생각해보자. 그러면 \\(J=0,1,2\\)에서의 상세와 성김은 Figure 2.2: Graphical depiction of a multiscale transform. 로 구할 수 있다. 이는 Mallat이 1998년 개발하였으며 피라미드 알고리즘(pyramid algorithm)이라 부른다. 다중척도 변환은 다음과 같이 같은 차원의 새로운 벡터를 정의하는 과정으로 볼 수 있다. \\[(1,1,7,9,2,8,8,6) \\rightarrow (42,6,14,4,0,2,6,-2).\\] 바뀐 벡터를 살펴보면, 42는 평균(global trend)에 해당되고, 6은 \\(J=0\\)일 때의 상세, \\((14,4)\\)는 \\(J=1\\)일 때의 상세, \\((0,2,6,-2)\\)는 \\(J=2\\)일 때의 상세이다. \\(d_{j,k}\\)는 웨이블릿 계수(wavelet coefficient)로, \\(c_{j,k}\\)는 압축 계수(scaling coefficient) 또는 부드러움 계수(smooth coefficient)라 부른다. 여기서 \\(j\\)는 수준(level), 척도(scale), 또는 해상도(resolution)을 나타내며, \\(k\\)는 위치(location)를 나타낸다. 2.2 역(inverse) 우리는 \\(\\{ d_{j,k} \\}\\)와 \\(\\{ c_{j,k} \\}\\)를 가지고 \\(\\mathbf{y}\\)를 구할 수 있다. 이를 위해서는 \\[c_{j-1,2k}=\\frac{(c_{j-1,2k}+d_{j-2,k})}{2}, c_{j-1,2k-1}=\\frac{(c_{j-1,2k}+-d_{j-2,k})}{2}\\] 이 두 가지만 알고 있으면 된다. 앞서 다룬 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)를 생각해보자. 이것의 다중척도 변환 결과는 \\[(c_{01},d_{0,1},d_{11},d_{12},d_{21},d_{22},d_{23},d_{24})=(42,6,14,4,0,2,6,-2)\\] 였다. 이를 가지고 역변환을 해 보면, \\[c_{12}=\\frac{(42+6)}{2}=24, c_{11}=\\frac{(42-6)}{2}=18,\\] \\[c_{24}=\\frac{(24+4)}{2}=14, c_{23}=\\frac{(24-4)}{2}=10, c_{22}=\\frac{(18+14)}{2}=16, c_{21}=\\frac{(18-14)}{2}=2,\\] \\[c_{38}=\\frac{(14-2)}{2}=6, c_{37}=\\frac{(14+2)}{2}=8, c_{36}=\\frac{(10+6)}{2}=8, c_{35}=\\frac{(10-6)}{2}=2, \\] \\[c_{34}=\\frac{(16+2)}{2}=9, c_{33}=\\frac{(16-2)}{2}=7, c_{32}=\\frac{(2+0)}{2}=1, c_{31}=\\frac{(2-0)}{2}=1.\\] 이다. 여기서 알 수 있는 사실 중 하나는 가장 섬세한 부드러움 계수는 데이터, 즉 자료라는 것이다. 그런데 데이터를 부드러움 계수로 다루는 것이 과역 적절한가? 라는 의문이 들 수 있다. 예를 들어, 자료에 잡음이 너무 많은 경우 부드러움 계수 또한 오류가 많이 생길 것이다. 통계학에서는 이를 보완하기 위해 \\(\\mathbf{y}\\) 또는 \\(d\\)에 적절한 추정을 한 \\(\\hat{y}\\) 또는 \\(\\hat{d}\\) 등을 고려하기도 한다. (통계학자들은 데이터를 언제나 잡음이 끼어있는 신호라고 생각하고 있음을 명심해야 한다. 이 점이 통계학자와 다른 분야의 학자들이 자료를 보는 관점의 가장 큰 차이점 중 하나이다.) 2.3 희소성(sparsity) Figure 2.3: Example of sparse function. 위 그림은 희소성을 갖는 함수의 전형적인 예 중 하나이다. 이 자료는 왼쪽은 항상 1, 오른쪽은 항상 2의 값을 갖고 있는 부드러운(smooth) 함수이며 변화가 없다. 그러나 가운데 지점에서는 함숫값이 1에서 2로 바뀌면서 급격한 점프가 일어난다. 이 지점은 다른 지역과는 달리 굉장히 다른 정보를 갖고 있는 것이다. 기존 회귀분석에서는 근본적인 함수(underlying function)들의 동질성(homogeneous) 가정을 바탕으로 분석한다. 이 말은 변동이 항상 일정하다는 뜻으로, 함수가 어떤 지역에서 두 번 미분 가능하면 다른 지역에서도 똑같이 두 번 미분 가능해야 한다는 것이다. 그러나 위 그림의 함수처럼 어떤 지역에서는 한 번만 미분 가능하거나 아예 미분 가능하지 않을 수도 있다. 이런 함수들을 다룰 때에는 다중척도 방법으로 접근하는 것이 필요하다. (Donoho and Johnstone 1994)의 논문 이전까지 통계학자들의 관심사는 부드러운 함수의 평균 추정에 집중되어 있었다. Donoho는 논문에서 몇 가지 혁신적인 개념들을 제시했는데, 임계화(thresholding), 희소성(sparsity) 등이 그것이다. 그의 아이디어는 당시에는 이해하기 힘든 것들이었다. 그러나 이 논문은 후대에 들에 고차원 자료 분석의 밑거름이 되게 해 주었고, least absolute sharinkage and selection operator (LASSO)와 거의 같은 개념을 먼저 제시하였다. Donoho의 제자인 Fan은 후에 스승의 아이디어를 알기 쉽게 해석하여 smoothly clipped absolute deviation (SCAD)라는 것을 제안하기도 하였다. 우리가 지금까지 일반적으로 배운 회귀분석 모형은 다음과 같이 나타낼 수 있다. \\[y=f+\\epsilon, \\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})\\] 다시 말하면, 우리가 지금까지 다뤘던 모든 자료에는 오차(\\(\\epsilon\\))와 \\(f\\)가 공존하는 형태의 모형이다. 이러한 모형에서는, 평균이 매우 중요하며 큰 의미를 갖게 된다. 그러나 성긴 모형에서는 상황이 조금 달라진다. 어떤 \\(y\\)는 \\(f\\)만 갖기도 하고, 또는 \\(\\epsilon\\)만 갖기도 한다. \\(\\mathbf{y}=(y_{1},y_{2},y_{3},y_{4})\\)라는 자료가 있을 때 이들 중 \\(y_{3}\\)만 \\(f\\)의 정보가 들어있는 진짜 신호이고 나머지 \\(y_{1},y_{2},y_{4}\\)는 잡음만 있을 수도 있는 것이다. 이런 자료에서 가장 좋은 추정량은 평균이 아니라 \\(y_{3}\\)이다. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,1,1,2,2,2,2)\\)를 생각해보자. 이 자료를 다중척도 변환해 보면 \\[(1,1,1,1,2,2,2,2) \\rightarrow (1,2,4,0,0,0,0,0)\\] 과 같이 0이 많은 벡터로 변환될 것이다. 위 예제와 같이 0이 많이 있는 자료들을 희소성(sparsity)이 있는 자료라 하며, 다중척도 변환은 희소성이 있는 자료를 다룰 때 많은 도움이 될 수 있다. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)을 다중척도 변환한 결과는 \\((42,6,14,4,0,2,6,-2)\\)였다. 이 벡터를 \\(\\mathbf{d}\\)라 하자. 그러면 \\[\\| \\mathbf{y} \\|^{2}=\\sum_{i=1}^{8}y_{i}^{2}=219, \\| \\mathbf{d} \\|^{2}=\\sum_{i=1}^{8}d_{i}^{2}=2056\\] 이다. 그러나 때때로 우리는 \\(\\| \\mathbf{y} \\|^{2}=\\| \\mathbf{d} \\|^{2}\\)가 되도록 만들고 싶어한다. 이를 해결해 줄 수 있는 것이 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform)이다. 2.4 신호처리에서의 필터(filter in signal processing) 이 절의 내용은 위키피디아의 것을 참조하였다. 신호처리에서 필터(filter)란 신호에서 원하지 않는 특징들을 제거해주는 장치다. 필터는 linear or non-linear time-invariant or time-variant causal or not-causal: depending if present output depends or not on “future” input; of course, for time related signals processed in real-time all the filters are causal; it is not necessarily so for filters acting on space-related signals or for deferred-time processing of time-related signals. analog or digital discrete-time (sampled) or continuous-time passive or active type of continuous-time filter infinite impulse response (IIR) or finite impulse response (FIR) type of discrete-time or digital filter. 등으로 구분할 수 있다. 다음은 선형 필터(linear filter)에서 쓰이는 용어들이다. 여기서 원하지 않는 특징들은 주로 주파수를 의미하는 것이라 볼 수 있다. Figure 2.4: Various forms of linear filters. Low-pass filter – 낮은 주파수는 통과시키고(low frequencies are passed), 높은 주파수는 감쇠, 즉 약하게 한다(high frequencies are attenuated). High-pass filter – 높은 주파수는 통과시키고(high frequencies are passed), 낮은 주파수는 감쇠하도록 한다(low frequencies are attenuated). Band-pass filter – only frequencies in a frequency band are passed. Band-stop filter or band-reject filter – only frequencies in a frequency band are attenuated. Notch filter – rejects just one specific frequency - an extreme band-stop filter. Comb filter – has multiple regularly spaced narrow passbands giving the bandform the appearance of a comb. All-pass filter – all frequencies are passed, but the phase of the output is modified. Cutoff frequency is the frequency beyond which the filter will not pass signals. It is usually measured at a specific attenuation such as 3 dB. Roll-off is the rate at which attenuation increases beyond the cut-off frequency. Transition band, the (usually narrow) band of frequencies between a passband and stopband. Ripple is the variation of the filter’s insertion loss in the passband. 2.5 R 예제(R-multiscale) 웨이블릿과 관련된 R 예제를 담고 있는 책은 (G. Nason 2010)이 있다. 이 책의 저자는 wavethresh란 R 패키지를 만들기도 했다. 또 다른 R 패키지로 waveslim이라는 것도 있다. 여기서는 (G. Nason 2010)의 예제를 일부 다뤄보기로 한다. wavethresh 라이브러리를 실행시킨 상황에서, 다음 벡터의 웨이블릿 변환을 실행해본다. wd라는 함수가 이를 가능케 해 준다. wr이라는 함수는 reconstruction을 시킨다. y &lt;- c(1,1,7,9,2,8,8,6) ## wd: wavelet transform ywd &lt;- wd(y, filter.number=1, family=&quot;DaubExPhase&quot;) names(ywd) &gt; [1] &quot;C&quot; &quot;D&quot; &quot;nlevels&quot; &quot;fl.dbase&quot; &quot;filter&quot; &quot;type&quot; &gt; [7] &quot;bc&quot; &quot;date&quot; ## what filter produced a particular wavelet decomposition object ywd$filter &gt; $H &gt; [1] 0.7071068 0.7071068 &gt; &gt; $G &gt; NULL &gt; &gt; $name &gt; [1] &quot;Haar wavelet&quot; &gt; &gt; $family &gt; [1] &quot;DaubExPhase&quot; &gt; &gt; $filter.number &gt; [1] 1 ## level 2 detail coefficients accessD(ywd, level=2) &gt; [1] 0.000000 -1.414214 -4.242641 1.414214 ## plot wavelet decomposition coefficients plot(ywd) Figure 2.5: Wavelet decomposition coefficients. &gt; [1] 7 7 7 ## wr: wavelet reconstruction wr(ywd) &gt; [1] 1 1 7 9 2 8 8 6 References "],
["wavelettransform.html", "Chapter 3 웨이블릿 변환 3.1 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform) 3.2 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념 3.3 섬세한 척도 근사(fine-scale approximation) 3.4 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale) 3.5 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름) 3.6 웨이블릿의 종류들(types of wavelets)", " Chapter 3 웨이블릿 변환 웨이블릿은 ’Wave’와 프랑스어 ’let’의 합성어로, ’let’은 ’small’이라는 뜻을 가지고 있다. 즉 웨이블릿은 ’small wave’라는 뜻으로, 컴팩트 받침(compactly supported)인 함수들을 일컷는 말이다 . 사인(Sine), 코사인(cosine) 기저(basis)는 \\((-\\infty, \\infty)\\)에서 정의되는 매우 큰 파동이므로 웨이블릿에 해당하지 않는다. 웨이블릿 변환(wavelet transform)이란 웨이블릿 기저함수를 이용해 데이터를 변환하는 것을 말한다. 여기서 웨이블릿 기저함수라는 건 적분하면 0이 되고, 진동하면서 진폭이 0으로 수렴하는 함수를 말한다. 3.1 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform) 가장 단순한 웨이블릿 변환으로 Haar 웨이블릿 변환(Haar wavelet transform)이 있다. 앞서 상세와 성김은 다음과 같이 구할 수 있었음을 상기하자. \\[d_{k}=y_{2k}-y_{2k-1}, c_{k}=y_{2k}+y_{2k-1}.\\] 에너지를 보존하기 위해 다음과 같이 \\(\\alpha\\)라는 상수를 고려하자. \\[d_{k}=\\alpha(y_{2k}-y_{2k-1}), c_{k}=\\alpha(y_{2k}+y_{2k-1}).\\] 그러면 \\begin{eqnarray*} d_{k}^{2}+c_{k}^{2}&amp;=&amp;\\alpha^{2}(y_{2k}^{2}-2y_{2k}y_{2k-1}+y_{2k-1}^{2}+\\alpha^{2}(y_{2k}^{2}+2y_{2k}y_{2k-1}+y_{2k-1}^{2})\\\\ &amp;=&amp;2\\alpha^{2}(y_{2k}^{2}+y_{2k-1}^{2}) \\end{eqnarray*} 즉 \\(2\\alpha^{2}=1 \\Rightarrow \\alpha=\\frac{1}{\\sqrt{2}}\\)이면 \\(y\\)와 \\(d\\)의 에너지가 보존(conserved)된다. 이렇게 \\[d_{k}=\\frac{1}{\\sqrt{2}}(y_{2k}-y_{2k-1}), c_{k}=\\frac{1}{\\sqrt{2}}(y_{2k}+y_{2k-1}).\\] 하는 것을 표준화(normalization)라고 말하기도 한다. 정리하면 Haar 웨이블릿 변환(Haar wavelet transform)의 이산 웨이블릿 계수(discrete wavelet coefficient) \\(d_{k}\\)는 \\[d_{k}=g_{0}y_{2k}+g_{1}y_{2k-1}=\\sum_{l=-\\infty}^{\\infty}g_{l}y_{2k-l}\\] 이며 여기서 \\[ g_{l} = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=0$} \\\\ -\\frac{1}{\\sqrt{2}} &amp; \\text{if $l=1$}\\\\ 0 &amp; \\text{o.w.}\\\\ \\end{cases} \\] 여기서 \\(g_{l}\\)을 고역 필터(high-pass filter)라고 부른다. 마찬가지로 성김에 대해서도 \\[ c_{k}= \\sum_{l=-\\infty}^{\\infty}h_{l}y_{2k-l}, h_{l} = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=0$}\\\\ \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=1$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] 로 나타낼 수 있고 \\(h_{l}\\)을 저역 필터(low-pass filter)라 부른다. 세부와 성김은 앞서 언급한 피라미드 알고리즘으로 구할 수도 있지만 여기서는 \\(\\mathbf{d}=\\mathbf{Wy}\\)처럼 통계학자들에게 익숙한 행렬 꼴로 바꾸어 표현한다. 행렬을 이용해 웨이블릿 계수를 계산해보자. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)에 행렬 \\(W\\)을 다음과 같이 정의하면 \\[ W = \\begin{bmatrix} \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4}\\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{2} &amp; \\frac{1}{2} &amp; -\\frac{1}{2} &amp; -\\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; -\\frac{1}{2} &amp; -\\frac{1}{2}\\\\ \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4}\\\\ \\end{bmatrix} \\] \\(\\mathbf{d}=(\\frac{21\\sqrt{2}}{2},0,-\\sqrt{2},-3\\sqrt{2},\\sqrt{2},-7,-2,\\frac{3\\sqrt{2}}{2})\\)를 얻을 수 있다. (이 예제에서는 (G. Nason 2010)의 정의를 따라갔다.) 위 예제의 \\(W\\)처럼 Haar 웨이블릿의 \\(W\\)는 정규직교(orthonormal)라는 성질을 갖는데, 정규직교의 의미는 \\(W^{T}W=\\mathbf{I}\\)이다. 그러나 모든 웨이블릿의 \\(W\\)가 정규직교인 것은 아니다. 그리고 \\[\\| \\mathbf{d} \\|^{2}=\\mathbf{d}^{T}\\mathbf{d}=(W\\mathbf{y})^{T}(W\\mathbf{y})=\\mathbf{y}^{T}W^{T}W\\mathbf{y}=\\| \\mathbf{y} \\|^{2}\\] 이 식은 Parseval 등식(Parseval’s identity)에 대응된다. 정규직교인 웨이블릿의 \\(W\\)은 다음과 같은 장점을 갖는다. 다음과 같이 원래 자료와 추정량에 대한 공식이 다음과 같이 주어졌을 때, \\[y=f+\\epsilon, \\epsilon \\sim (\\cdot, \\sigma^{2}I) \\rightarrow d=\\theta +e, Wy=d, Wf=\\theta, W\\epsilon=e\\] 정규직교인 \\(W\\)이면 \\[Var(W\\epsilon)=WVar(\\epsilon)W^{T}=\\sigma^{2}I=Var(\\epsilon)\\] 이다. 즉 원래 자료와 추정량의 분산 구조가 같다. 3.2 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념 \\(p(x)\\)라는 함수가 주어졌을 때, 이것의 압축 및 전이된 버전(scaled and translated version)은 다음과 같이 정의된다. \\[ \\| p_{j,k}(x) \\|^{2}=\\int_{\\infty}^{\\infty}p_{j,k}^{2}(x)dx=\\int_{\\infty}^{\\infty}2^{j}p^{2}(2^{j}x-k)dx=\\int_{\\infty}^{\\infty}p^{2}(y)dy=\\| p(y) \\|^{2} \\] \\[p_{j,k}(x)=2^{\\frac{j}{2}}p(2^{j}x-k)\\] 3.3 섬세한 척도 근사(fine-scale approximation) 다음과 같이 Haar 함수(Haar function)를 정의한다. \\[ \\phi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,1]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] Figure 3.1: Plot of Haar function. 이 때 \\(\\phi(x)=\\phi_{0,0}(x)\\)이다. 즉 척도도 바꾸지 않고 어떤 전이(translation)도 없을 때의 \\(\\phi\\)인 것이다. Figure 3.2: Scaling and translation version of Haar function. 가장 성긴 레벨의 (Haar) 척도 웨이블릿(scaling wavelet, father wavelet)은 \\[c_{J,k}=\\int f(x) \\phi_{J,k}(x)dx=\\int f(x) 2^{\\frac{J}{2}}\\phi(2^{J}x-k)dx\\] 이다. 이것은 앞서 말한 \\(p\\)를 \\(\\phi\\)로 바꾸면 되며 또한 데이터와 같음을 알고 있다. 그리고 앞의 정의들을 이용하면 \\[ \\phi_{J,k} = \\begin{cases} 2^{\\frac{J}{2}} &amp; \\text{if $x \\in [2^{-J}k,2^{-J}(k+1)]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] 이다. 여기서 정의하는 간격을 \\(I_{J,k}\\)라 한다. 우리는 다양한 척도에서 \\(f\\)를 근사할 수 있다. 가장 섬세한 척도로는 \\[f_{J}=\\sum_{k=0}^{2^{J}-1}c_{J,k}\\phi_{J,k}(x)\\] 가 있으며, 가장 성긴 척도로 근사하고 싶으면 \\[f_{0}=\\sum_{k=0}^{2^{0}-1}c_{0,k}\\phi_{J,k}(x)\\] 로 \\(f\\)를 근사한다. 3.4 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale) 앞에서 말한대로, 우리는 섬세한 척도의 계수들로부터 좀 더 성긴 척도의 계수들을 구할 수 있다. \\begin{eqnarray*} c_{J-1,k}&amp;=&amp;\\int_{2^{-(J-1)}k}^{2^{-(J-1)}(k+1)}f(x)\\phi_{J-1,k}(x)dx\\\\ &amp;=&amp;\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{(\\frac{J-1}{2})}\\phi(2^{J-1}x-k)dx\\\\ &amp;=&amp;2^{-\\frac{1}{2}}\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J-1}x-k)dx\\\\ &amp;=&amp;2^{-\\frac{1}{2}}[\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J}x-2k)dx + \\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J}x-2k-1)dx]\\\\ &amp;=&amp;2^{-\\frac{1}{2}}(c_{J,2k}+c_{J,2k+1}).\\\\ \\end{eqnarray*} 즉 \\({J-1}\\)척도 계수는 \\(J\\)척도 계수로부터 구할 수 있다. 여기서 중간에 \\[\\phi(x)=\\phi(2x)+\\phi(2x-1)\\] 이라는 사실을 이용하였는데, 이것은 매우 중요하다. 이것을 척도방정식(two-scale relationship) 또는 팽창방정식(dilation relationship)이라고 한다. Figure 3.3: Relationship between Haar scale function. 3.5 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름) 척도 근사들의 차이에서 도출된 함수들이 작은 파도(small wave) 형태를 띄므로 이것을 웨이블릿이라 부른다. 앞서 근사식 \\(f_{J}=\\sum_{k=0}^{2^{J}-1}c_{J,k}\\phi_{J,k}(x)\\)과 \\(p_{j,k}(x)=2^{\\frac{j}{2}}p(2^{j}x-k)\\)으로부터 \\(J=1\\)일 때에는 \\[f_{1}(x)=c_{10}\\phi_{10}(x)+c_{11}\\phi_{11}(x)=c_{10}2^{\\frac{1}{2}}\\phi(2x)+c_{11}2^{\\frac{1}{2}}\\phi(2x-1) \\] 이다. 여기서 \\(f_{0}(x)=c_{00}\\phi_{00}(x)\\)라 하고 \\(c_{J-1,k}=\\frac{c_{J,2k}+c_{J,2k-1}}{\\sqrt{2}}\\)를 이용하면 \\begin{eqnarray*} f_{1}(x)-f_{0}(x)&amp;=&amp;c_{10}\\phi_{10}(x)+c_{11}\\phi_{11}(x)-c_{00}\\phi_{00}(x)\\\\ &amp;=&amp;c_{10}2^{\\frac{1}{2}}\\phi(2x)+c_{11}2^{\\frac{1}{2}}\\phi(2x-1)-c_{00}(\\phi(2x)+\\phi(2x-1))\\\\ &amp;=&amp;(c_{10}2^{\\frac{1}{2}}-c_{00})\\phi(2x)+(c_{11}2^{\\frac{1}{2}}-c_{00})\\phi(2x-1)\\\\ &amp;=&amp;(\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\phi(2x)-(\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\phi(2x-1)\\\\ &amp;=&amp;(\\frac{c_{10}-c_{11}}{\\sqrt{2}})(\\phi(2x)-\\phi(2x-1))\\\\ &amp;=&amp;d_{00}\\psi(x).\\\\ \\end{eqnarray*} 여기서 \\((\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\)은 웨이블릿 상수에 해당하고, \\(\\psi(x)=\\phi(2x)-\\phi(2x-1)\\)은 Haar 모웨이블릿(Haar mother wavelet)이라고 한다. \\[ \\psi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,\\frac{1}{2})$}\\\\ -1 &amp; \\text{if $x \\in [\\frac{1}{2},1)$}\\\\ 0 &amp; \\textrm{o.w.} \\end{cases} \\] Figure 3.4: Haar mother wavelet function. 우리는 웨이블릿을 가지고 어떤 함수를 분해(decompose)할 수 있다. 앞의 결과는 \\[f_{1}(x)=f_{0}(x)+d_{00}\\psi(x)=c_{00}\\phi(x)+d_{00}\\psi(x)\\] 로 쓸 수 있으며 \\(f_{1}\\)을 좀 더 성긴 척도함수인 \\(f_{0}\\)과 차이(difference)에 해당하는 \\(\\psi(x)\\)로 분해할 수 있음을 보여준다. 일반적으로 \\(f_{j+1}(x)\\)는 다음과 같이 쓸 수 있다. \\begin{eqnarray*} f_{j+1}(x)&amp;=&amp;\\sum_{k=0}^{2^{j}-1}c_{jk}\\phi_{jk}(x)+\\sum_{k=0}^{2^{j}-1}d_{jk}\\psi_{jk}(x)\\\\ &amp;=&amp;f_{j}(x)+g_{j}(x)\\\\ &amp;=&amp;f_{j-1}(x)+g_{j-1}(x)+g_{j}(x)\\\\ &amp;=&amp; \\vdots \\\\ &amp;=&amp;f_{0}(x)+\\sum_{l=0}^{j}g_{l}(x).\\\\ \\end{eqnarray*} 즉 \\(f_{j+1}(x)\\)은 가장 성긴 근사함수인 \\(f_{0}\\)와 각 수준에서의 차이인 \\(g_{l}\\)들의 합으로 표현할 수 있다. 3.6 웨이블릿의 종류들(types of wavelets) 3.6.1 Haar 웨이블릿(Haar wavelet) 다음과 같이 Haar 함수(Haar function)의 정의를 다시 상기하자. \\[ \\phi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,1]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] \\(x\\)를 물리적 영역(physical domain) 또는 시간 영역(time domain, t)이라 생각하면, 시간에 대해 컴팩트 받침(compactly supported)인 함수이다. 우리의 궁금점은 이 함수과 과연 주파수 영역(frequency domain)에서도 컴팩트 받침인가이다. 주어진 함수의 각진동수(angular frequency)를 이용한 유니터리 푸리에 변환(Fourier transform with unitary and angular frequency)은 다음과 같다. \\[\\hat{f}(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}f(x)e^{-i\\omega x}dx\\] 여기서 \\(\\frac{1}{\\sqrt{2\\pi}}\\)는 이 변환을 유니터리 푸리에 변환으로 만들기 위해 곱해지는 상수이다. 유니터리 변환과 푸리에 변환에 대한 보다 자세한 나용은 인터넷을 참조하기 바란다. 앞서 나온 푸리에 변환을 이용해 Haar 함수를 푸리에 변환한 결과는 다음과 같다. \\[\\hat{\\phi}(\\omega)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{i\\omega}{2}}\\text{sinc}(\\frac{\\omega}{2}).\\] 여기서 \\[ \\text{sinc}(\\omega)= \\begin{cases} \\frac{\\sin (\\omega)}{\\omega} &amp; \\text{if $\\omega \\neq 0$}\\\\ 1 &amp;\\text{if $\\omega = 0$} \\end{cases} \\] 이다. \\(\\hat{\\phi}(\\omega)\\)는 \\(| \\omega |^{-1}\\)만큼의 감쇠(decay)를 가지며 꼬리가 굉장히 긴 함수이다. 즉 주파수 영역에서 이 함수는 컴팩트 받침과 거리가 먼 함수가 된다. 시간 영역과 주파수 영역 사이에 불확정성 원리(uncertainty principle)이 있다는 것은 알려진 사실이다. Figure 3.5: Haar function (left) and Haar function in frequency domain after Fourier transform (right). 3.6.2 Shannon 웨이블릿(Shannon wavelet) Haar와 반대로 주파수 영역에서 컴팩트 받침인 웨이블릿을 생각할 수 있다. 이것의 대표적인 예가 Shannon 웨이블릿(Shannon wavelet)이다. 다음과 같은 모웨이블릿(mother wavelet)을 생각하자. 그리고 그것의 푸리에 변환은 \\[\\psi(x)=\\frac{\\sin(2\\pi x)-\\cos(\\pi x)}{\\pi (x - \\frac{1}{2})} \\rightarrow \\hat{\\psi}(\\omega)=-e^{-\\frac{i\\omega}{2}}I_{[-2\\pi,-\\pi]\\cup[\\pi,2\\pi]}(\\omega).\\] 따라서 주파수 영역에서는 컴팩트 받침이다. Shannon의 부웨이블릿(father wavelet)와 대응되는 푸리에 변환은 다음과 같다. \\[\\phi(x)=\\text{sinc}(\\pi x) \\rightarrow \\hat{\\phi}(\\omega)=1.\\] 즉 주파수 영역에서는 컴팩트 받침이다. 한편 시간 영역에서 \\(\\phi(x)\\)의 그림은 다음과 같다고 한다(출처: 위키, 체크 필요) Figure 3.6: Real Shannon wavelet function. 3.6.3 Meyer 웨이블릿(Meyer wavelet) Shannon 웨이블릿과 유사하며, 주파수 영역에서 상자 함수(box function)를 약간 부드럽게 한 형태의 웨이블릿이 Meyer 웨이블릿(Meyer wavelet)이다. Figure 3.7: Scaling function and wavelet function of Meyer wavelet on time domain. Figure 3.8: Meyer scaling function on frequency domain. References "],
["waveletshrinkage.html", "Chapter 4 웨이블릿 수축 4.1 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage) 4.2 오라클(oracle) 4.3 만능 임계화(universal thresholding) 4.4 Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE)) 4.5 R 예제(R-waveletshrinkage)", " Chapter 4 웨이블릿 수축 이 장의 주된 내용과 그림들은 (G. Nason 2010)를 참고하였다. 다음과 같이 자료를 관찰하는 도메인(domain)인 physical domain (physical model)에서의 모델 \\(y_{i}=g(x_{i})+e_{i}, i=1,\\cdots,n\\)에서 관측한 길이 \\(n\\)의 자료 \\(\\mathbf{y}=(y_{1},\\cdots,y_{n})^{T}\\)이 있다고 하자. 여기서 \\(x_{i}=\\frac{i}{n} \\text{ (designed point)}\\)이라고 하자. 이 공간(space)은 equally-spaced이고 \\(x \\in (0, 1]\\)이다. 우리의 목표는 알려지지 않은 함수 \\(g(x), x \\in [0,1]\\)를 추정하는 것이다. 일반적으로 \\(e_{i} \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\\)으로 가정한다. 독립 동일 분포 가정(independent and identically distributed, iid)이 없으면 모형이 좀 더 복잡해진다. 정규분포(normal distribution, Gaussian distribution) 가정도 중요한데, 정규분포처럼 대칭(symmetric)인 분포를 가정하지 않을 경우 평균 추정이 힘들어지므로 보통 분위수(quantile) 추정을 하게 된다. 우리가 얻는 자료 \\(\\mathbf{y}\\)가 noise가 전혀 없는 순수한 signal이라고 하면, wavelet transform후 바로 wavelet reconstruction을 통해 원래 자료를 얻을 수 있다. \\[\\mathbf{y} \\xrightarrow{W} \\boldsymbol{\\delta} \\xrightarrow{W^{-1}} \\mathbf{y}\\] 그런데 자료에 잡음(noise)이 있는 경우 얘기가 좀 달라진다. \\[\\mathbf{d}=\\mathbf{Wy} =\\mathbf{Wg}+\\mathbf{We}=\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}\\] 이 경우에는 위와 같은 방법을 적용하면 잡음이 낀 신호가 그대로 나오게 된다. 우리는 적당한 방법을 통해 noise가 거의 없는 \\(\\hat{\\mathbf{d}}\\)를 추정해 \\(\\mathbf{W}^{-1}\\hat{\\mathbf{d}} \\rightarrow \\hat{\\mathbf{g}}\\)를 하고 싶다. 이럴 떄 쓰는 방법이 임계화(thresholding)이다. 4.1 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage) 다시 원래 얘기로 돌아가서 우리는 웨이블릿 변환을 통해 \\(g\\)를 추정하고자 한다. \\(\\mathbf{W}\\)를 이산 웨이블릿 변환 연산자(연산자)라고 하면, 다음과 같은 웨이블릿 변환을 생각해 볼 수 있다. \\[\\mathbf{y}=\\mathbf{g}+\\mathbf{e} \\rightarrow \\mathbf{Wy} =\\mathbf{Wg}+\\mathbf{We} \\text{ or } \\mathbf{d}=\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}.\\] 웨이블릿 변환 연산자는 physical domain에 있는 자료를 wavelet domain (model in the wavelet domain, wavelet-transformed model or wavelet model)으로 보내주는 역할을 한다. 웨이블릿 변환은 정규직교(orthonormal)이므로 변환된 오차항 또한 \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2}\\mathbf{I})\\)로 정규분포를 따르는 좋은 성질을 가진다. 또 웨이블릿 변환은 오차(error(가 약하게 correlated (stationary process)된 경우 웨이블릿 변환을 하면 변환된 오차가 de-correlated(whitening, 더 약하게 correlated되는 것)되는 좋은 성질이 있다. 웨이블릿 수축(wavelet shrinkage)을 위해 알아두어야 할 컨셉들은 다음과 같다. \\(\\boldsymbol{\\theta}\\)는 많은 함수들의 성긴 벡터(sparse vector)이다. 그리고 \\(\\boldsymbol{\\theta}\\)는 다음과 같이 Parseval’s identity를 만족시킨다. 즉 데이터의 에너지와 계수들의 에너지가 같다(보존된다). \\[\\sum g^{2}(x)=\\sum \\theta_{i}^{2}.\\] \\(\\boldsymbol{\\theta}\\)는 “concentrated”되어있다. \\(\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2}\\mathbf{I})\\), 즉 웨이블릿 계수 \\(\\mathbf{d}\\)에는 \\(\\boldsymbol{\\theta}\\)뿐 아니라 \\(\\mathbf{\\epsilon}\\)의 정보도 들어있다. 위의 사실에 기초하여 \\(\\mathbf{d}\\)중 값이 큰 원소의 경우에는 진짜 신호 + 잡음의 형태로 이루어져 있을 것이다. \\(\\mathbf{d}\\)중 값이 작은 원소의 경우에는 잡음만 있을 것이다. 이런 상황에서는 평균이 틀리게 된다. 즉 \\(\\hat{\\theta}=\\frac{1}{n}\\sum_{i=1}^{n}d_{i}\\)가 \\(\\theta\\)의 좋은 추정량이 될 수 없다는 것이다. 그래서 이를 해결하기 위해 도입된 아이디어가 임계화(thresholding)이다. 임계화가 등장하기 전까지 모든 추정량에는 평균 개념이 있었다. (ex. Ridge) 기존의 자료분석들은 “aggregation”에 치중했다. 모든 변수에 다 신호가 존재한다고 생각한 것이다. 이런 방식으로는 위의 문제를 해결할 수 없다. 그러나 웨이블릿 변환의 등장 이후에는 “sparsity” 개념이 등장하였고 몇 개의 신호만 선택하게 되었다. 이 개념 덕분 에 고차원(high-dimensional) 자료(\\(n \\ll p\\))를 분석할 수 있게 되었다. 수축 방법에는 두 가지가 있다. 하드 임계화(hard thresholding)와 소프트 임계화(soft thresholding)가 그것이다. 두 임계화를 다음과 같이 정의한다. 주어진 (empirical) 웨이블릿 상수 d와 threshold \\(\\mathbf{\\lambda}\\)가 있을 때, 그것의 하드 임계화(hard thresholding)는 \\[\\hat{\\theta}_{H}=\\eta_{H}(d,\\lambda)=d\\mathbb{I}\\{ |d| &gt; \\lambda \\}\\] 이다. 그리고 소프트 임계화(soft thresholding)는 \\[\\hat{\\theta}_{S}=\\eta_{S}(d,\\lambda)=\\text{sgn}(d)(|d|-\\lambda)\\mathbb{I}\\{ |d| &gt; \\lambda \\}\\] 이다. Figure 4.1: Hard thresholding (dotted line) and soft thresholding. 두 방법 다 공통적으로 \\(\\mathbf{d} \\in (-\\lambda, \\lambda)^{n}\\)이면 0이 된다. 하드 임계화는 “keep or kill” 방법이라고도 불린다. 그 이유는 값이 어떤 threshold(\\(\\mathbf{\\lambda}\\))보다 작을 경우 무조건 0으로 놓기 때문이다. 이것은 회귀분석의 변수 선택(variable selection)과 동일한 아이디어이다. 변수 선택에서도 변수를 넣기 또는 빼기 두 가지 선택지만 있다는 것을 생각하기 바란다. 그리고 하드임계화에서는 축소를 하지 않는다. 소프트 임계화는 하드 임계화를 함과 동시에 신호 변환 함수가 연속이 되도록 값이 큰 signal도 같이 축소(shrinkage)하는 방법이다. 이는 변수선택에서 LASSO와 대응되는 방법이다. 때때로 굉장히 큰 \\(\\mathbf{d}\\)에는 오차가 작게 들어있을 것이라 생각할 수도 있다. 이를 보완하기 위해 SCAD 같은 방법들이 나중에 제안되었는데, 원래 이는 웨이블릿을 연구하는 학자들이 생각했던 개념으로 이를 통계학 언어로 옮긴 것에 불과하다. 여기서 등장하는 \\(\\lambda\\)는 핵평활(kernel smoothing)이나 평활 스플라인(smoothing spline)에서 나오는 띠너비(bandwidth)와 비슷한 개념이라고 생각하면 된다. \\(\\lambda\\)의 선택 또한 중요한 이슈가 된다. 이것을 어떻게 선택하느냐에 따라 performance가 굉장히 변하고 \\(\\hat{g}\\)의 질(quality)에 영향을 미친다. \\[y \\xrightarrow{W} d \\xrightarrow{\\text{th}} \\hat{\\theta}_{Shrink} \\xrightarrow{W^{-1}} \\hat{g}.\\] 4.2 오라클(oracle) 만약 우리가 \\(g\\)를 알고 있다면, \\(\\hat{g}\\)의 quality를 계산하는 방법 중 하나로 다음과 같은 적분제곱오차(integrated squared error, ISE, \\(\\hat{M}\\))를 생각해 볼 수 있다. \\[\\hat{M}=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{g}(x_{i})-g(x_{i}))^{2}.\\] 그러나 우리는 \\(g\\)를 모르기 때문에 실제로 ISE를 계산할 수는 없다. 대신 ‘평균(average)’ 개념을 적용한 평균적분제곱오차(mean integrated squared err, MISE) \\(E(\\hat{M})\\)을 정의한다. \\[ M \\triangleq E(\\hat{M})=\\text{Risk of }\\hat{g}.\\] 웨이블릿에서 \\(\\hat{g}\\)는 \\(\\lambda, \\eta, \\theta\\)에 좌우(depend)한다. 참고로 보통 \\(g\\)가 정의되는 함수공간 \\(g \\in \\mathcal{F}\\)은 일반적으로 \\(L^{2}(\\mathbb{R})\\)에서만 생각한다. 웨이블릿은 점프가 있는 함수도 다룰 수 있긴 하다. 결국 통계적 추정의 목표는 이 MISE를 최소화하는 \\(\\hat{g}\\)를 찾는 것이다. 웨이블릿에서 \\(\\hat{M}=\\sum_{j,k}(\\hat{\\theta}_{jk}-\\theta_{jk})^{2}\\)이다. 여기서 웨이블릿 변환은 정규직교이므로 ’decoupling’이라는 성질을 이용할 수 있다. 이 얘기는 위의 값을 계산할 때 \\(j,k\\)를 무시하고 마치 하나만 있는 것처럼 계산해도 된다는 것이다. 마치 벡터(vector)를 스칼라(scalar)처럼 볼 수 있다는 것이다. 잠시 선형 회귀분석 모형을 복습해보자. 다음과 같은 선형 회귀분석 모형 \\[\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\] 이 있다고 하자. 여기서 \\(\\mathbf{y}\\)는 \\(n \\times 1\\) 행렬, \\(\\mathbf{X}\\)는 \\(n \\times d\\) 행렬, \\(\\boldsymbol{\\beta}\\)는 \\(d \\times 1\\) 행렬, 그리고 \\(\\boldsymbol{\\epsilon}\\)은 \\(n \\times 1\\) 행렬이다. 만약 여기서 \\(X\\)의 열(column)이 정규직교라고 해보자. 그러면 \\[\\begin{eqnarray*} \\hat{\\boldsymbol{\\beta}}&amp;=&amp;(\\hat{\\beta}_{1},\\cdots,\\beta_{d})^{T}=(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}=\\mathbf{X}^{T}\\mathbf{y}\\\\ &amp;\\Longrightarrow&amp; \\hat{\\beta}_{1}=\\sum X_{i1}y_{i}, \\hat{\\beta}_{2}=\\sum X_{i2}y_{i}, \\cdots \\end{eqnarray*} \\] 로 모든 \\(\\boldsymbol{\\beta}\\)의 원소들이 separate(decoupled)되는 것을 볼 수 있다. 참고로 \\((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\neq \\mathbf{I}\\)인 경우 \\(\\hat{\\beta}\\)가 다 연결되므로 이렇게 분석할 수 없다. 그리고 \\[\\hat{\\boldsymbol{\\beta}}=\\min \\| \\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta} \\|^{2} \\Leftrightarrow \\min \\| \\mathbf{X}^{T}\\mathbf{y} - \\mathbf{X}^{T}\\boldsymbol{\\beta} \\|^{2}=\\min \\| \\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta} \\|^{2}\\] 가 된다. (Donoho and Johnstone 1994) 논문에 갑자기 이 사실을 이용해 전개하는 내용이 있다. 더 나아가 벌점화 최소자승법(penalized least square) 문제를 생각해보자. \\(\\mathbf{z}=\\mathbf{X}^{T}\\mathbf{y}\\), \\(\\hat{\\mathbf{y}}=\\mathbf{Xz}=\\mathbf{XX}^{T}\\mathbf{y}\\)를 정의하면 \\begin{eqnarray*} \\| \\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\|^{2}+\\lambda \\sum_{j=1}^{d}P(| \\beta_{j} |) &amp;=&amp; \\| \\mathbf{y}-\\hat{\\mathbf{y}}+\\hat{\\mathbf{y}}-\\mathbf{X}\\boldsymbol{\\beta}\\|^{2}+\\lambda \\sum_{j=1}^{d}P(|\\beta_{j}|)\\\\ &amp;=&amp;\\| \\mathbf{y} -\\hat{\\mathbf{y}} \\|^{2} + \\sum_{j}(z-{j}-\\beta_{j})^{2}=\\lambda\\sum_{j}P(|\\beta_{j}|)\\\\ \\end{eqnarray*} 여기서 \\(\\| \\mathbf{y} -\\hat{\\mathbf{y}} \\|^{2}\\)는 \\(\\beta_{j}\\)와 관련 없으므로 뒤의 두 항만 최소화(minimize)하면 된다. 그런데 \\(\\beta_{j}\\)는 seperate되므로 벌점화 최소자승법의 해는 \\[\\hat{\\beta}=\\min_{\\beta}(z-\\beta)^{2}+\\lambda P(| \\beta |)\\] 이다. 다시 웨이블릿 문제로 돌아가서, 웨이블릿 변환 행렬 \\(\\mathbf{W}\\) (회귀분석에서 \\(\\mathbf{X}\\)와 같은 역할을 함)이 정규직교이므로, 우리는 \\(E(\\hat{\\theta}-\\theta)^{2}\\)(=risk)만 보면 된다. 참고로 \\(\\mathbf{W}\\)는 정방행렬(square matrix)이라는 점에서 \\(\\mathbf{X}\\)와 다르다. Separated 성질에 의해 \\[ M(\\hat{\\theta},\\theta)=E(\\hat{\\theta}-\\theta)^{2} = \\begin{cases} E(d-\\theta)^{2}=E\\epsilon^{2} &amp; \\text{if $|d| &gt; \\lambda$}\\\\ E(\\theta^{2})=\\theta^{2} &amp; \\text{o.w.} \\end{cases} \\] 이 된다. 결론적으로 임계화(thresholding)를 위해서는 신호와 오차의 크기를 비교해 보면 되는데, 만약 신호가 오차보다 굉장히 큰 경우, \\(\\theta \\gg \\sigma\\)인 경우면 우리는 \\(|d| &gt; \\lambda\\)인 경우를 취하는게 유리하므로 \\(\\lambda\\)를 작게 선택하면 된다. 반대의 경우에는 \\(\\lambda\\)를 크게 취하는 것이 유리하다. 통계학에서 오라클이라는 개념을 처음 사용한 사람은 Dave Donoho이다. 오라클이라는 개념이 처음 등장하는 논문은 (Donoho and Johnstone 1994)인데, 오라클을“With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel to the unknown function”이라고 소개하고 있다. 교수님의 요약은 다음과 같다. “The oracle is notional device that tells you which coefficients you should select.” 오라클에 의한 ideal risk는(hard thresholding의 경우) \\(M_{ideal}=\\sum_{j,k}\\min(\\theta_{j,k}^{2},\\sigma^{2})\\)이다. 그렇다면 \\(\\hat{\\theta}\\)를 어떻게 구하는가? 이 문제는 결국 \\(\\eta_{H}, \\eta_{S}, \\lambda\\)를 선택하는 문제로 귀착된다. Donoho와 Johnstone은 \\(M_{ideal}\\approx M\\)이 되게 하는 \\(\\hat{\\theta}\\)를 몇 가지 제시하였다. (Donoho and Johnstone 1994)에서 \\(\\hat{\\theta}=\\eta_{x}(d,\\lambda)\\), \\(\\lambda=\\sigma\\sqrt{2\\log n}\\)으로 할 시 \\[M_{\\text{universal}}\\leq(2\\log n +1)(\\sigma^{2}+M_{\\text{ideal}})\\] 임을 증명하였다. 다시 말하면 이 \\(\\hat{\\theta}\\)가 오라클 성질(oracle property)과 굉장히 유사하며 \\(M_{\\text{ideal}}\\)에 가깝게(대략\\(2\\log n\\)배 보다 작다) 행동한다는 것이다. 위 논문에 따르면 핵평활(kernel smoothing)이나 평활 스플라인(smoothing spline)도 \\(2\\log n\\)을 만족하지 못한다(n). 가장 이상적인 fitting은 정확한 knot point들을 모두 알고 있는 piecewise polynomial이다. 그러나 ideal한 knot을 모두 안다는 것은 true을 안다는 것이므로 이는 불가능하다. Bandwidth나 knot selection을 잘 한다는 것은 true의 분산을 안다는 것과 거의 같은 얘기다. 4.3 만능 임계화(universal thresholding) 앞서 등장한 \\(\\lambda^{u}=\\sigma \\sqrt{2 \\log n}\\)을 특별히 만능 임계화(universal Thresholding)라고 한다. 실제로는 \\(\\sigma\\)를 모르므로 \\(\\hat{\\sigma}\\)를 사용한다. \\(X_{1},\\cdots , X_{n}\\)을 \\(EX_{i}=0, EX_{i}^{2}=1, EX_{i}X_{i+k}=\\gamma(k)\\)인 stationary Gaussian process (Lag-k covariance structure를 갖는 Gaussian process)라고 하고 특별히 \\(X_{(n)}=\\max \\{ X_{i} \\}\\)라 하자. 만약 \\(\\lim_{k \\rightarrow \\infty} \\gamma (k) =0\\)이면, \\[\\frac{X_{(n)}}{\\sqrt{2 \\log n}} \\rightarrow 1 \\text{ as } n \\rightarrow \\infty\\] 이다. 위 정리는 n Gaussian 확률 변수들 중 가장 큰 것은(독립일 필요는 없다) 대략 \\(\\sqrt{2 \\log n}\\) 사이즈라는 것이다. 이 정리에 비추어 만능 임계화를 생각하면 이 임계화는 오차(error)가 Gaussian random variable을 따르는 것이라면 모두 다 임계화하겠다는 뜻으로 해석할 수 있다. 이 방법은 이론적으로는 완벽해 보이기는 하나 너무나 많은 잡음(noise)을 줄이는 underfit한 임계화이다. 결국 SURE와 같은 실용적인 임계화 방법을 생각하게 된 것이다. 이 얘기는 추후에 다시 나올 것이다. 만능 임계화로 돌아가서, 우리는 \\(\\sigma\\)를 모르므로 대신 \\(\\hat{\\lambda}^{u}=\\hat{\\sigma}\\sqrt{2 \\log n}\\)을 이용해야 할 것이다. 그렇다면 \\(\\sigma\\)를 어떻게 추정할 것인가? 대부분의 방법은 data를 제외한 가장 finest scale (ex.J-1)의 detail 웨이블릿 계수(\\(d_{J-1}\\))를 이용해 추정한다. \\(y\\)를 이용해 \\(\\epsilon\\)의 분산을 추정하려고 할 경우 \\(f\\)의 정보가 너무 강해서 \\(\\epsilon\\)의 분산구조를 알 수 없을 것이다. 그리고 좀 더 성긴 스케일(coarser scale)로 갈수록 잡음보다는 신호 정보가 많을 것이라는 생각을 하면, \\(d_{J-1}\\)를 이용해 분산 구조를 추정하는 것이 당연하다. 가장 널리 알려진 방법은 \\[\\hat{\\sigma}=\\sqrt{\\frac{1}{n/2-1}\\sum_{k=1}^{n/2}(d_{J-1,k}-\\bar{d_{J-1}})^{2}}\\] 이다. 이 방법은 자료가 희소(sparse)한 경우에는 잘 맞지 않음이 알려져 있다. 그런 경우에는 대신 중앙값(median)을 이용하여 \\[ \\hat{\\sigma}=1.4826 \\times \\text{median}(|d_{J-1,1}-\\tilde{d_{J-1}}|,\\cdots,|d_{J-1,\\frac{n}{2}}-\\tilde{d_{J-1}}|\\\\ \\text{ where } \\tilde{d}_{J-1}=\\text{median}(\\mathbf{d}_{J-1}) \\] 이런 식으로 추정하기도 한다. 지금까지 했던 방법은 universal threshold rule(\\(\\lambda^{u}\\))에 soft thresholding function \\(\\eta_{s}\\)를 적용한 \\(\\hat{\\theta}=\\eta_{s}(d,\\lambda^{u})\\)로 이것을 VisuShrink라고 부른다. 이 방법은 앞서 말한 대로 noise-free reconstrunction이나 oversmooth (underfit)하는 문제가 생긴다. 즉 noise-free하지만 signal도 너무 많이 죽일 가능성이 있다는 것이다. 한편 \\(\\lambda^{u}\\)는 noise-free reconstruction을 하는 최소의 \\(\\lambda\\)이므로, \\(0&lt; \\lambda^{*} \\ll \\lambda^{u}\\)인 \\(\\lambda^{*}\\)를 생각할 수 있을 것이다. 이런 \\(\\lambda^{*}\\) 중의 하나로 (Donoho and Johnstone 1994)에서는 RiskShrink라는 것을 제시했다. 이 방법은 \\(\\Lambda_{n}^{*}(=2 \\log n +1)\\)에 해당하는 \\(\\Lambda_{n}^{*}\\)과 이에 대응되는 \\(\\lambda^{*}\\)을 table 형태로 계산한 것이다. 예를 들어 \\(n=1024\\) 일 때 \\(\\lambda^{u}=3.72, \\lambda^{*}=2.23, \\Lambda_{n}^{*}=5.976\\)이다. 참고로 이 논문의 결과와 더불어 일반적으로 알려져 있는 사실은 함수가 부드럽(smooth)지 않을 때 웨이블릿이 다른 어떤 비모수 방법들보다 좋다는 것이다. 4.4 Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE)) 앞서 얘기했던 VisuShirnk나 RiskShrink는 이론상으로는 완벽하나 실용성이 떨어져 실제로는 많이 쓰이지 않고 있다. 실제로 많이 쓰이는 shrinkage 방법 중 하나가 Steins Unbiased Risk Estimator (SURE)이다. Shrinkage 추정량들은 Bayesian과 밀접한 관련이 있다. Bayesian들이 주로 하는 것은 자료를 prior의 정보에 민감하게 반응하도록 수축(shrinkage)해 주는 것이다. SURE가 처음 등장한 논문은 (Donoho and Johnstone 1995)로, (Stein 1981)의 내용을 웨이블릿 도메인으로 갖고 온 것이다. 다음과 같은 data domain에서의 모델 \\(y_{i}=g(x_{i})+e_{i}, i=1,\\cdots,n,e_{i} \\stackrel{iid}{\\sim} \\mathcal{N}(0,\\sigma^{2})\\)과 이를 웨이블릿 도메인(domain)으로 옮긴 \\(\\mathbf{d}=\\boldsymbol{\\theta}+\\boldsymbol{\\epsilon}, \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0,\\sigma^{2}\\mathbf{I})\\)를 생각하자. Stein의 논문에서는 이 notation을 다음과 같이 썼다. \\[\\mathbf{x}=\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon}.\\] (Stein 1981) 만약 \\(\\hat{\\boldsymbol{\\mu}}\\mathbf{(x)}=\\mathbf{x}+\\mathbf{g(x)}\\), \\(g:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\) is weakly differentiable 조건이면 \\[E \\| \\hat{\\boldsymbol{\\mu}}\\mathbf{(x)}-\\boldsymbol{\\mu} \\|^{2} =n+ E\\{ \\|\\mathbf{g(x)}\\|^{2}+2\\bigtriangledown \\cdot \\mathbf{g(x)} \\}, \\bigtriangledown\\cdot \\mathbf{g}=\\sum_{i} \\frac{\\partial}{\\partial x_{i}}g_{i}\\] 이다. 그리고 \\(\\hat{\\mu}_{i}(\\lambda)=\\eta_{s}(x_{i},\\lambda) \\Rightarrow \\frac{\\partial}{\\partial x_{i}}\\hat{\\mu}_{i}(\\lambda)=I( |x_{i}|&gt;\\lambda)\\)과 \\(\\| \\mathbf{g(x)} \\|^{2}=\\sum \\hat{\\mu}_{i}(\\lambda, x)^{2}=\\sum_{i=1}^{n}(|x_{i}|-\\lambda)^{2}I(|x_{i}&gt;\\lambda)\\) 사실을 이용해 SURE를 정의한다. \\(\\text{SURE}(\\lambda,\\mathbf{x})=n-2\\#\\{ i: |x_{i}| \\leq \\lambda \\} + \\sum_{i=1}^{n}(|x_{i}| \\wedge \\lambda)^{2}\\)는 risk의 불편추정량이다. \\[E \\| \\eta_{s}(\\mathbf{x},\\lambda)-\\mu \\|^{2}=E\\text{SURE}(\\lambda, \\mathbf{x}).\\] 실제로, \\(\\lambda=\\text{argmin}_{0&lt;\\lambda \\leq lambda^{u}}\\text{SURE}(\\lambda,\\mathbf{x})\\)이며, 이 방법을 SUREShrink라고 한다. 4.5 R 예제(R-waveletshrinkage) 다음은 R 패키지 wavethresh를 이용한 축소 예제이다. 임계화를 위해 threshold라는 함수를 사용하며 type 및 policy를 선택할 수 있다. par(mfrow=c(1,3)) set.seed(1234) data_bump &lt;- example.1() x &lt;- data_bump$x; y &lt;- data_bump$y plot(x,y, type=&#39;l&#39;, main=&quot;Original&quot;) y_noise &lt;- y + rnorm(length(y), sd=0.1) plot(x,y_noise, type=&#39;l&#39;, main=&quot;Noisy&quot;) y_noise_wd &lt;- wd(y_noise) y_noise_threshold &lt;- threshold(y_noise_wd, type=&quot;soft&quot;, policy=&quot;sure&quot;) y_sure &lt;- wr(y_noise_threshold) plot(x,y_sure, type=&#39;l&#39;, main=&quot;Soft Thresholding&quot;) Figure 4.2: Wavelet shrinkage example. References "],
["advwaveletshrinkage.html", "Chapter 5 웨이블릿 수축의 고등 논제들 5.1 교차타당성(cross-validation) 5.2 다중 비교(multiple testing) 5.3 베이지안 웨이블릿 축소(Bayesian wavelet shrinkage) 5.4 선형 웨이블릿 평활화(linear wavelet smoothing) 5.5 블록 임계화(block thresholding)", " Chapter 5 웨이블릿 수축의 고등 논제들 이 장의 내용은 앞 장의 내용과 이어진다. 5.1 교차타당성(cross-validation) 다음과 같은 일반적인 모델 \\(y_{i}=f(x_{i})+e_{i}\\)이 있고, \\(f\\)를 회귀적합 \\(f_{lambda}\\)를 통해 추정하려고 한다(\\(\\lambda\\): smoothing parameter). 그렇다면 \\(\\lambda\\)를 어떻게 선택할 것인가? 이를 해결하기 위해 등장한 방법이 교차타당성(cross-validation)이다. 교차타당성의 정의는 다음과 같다. \\[\\text{CV}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}_{\\lambda}^{-i}(x_{i}))^{2}.\\] 여기서 \\(\\hat{f}_{\\lambda}^{-i}(x_{i})\\)는 i번째 자료를 제외하고 \\(f\\)를 적합한 다음 \\(x_{i}\\)의 예측값이다(\\(x_{i}\\)값이 없으므로 추정값이 아니라 예측값이 된다). 그렇다면 왜 교차타당성이 쓰이게 되었는가? 이것을 이해하기 위해서는 Mean squared error (MSE)와 Predicted squared error (PSE)에 대해 알아야 한다. 위와 같은 모형 하에서 MSE와 PSE는 \\[\\text{MSE}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}E(\\hat{f}_{\\lambda}(x_{i})-f(x_{i}))^{2}, \\text{PSE}(\\lambda)=\\frac{1}{n}\\sum_{i=1}^{n}E(y_{i}^{*}-\\hat{f}_{\\lambda}(x_{i}))^{2}\\] 이다. 여기서 \\(y_{i}^{*}\\)는 \\(x_{i}\\)에서의 새로운 관찰값이다. 즉, \\(y_{i}^{*}=f(x_{i})+\\epsilon_{i}^{*}, (\\epsilon_{i}^{*}\\)는 \\(\\epsilon_{i}\\)와 독립)이다. 위의 PSE를 약간 변형해 보면 \\begin{eqnarray*} \\text{PSE}(\\lambda)&amp;=&amp;\\frac{1}{n}\\sum_{i=1}^{n}E(y_{i}^{*}-\\hat{f}_{\\lambda}(x_{i}))^{2}\\\\ &amp;=&amp;\\frac{1}{n}\\sum_{i=1}^{n}E(y_{i}^{*}-f(x_{i}))^{2}+\\frac{1}{n}\\sum_{i=1}^{n}E(f(x_{i})-\\hat{f}_{\\lambda}(x_{i}))^{2}\\\\ &amp;=&amp;\\sigma^{2}+\\text{MSE}(\\lambda). \\end{eqnarray*} 중간에 두 항은 독립이라고 보고 cross-product term을 생략하였다. 이 전개는 회귀분석에서 prediction interval이 커지는 것과 일맥상통한다. 한편, CV의 기댓값은 \\begin{eqnarray*} E(y_{i}-\\hat{f}_{\\lambda}^{-i}(x_{i}))^{2}&amp;=&amp;E(y_{i}-f(x_{i})+f(x_{i})-\\hat{f}_{\\lambda}^{-i}(x_{i}))^{2}\\\\ &amp;=&amp;E(y_{i}-f(x_{i}))^{2}+E(f(x_{i})-\\hat{f}_{\\lambda}^{-i}(x_{i}))^{2}+2E(y_{i}-f(x_{i}))(f(x_{i})-\\hat{f}_{\\lambda}^{-i}(x_{i}))\\\\ &amp;=&amp;\\sigma^{2}+E(f(x_{i})-\\hat{f}_{\\lambda}^{-i}(x_{i}))^{2}.\\\\ \\end{eqnarray*} 만약 \\(\\hat{f}_{\\lambda}^{-i}(x_{i}) \\approx \\hat{f}_{\\lambda}(x_{i})\\)이면 \\(E(\\text{CV})=\\text{PSE}(\\lambda)\\)이고 \\(\\min_{\\lambda}E(\\text{CV}) \\approx \\min_{\\lambda}\\text{PSE}(\\lambda) \\approx \\min_{\\lambda}\\text{MSE}(\\lambda)\\)이다. 물론 \\(\\min_{\\lambda}E(\\text{CV}) \\neq \\min_{\\lambda}\\text{CV}\\)이나 아주 틀린 생각은 아니다. (G. P. Nason 1996)에서는 웨이블릿에서 교차타당성을 하기 위한 몇 가지 방법을 제시했다. 첫 번째 방법은 하나의 자료 대신 절반의 자료(\\(\\frac{n}{2}\\))를 제거하는 이다. 다음과 같은 \\(y_{1}, \\cdots, y_{n}, y_{i}=g(x_{i})+\\epsilon_{i}, n=2^{J}\\)이라는 자료가 있다고 가정하자. 그러면 two-fold CV를 하는 방법은 다음과 같다.\\ \\(\\lambda\\)의 후보군 \\(\\lambda \\in (\\lambda_{L}, \\lambda^{U})\\)를 설정한다.\\ 먼저 모든 홀수번째 항의 \\(y_{i}\\)를 제거하고 남은 \\(y_{j}\\)에 대해 re-index를 한다(\\(y_{j},j=1,\\cdots,\\frac{n}{2}\\)).\\ 웨이블릿 축소를 이용해 \\(y_{j},j=1,\\cdots,\\frac{n}{2}\\)로부터 \\(\\hat{g}^{E}\\)를 얻는다(이 때 bound problem이 생기므로 bound treatment를 해 줘야 한다).\\ \\[\\mathbf{y} \\xrightarrow{\\text{DWT}} \\mathbf{d} \\text{ (thresholding $\\lambda$) } \\xrightarrow{\\text{IDWT}} \\hat{g}^{E}\\] Even-index 자료를 가지고 odd index의 함수값을 예측하기 위해 다음과 같은 예측값 \\(\\bar{g}_{\\lambda,j}^{E}=\\frac{(\\hat{g}_{\\lambda,j+1}^{E}+\\hat{g}_{\\lambda,j}^{E})}{2}, j=1,2,\\cdots,\\frac{n}{2}\\)를 계산한다.\\ 비슷한 방법으로 \\(\\bar{g}_{\\lambda,j}^{O}\\)를 계산한다.\\ \\(\\hat{M}(\\lambda)=\\sum_{j=1}^{\\frac{n}{2}}\\{(\\bar{g}_{\\lambda,j}^{E}-y_{2j+1})^{2}+(\\bar{g}_{\\lambda,j}^{O}-y_{2j})\\}^{2}\\)를 계산한다. 여기서 앞 항은 even-index 자료를 가지고 odd 자료를 예측한 것이고, 뒤 항은 odd-index 자료를 가지고 even 자료를 예측한 것이다.\\ \\(\\hat{M}(\\lambda)\\)가 제일 작은 \\(\\lambda^{*}=\\text{argmin}_{\\lambda \\in (\\lambda_{L},\\lambda^{U})} \\hat{M}(\\lambda)\\)를 최종적으로 선택한다. Figure 5.1: Relation between the large number of folds and CV. Figure 5.2: Relation between the small number of folds and CV. Fold 수가 커지면 bias가 줄어드나(더 정밀함) estimator의 variance는 커지고 계산 시간도 길어진다. Fold 수가 작아지면 계산 시간도 작아지고 estimator의 variance는 작아지나 bias는 커진다. 보통 K-fold 방법이 K의 선택은 data-dependent하게 한다. 매우 큰 자료에서는 K=3이어도 정밀하며, 성긴 자료에서는 가능한 한 많은 자료를 training하기 위해 leave-one out cross-validation (LOCV)를 사용하게 된다. 일반적인 선택은 K=10이다. 5.2 다중 비교(multiple testing) 다음과 같은 성긴 웨이블릿 모형 \\(\\mathbf{d}=\\mathbf{\\theta}+\\mathbf{\\epsilon}\\)을 고려하자. 여기서 다음과 같은 여러 개의 귀무가설을 동시에 생각해 볼 수 있다. \\[H_{0}=\\theta_{j,k}=0, \\forall j,k.\\] 이러한 여러 개의 귀무가설을 동시에 생각해보는 문제는 천문학, 뇌과학, 마이크로어레이, 전기공학 등에서 볼 수 있다. 이 문제를 좀 더 일반적으로 설명해보면 다음과 같이 \\(m\\)개의 귀무가설 \\(H_{0i} \\text{ vs. } H_{1i}, i=1,\\cdots ,m\\)의 검정을 하는 문제로 볼 수 있다. \\(p_{1},\\cdots , p_{m}\\)을 대응되는 p-value로 정의하자. 다중 비교(multiple testing) 중 가장 널리 알려진 본페로니 방법(Bonferroni method)은 \\(p_{i} &lt; \\frac{\\alpha}{m}\\)일 경우 \\(H_{0i}\\)를 기각하는 방법이다. 그러나 이 방법은 m이 많아지면 너무 보수적으로 바뀌는 경향이 있다. \\(H_{0}\\) 기각 안함 \\(H_{0}\\) 기각 계 \\(H_{0}\\)가 참 \\(U\\) \\(V\\) \\(m_{0}\\) \\(H_{0}\\)가 거짓 \\(T\\) \\(S\\) \\(m_{1}\\) \\(m-R\\) \\(R\\) \\(m\\) 오류 발견율(false discovery rate, FDR)을 정의하기 위해 다음과 같은 표를 생각해보자. False discovery proportion은 \\(H_{0}\\)를 기각한 가설들 중 실제 \\(H_{0}\\)가 true (false positive)인 것의 비율이다. 다시 말하면 \\[ \\text{FDP}= \\begin{cases} \\frac{V}{R} &amp; \\text{if R $&gt;$ 0}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] 이다. 그리고 \\(E(\\text{FDP})=\\)FDR로 정의한다. Benjamini-Hocheberg 방법(Benjamini-Hocheberg method)은 level \\(\\alpha\\)에 맞춰 FDR을 조절하기 위해 고안되었다. 이 방법은 다음과 같이 진행된다. \\(m\\)개의 p-value들을 \\(p_{(1)}, &lt; \\cdots &lt; p_{(m)}\\)으로 순서를 매긴(ordering)다. \\(l_{i}=\\frac{i\\alpha}{c_{m}m}\\)과 \\(R=\\max \\{i: p_{(i)}&lt; l_{i}\\}\\)를 정의한다. 여기서 \\[ c_{m}= \\begin{cases} 1 &amp; \\text{if p-values are independent}\\\\ \\sum_{i=1}^{m}(\\frac{1}{i}) &amp; \\text{o.w.} \\end{cases} \\] 이다. 일반적으로 모든 가설들은 독립이니 거의 1을 쓴다고 봐도 무방하다. Threshold \\(T=p_{R}\\)을 정의한다. \\(p_{i} \\leq T\\)일 경우 \\(H_{0i}\\)를 기각한다. 이 방법은 다중 비교를 할 때 가장 안정적으로 값을 준다고 알려져 있다. 그러면 이 방법을 웨이블릿에 똑같이 적용해보자. 다음과 같은 다중 비교 문제에서의 FDR control 방법은 다음과 같다. \\[H_{0i}:\\theta_{jk}=0 \\text{ vs. } H_{1}:\\theta_{jk} \\neq 0, j=0,\\cdots, J-1 \\text{ and } k=0, \\cdots, 2^{j}-1.\\] 각각의 \\(d_{jk}\\)에 대해 양뱡향 p-value를 \\(p_{jk}=2(1-\\Phi (\\frac{| d_{jk}|}{\\sigma}))\\)와 같이 정의한다(\\(\\sigma\\)는 주로 MAD로 추정한다). \\(p_{(1)} \\leq \\cdots \\leq p_{(m)}\\)으로 순서를 매긴다. \\(i_{0}\\)을 \\(p_{(i)} \\leq (\\frac{i\\alpha}{m})\\)을 만족하는 가장 큰 \\(i\\)라고 정의한다. 각각의 \\(i_{0}\\)에 대해 \\(\\lambda = \\sigma \\Phi^{-1}(1-\\frac{p_{i0}}{2})\\)를 계산한다. 이는 \\(|d_{jk}|\\)를 \\(\\lambda\\)와 비교하기 위함이다. 각 level에서 \\(\\lambda\\)보다 작은 것을 kill하도록 \\(d_{jk}\\)를 threshold한다. 5.3 베이지안 웨이블릿 축소(Bayesian wavelet shrinkage) 희소(sparse)한 성질을 갖는 웨이블릿 축소의 특징은 \\(\\theta\\)에 대한 사전 정보를 갖고 있다(거의 대부분의 \\(\\theta\\)는 0이다)고도 볼 수 있고, 따라서 Bayesian 방법을 적용할 수 있다. 여기서 다루는 모든 Bayesian 방법은 prior의 모수를 미리 추정하고 사후 평균(posterior mean)을 구해놓는 경험적 베이즈(empirical Bayes) 방법이다. 5.3.1 Prior mixture of Gaussian 가장 처음 등장한 베이지안 웨이블릿 축소 방법은 “prior mixture of Gaussian”이다. 이는 가우스 분포 두 개를 합성한 것을 prior로 생각한 것이다. \\[\\theta_{jk}|\\gamma_{jk} \\sim \\gamma_{jk} \\mathcal{N}(0,c_{j}^{2}, \\tau_{j}^{2})+(1-\\gamma_{jk}) \\mathcal{N}(0,\\tau_{j}^{2}).\\] 여기서 \\(\\gamma_{jk}\\)는 \\(P(\\gamma_{jk}=1)=p_{j}\\)를 만족시키는 베르누이 확률변수이다. 그리고 \\(p_{j},c_{j},\\tau_{j}\\)는 초모수(hyperparamer)이다. 초모수란 prior의 모수를 의미한다. 성김 성질을 만들기 위해서는 \\(\\tau_{j}\\)는 작게, \\(c_{j}\\)는 1보다 크게 설정한다. 초모수들을 data로부터 계산하는 방법을 경험적 베이지안(empirical Bayesian)이라고 한다. 그 후 우도 \\(d|\\theta\\)를 다음과 같이 계산한다. \\[d|\\theta \\sim \\mathcal{N}(\\theta, \\sigma^{2}).\\] 그 다음에는 posterior distribution \\(F(\\theta | d)\\)를 계산한다. 문제는 \\(F\\)의 계산이 쉽지 않다는 것이다. 그래서 대신 계산이 쉬운 점추정값 \\(E(\\theta | d)\\)를 주로 계산한다. \\[\\hat{d}\\approx \\hat{\\theta} \\approx E(\\theta | d)=s(d)d,\\] \\[s(d)=\\frac{(c\\tau)^{2}}{\\sigma^{2}+(c\\tau)^{2}}P(\\gamma=1 | d)+\\frac{\\tau^{2}}{\\sigma^{2}+\\tau^{2}}P(\\gamma=0 | d).\\] Figure 5.3: Posterior mean and variance of parameter uwing prior mixture of Gaussian. 여기서 \\(\\frac{(c\\tau)^{2}}{\\sigma^{2}+(c\\tau)^{2}}, \\frac{\\tau^{2}}{\\sigma^{2}+\\tau^{2}}\\)이 기울기(slope)를 결정해준다. 예를 들어 만약 \\(\\tau^{2}\\)이 작으면 \\(\\frac{\\tau^{2}}{\\sigma^{2}+\\tau^{2}}\\) 또한 작아질 것이다. \\(\\sigma^{2}\\)의 선택 또한 매우 중요하다. 그런데 이 방법은 그림에서 볼 수 있듯이 축소(shrinkage)는 하나 임계화(thresholding)는 하나도 못한다는 단점이 있다. 5.3.2 Prior mixture of point mass and Gaussian 이러한 단점을 보완하기 위해 등장한 방법이 “prior mixture of point mass and Gaussian”로, (Abramovich, Sapatinas, and Silverman 1998)이 제안한 방법이다. 이는 다음과 같이 prior를 바꾸는 것에서 출발한다. \\[\\theta_{j} \\sim \\gamma_{j}\\mathcal{N}(0,\\tau_{j}^{2})+(1-\\gamma_{j})\\delta_{0} \\text{ where $\\delta_{0}$ is a point mass at zero}.\\] 이를 이용해 posterior distribution \\(F(\\theta | d)\\)를 구한 후 그것의 median를 점추정값으로 삼는다. \\[\\text{median}(\\theta |d)=\\text{sgn}(d)\\max(0,\\xi),\\] \\[\\text{ where } \\xi =\\frac{t_{j}^{2}}{\\sigma^{2}+\\tau_{j}^{2}}|d| -\\frac{\\tau_{j}\\sigma}{\\sqrt{\\sigma^{2}+\\tau_{j}^{2}}}\\Phi^{-1}(\\frac{1+\\min(\\omega,1)}{2}).\\] 또 책에 의하면 \\(\\omega=\\frac{1-p}{p}\\frac{\\sigma^{2}+\\tau_{j}^{2}}{\\tau_{j}^{2}}\\exp(-\\frac{d^{2}(\\sigma^{2}+\\tau_{j}^{2})^{2}}{2\\sigma^{2}\\tau_{j}^{4}})\\)이다. Figure 5.4: Posterior median plot using prior mixture of point mass and Gaussian. 이 그림은 prior mixture of point mass and Gaussian 방법을 이용했을 때 모수의 posterior median 그림. 절대 \\(\\hat{d} = d\\)가 되지 않는다는 사실을 참고하자. 5.3.3 Mixture of point mass and heavy-tail distribution 또 다른 방법은 (I. Johnstone and Silverman 2005)에 등장하는 “Mixture of point mass and heavy-tail distribution”이다. 이 논문에는 sparse에 대한 설명도 잘 되어있다. 이 논문의 저자들은 이 방법의 idea를 건초더미에서 바늘 찾기(finding a needle in a haystack)로 요약하였다. 이 방법에서는 다음과 같은 spike-flat prior를 고려한다. \\[f_{\\text{prior}}(\\theta)=w \\tau(\\theta)+ (1-w)\\delta_{0}.\\] 여기서 \\(\\tau(\\theta)\\)는 Laplace distribution과 같은 두꺼운 꼬리 분포를 의미한다. 이는 “sparse signal을 Normal보다 두꺼운 꼬리를 갖는 분포로 표현하는 것이 더 정확할 것이다”라는 믿음을 가지고 있는 것이다. 이는 매우 훌륭한 임계화(thresholding) 방법이나 \\(F(\\theta |d), \\text{median}(\\theta |d)\\)계산이 복잡하다는 단점이 있다. 마지막으로 이들 Bayesian 방법들을 frequentist 방법들과 비교해보자. \\(p(\\theta,\\lambda)=l(\\theta, d)+\\lambda p(\\theta)\\)라는 벌점화 최소자승(penalized least square) 방법을 생각해보자. 여기서 목표는 \\(p(\\theta,\\lambda)\\)를 최소화 하는 것이다. 이 때 \\(d\\)와 \\(\\lambda\\) 사이에는 일대일 대응관계가 있어 \\(d\\)가 매우 크면 \\(d\\)의 분산 역할을 하는 \\(\\lambda\\) 또한 0이 된다. 따라서 웨이블릿 변환을 고려하는 것이다. 오라클을 이용할 경우 \\(p(\\theta)\\)에 대한 조건이 필요하며 SCAD 등 frequentist 방법들이 이를 만족한다. 미리 p를 정해놓는 것은 frequentist들의 접근 방법으로 오라클은 frequentist 관점에서의 성질이다. Bayesian에게 이를 적용하기에는 무리가 있다. Bayesian은 p를 자료에 맞게 정하자는 것이다. “EBayes”는 \\(\\lambda\\)와 p를 동시에 계산하는 매우 강력한 방법이다. 일반적으로 \\(E(\\hat{f}_{\\lambda, \\text{EBayes}}-f)^{2}\\)이 다른 방법보다 더 좋은 수렴속도를 자랑하며 EBayes 자체를 physical domain에서 써도 매우 우수하다. 그리고 change point of detection등 다른 문제에도 쓰일 수 있다. 5.4 선형 웨이블릿 평활화(linear wavelet smoothing) 다음과 같이 웨이블릿을 이용한 f의 추정 문제를 생각해보자. \\[f_{J}=\\sum c_{0k}\\phi_{k}(x)+\\sum_{j=1}^{J}\\sum d_{jk}\\psi_{jk}(x).\\] 이때 \\[y \\rightarrow Wy \\rightarrow d \\xrightarrow{\\text{계산}} \\hat{d} \\xrightarrow{\\text{IWT}} W^{T}\\hat{d} \\rightarrow \\hat{f}\\] 로 \\(\\hat{f}\\)를 얻는다. 결과적으로 얻어진 \\(\\hat{f}_{J}\\)는 \\[\\hat{f}_{J}=\\sum c_{0k}\\phi_{k}(x)=\\sum_{j=1}^{L}\\sum d_{jk}\\psi_{jk}(x), L&lt;J.\\] 즉 \\(L\\)보다 높은 레벨의 \\(\\mathbf{d}_{i}\\)들은 모두 영벡터로 만드는 것이다. 그 동안은 invidual에 대해 임계화(thresholding)를 했으나 이 방법은 각 레벨에 대해 thresholding을 하는 것으로 이해할 수 있다. 그러나 performance가 그리 좋지는 않다. 이 방법에서 L을 결정하는 방법은 cross-validation으로 하는 것이 괜찮다. 이 방법은 지금까지 방법과는 달리 선형 방법으로 회귀분석의 내용을 그대로 가져올 수 있어 asymptotic이 쉬워진다. 그러나 잘 맞지는 않는다. 5.5 블록 임계화(block thresholding) 그림과 같은 모자 함수는 한 지점에서만 값이 달라지나 이 변하는 것을 나타내기 위해 여러 개의 nonzero 웨이블릿 계수를 써야한다는 문제점이 있다. Figure 5.5: Posterior median plot using prior mixture of point mass and Gaussian. 다음과 같은 모형 \\(y_{i}=g(x_{i})+e_{i}\\)를 생각해보자. \\(g(x_{i})\\)의 \\((j,k)\\)번째 true 웨이블릿 계수는 \\(\\theta_{jk}=\\int g(x)\\psi_{jk}(x)dx\\)이다. \\(\\theta_{jk}\\)를 잘 계산하기 위해 다음과 같은 경험적 quantity \\(\\hat{d}_{jk}=\\frac{1}{n}\\sum_{k=1}^{n}y_{i}\\psi_{jk}^{2}(x_{i})\\)를 생각해보자. 이것의 분산은 \\[ \\begin{eqnarray*} var(\\hat{d}_{jk})&amp;=&amp;\\frac{1}{n}\\sum_{i=1}^{n}var(y_{i})\\psi_{jk}^{2}(x_{i})\\\\ &amp;=&amp;\\frac{1}{n}\\sum_{i=1}^{n}\\sigma^{2}\\psi_{jk}^{2}(x_{i})\\\\ &amp;\\simeq&amp; \\frac{1}{n}\\sigma^{2}\\int \\psi_{jk}^{2}(x)dx=\\frac{\\sigma^{2}}{n}. \\end{eqnarray*} \\] 로 분산이 커지는 문제가 발생한다고 한다. 이 방법에 대한 해결책으로 “blockwise”하는 방법이 있다. \\(j\\) 스케일(레벨)에서 길이 l인 겹치지 않는 블록들 \\(B_{b}\\)를 만드는 것이다. “block truth”를 다음과 같이 정의한다. \\[B_{jb}=\\frac{1}{l}\\sum_{(b)}\\theta_{jk}^{2}.\\] 여기서 \\(\\sum_{(b)}\\)는 \\(k \\in B_{b}\\)에 대해 모두 더하라는 것이다. 이것에 대한 추정량은 \\(\\hat{B}_{jb}=\\frac{1}{l}\\sum_{(b)}d_{jk}^{2}\\)이며 블록 웨이블릿 계수 contribution은 다음과 같이 구할 수 있다. \\[\\sum_{j=0}^{q}\\sum_{-\\infty &lt; b &lt; \\infty} \\{ \\sum_{(b)}\\hat{d}_{jk}\\psi_{jk}(x_{i})\\}I(\\hat{B}_{jb} &gt; \\lambda^{2}).\\] 물론 length \\(l\\)의 선택과 overlapping을 하는 것이 좋은지에 대한 문제가 남아 있다. References "],
["multiscalets.html", "Chapter 6 다중척도 시계열분석 6.1 시계열 자료의 정상성(stationary time series) 6.2 정상과정의 백색화(whitening of stationary process) 6.3 정상과정의 스펙트럼 표현(spectral representation of stationary process) 6.4 압축 표본화되지 않은 이산 웨이블릿(non-decimated discrete wavelets)", " Chapter 6 다중척도 시계열분석 \\(y_{i}=g(x_{i})+e_{i}\\)라는 자료 분석을 할때, 지금까지는 error가 independent인 자료들만을 주로 다루었으나, 이제는 correlated 되어있는 자료들을 생각해보자. 가장 간단한 경우로 \\(\\mathbf{e} \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Gamma}), \\boldsymbol{\\Gamma}=[ \\gamma_{|r-s|}]_{r,s} \\neq \\sigma^{2}I\\) 가 정상 과정(stationary process) 이라고 가정하는 것이다. 이런 경우에 웨이블릿 임계화(thresholding)를 어떻게 될 것인가? 그 전에 왜 상관자료(correlated data)에서 웨이블릿을 고려하는지에 대해 잠시 설명하자면 이는 웨이블릿의 “whitening” 성질과 관련이 있다. \\(\\mathbf{y}\\)가 정상 과정일 경우 웨이블릿 계수 d가 독립에 가까워지는 \\(corr(d_{j},d_{j&#39;})\\approx 0\\)인 \\(|j-j&#39;|&gt;\\delta\\)를 만족하는 \\(\\delta\\)가 존재한다는 것이 “whitening”이다. 이로 인해 웨이블릿이 상관자료(correlated data)를 다룰 때 도움을 줄 수 있다. 6.1 시계열 자료의 정상성(stationary time series) \\(\\{ X_{t} \\}\\)는 다음과 같은 자기공분산 함수(autocovariance function) \\[\\gamma_{X}(r,s)=Cov(X_{r},X_{s})=E(X_{r}-EX_{r})(X_{s}-EX_{s}) \\text{ is finite for any } r,s.\\] 를 갖는 랜덤 프로세스라고 하자. 여기서 staionary보다 조금 더 약한 약정상과정(weakly stationary process)에 대해 생각해보자. 다음과 같은 세 가지 조건 \\(E|X_{t}|^{2} &lt; \\infty\\). \\(EX_{t}=m\\)(constant), \\(\\forall t\\). \\(Cov(X_{t+h},X_{t})=\\gamma_{X}(h) (\\text{ t에 독립})\\) 을 만족하는 \\(\\{ X_{t} \\}\\)를 약정상성(weakly stationary)을 갖는다라고 한다. 순정상 시계열(strictly stationary time seres)은 모든 \\(t_{i}, n\\) 그리고 \\(\\tau\\)에 대해 \\((X_{t_{1}}, \\ldots, X_{t_{n}})\\)의 결합분포가 \\(X_{t_{1}+\\tau}, \\ldots, X_{t_{n}+\\tau})\\)의 결합 분포와 항상 같은 것을 의미한다. 그러나 이 정의는 너무 강해 약정상 시계열(weakly stationary time seres)을 많이 쓰게된다. 보통 정상과정(stationary process)을 말하면 약정상성을 의미한다. 백색 잡음 과정 \\(\\{Z_{t}\\}\\)는 \\[E(Z_{t})=0, \\forall t,\\] \\[\\gamma(h)=\\sigma^{2}\\delta_{h}=E(Z_{t+h},Z_{t})=\\sigma^{2}I\\{h=0\\}\\] 이므로 정상 과정이다. AR(1) 과정 \\(\\{X_{t}\\}\\)는 \\[X_{t}-\\phi X_{t-1}=Z_{t}, Z_{t} \\sim WN(0,\\sigma^{2}), \\] \\[\\gamma(h)=\\sigma^{2}\\frac{\\phi^{|h|}}{1-\\phi}\\] 이므로 정상 과정이다. 다시 상기하는 의미에서 \\(X(t)\\)의 이산 웨이블릿 변환은 \\(\\{ d_{jk} \\}=\\int_{\\mathbb{R}}X(t)\\psi_{jk}(t)dt\\)이고, \\[E d_{jk}d_{j&#39;k&#39;}=\\int \\int_{\\mathbb{R}^{2}}\\gamma(t,s)\\psi_{jk}(t)\\psi_{j&#39;k&#39;}(s)dtds\\] 이다. 다음은 웨이블릿 변환과 정상성에 관련된 정리이다. \\(X(t)\\)가 약한 정상 과정이고 \\(\\gamma(t,s)\\)가 \\(\\mathbb{R}^{2}\\)에서 유계(bounded)이고 연속이면, \\(X(t)\\)가 약정상과정인 것과 \\(\\{ d_{jk} \\}\\)가 약정상과정인 것은 동치이다(만약 \\(\\psi\\)가 compactly supported이면 당연히 유계이므로 유계 조건이 따로 필요 없다). 6.2 정상과정의 백색화(whitening of stationary process) \\(X(t)\\)를 정상 과정이라고 하자. 그러면 \\[X_{m}(t)=\\sum c_{mk}\\phi_{mk}(t)=\\text{ projection on $V_{m}$}=\\sum_{j}^{m}d_{jk}\\psi_{jk}(t)\\] 와 같이 웨이블릿의 선형 결합으로 나타낼 수 있다(\\(d_{jk}=\\int X(t)\\psi_{jk}(t)dt\\)). 그리고 \\begin{eqnarray*} E d_{jk}d_{j&#39;k&#39;}&amp;=&amp;\\int \\int_{\\mathbb{R}^{2}}\\gamma(t,s)\\psi_{jk}(t)\\psi_{j&#39;k&#39;}(s)dtds\\\\ &amp;=&amp;\\frac{1}{2 \\pi}\\int_{\\mathbb{R}}\\hat{\\gamma}(\\omega)\\hat{\\psi}(\\frac{\\omega}{2^{j}})\\bar{\\hat{\\psi}}(\\frac{\\omega}{2^{j}})e^{-i\\omega k 2^{-j}}e^{-i\\omega k&#39; 2^{-j&#39;}}2^{-\\frac{j}{2}}2^{-\\frac{j&#39;}{2}}d\\omega.\\\\ \\end{eqnarray*} 이다. 갑자기 notation들이 막 튀어나오는데 자세한 내용은 (Zhang and Walter 1994)를 참고하라. \\(d_{jk}\\)와 \\(d_{j&#39;k&#39;}\\)는 \\(|j-j&#39;|&gt;1\\)일 경우 무상관(uncorrelated)이다(에를 들어 \\(d_{1}\\)과 상관(correlated)된 것은 \\(d_{0}, d_{2}\\)이다). \\(|j-j&#39;|=1\\)일 경우 약간의 correlation을 갖는다. \\(j \\neq j&#39;\\)일 경우 \\(O(|j-j&#39;|^{-p})\\)의 correlation을 갖는다. (Johnstone and Silverman 1997)에서는 위 정리의 결과를 그대로 웨이블릿 축소에 가져왔다. 즉 각 level끼리 독립이므로 level별로 따로 임계화(thresholding)를 하는 방법을 생각한 것이다. 이 때 레벨 \\(j\\)에서 \\[\\hat{\\lambda}_{j}=\\sqrt{2\\log n_{j}}\\hat{\\sigma}_{j} \\text{ or } \\hat{\\lambda}_{j}=\\min_{\\lambda}\\text{SURE}(\\lambda_{j})\\] 를 이용한다. \\(n_{j}\\)는 레벨 \\(j\\)에서의 계수의 총 갯수이고, \\(\\hat{\\sigma}_{j}\\)는 MAD로 계산한다. 그러나 자료가 정상성이 아닐 경우 방법이 없다. 왜냐하면 비정상 시계열(nonstationary time series)은 너무 광범위한 개념이기 때문이다. 따라서 사람들은 비정상 시계열들 중 다루기 쉬운 일부를 제한해 그것을 분석한다. Evolutionary stationary process가 그 예이다. 6.3 정상과정의 스펙트럼 표현(spectral representation of stationary process) \\(\\{ X(t) \\}\\)를 평균이 0인 정상과정이라고 하자. 그러면 다음 \\[X(t)=\\int_{-\\infty}^{\\infty} e^{it\\omega}dZ(\\omega)\\] 를 만족시키는 직교과정(orthogonal process) \\(\\{Z(\\omega)\\}\\)가 존재한다. 여기서 잠시 직교증분과정(orthogonal increment process)라는 것에 대해 소개하겠다. 두개의 다른 주파수 \\(\\omega\\), \\(\\omega&#39;\\) \\((\\omega \\neq \\omega&#39;)\\)가 있다고 하자. 만약 두 개의 process \\[dZ(\\omega)=\\{Z(\\omega +d\\omega)-Z(\\omega)\\} \\text{ and } dZ(\\omega&#39;)=\\{Z(\\omega&#39; +d\\omega&#39;)-Z(\\omega&#39;)\\}\\] 가 무상관\\((\\text{Cov}(dZ(\\omega), dZ(\\omega&#39;))=0)\\)일 때 \\(dZ(\\omega)\\)는 직교증분과정이라고 한다. 다시 직교과정으로 돌아가서, \\(\\{Z(\\omega)\\}\\)는 다음 성질들을 만족시킨다. \\(E(dZ(\\omega))=0, \\forall \\omega\\). \\(E|dZ(\\omega)|^{2}=d H(\\omega)=h(\\omega), \\forall \\omega, \\text{ where } H(\\omega)=\\int h(\\omega)=\\text{integrated spectrum of } X(t)\\). \\(Cov(dZ(\\omega), dZ(\\omega&#39;))=0\\). (직교증분과정) (G. Nason 2010)에 의하면, 어떤 시계열 \\(\\{Z_{t} \\}_{t\\in \\mathbb{Z}}\\)가 정상확률과정이라면 다음과 같이 표현할 수 있다고 한다. 즉 \\(\\sin\\), \\(\\cos\\) basis function으로 expansion할 수 있는 것이다. \\begin{equation} X_{t}=\\int_{-\\pi}^{\\pi}A(\\omega)e^{it\\omega}d\\xi(\\omega) \\end{equation} 이 때 \\(A(\\omega)\\), \\(d\\xi(\\omega)\\)가 \\(dZ(\\omega)\\) 역할을 하게 되는 것이며, 가장 영향력 있는 계수들의 제곱이 스펙트럼(spectrum)이 된다. \\[|A(\\omega)|^{2}=\\hat{h}(\\omega): \\text{ periodogram (point estimator but not consistent)}\\] \\[|\\tilde{A}(\\omega)|^{2}: \\text{ smoothing시 consistent 해짐}\\] \\[Cov(X(t), X(t+\\tau))=\\int_{-\\infty}^{\\infty}e^{it\\omega}dH(\\omega)\\] 이며, 여기서 \\(Cov(X(t), X(t+\\tau))\\)는 \\(X(t)\\)의 공분산함수이며, \\(dH(\\omega)\\)는 \\(X(t)\\)의 스펙트럼이고, 이들은 \\(e^{it\\omega}\\) 라는 푸리에변환을 통해 일대일변환(one-to-one transform)된다. 다음 내용은 (Priestley 1981) 및 (Dahlhaus 1996)에서 발췌한 내용이다. 비정상과정 \\(X(t)\\)는 다음과 같이 간주할 수 있다. \\[\\phi_{t}(\\omega) \\text{ instead of } e^{it\\omega}\\] \\begin{eqnarray*} X(t)&amp;=&amp;\\int \\phi_{t}(\\omega)dZ(\\omega)=\\int A(\\omega)e^{it\\omega}d\\xi(\\omega)\\\\ &amp;=&amp;\\int A_{t}(\\omega)e^{i\\theta(\\omega)t}dZ(\\omega) \\Longrightarrow \\text{ &quot;Oscillatory process&quot;}\\\\ \\end{eqnarray*} 여기서 \\(A_{t}(\\omega)\\)는 일종의 가중치(weight)이며 시간에 따라 계속 바뀐다. 이 내용은 비정상과정의 시계열을 분할하여 스펙트럴 표현(spectral representation)이 되도록 제한할 수 있다는 것이다. 그러나 유일(unique)하게 정의되지 않는다는 것은 문제이다. 함수추정에서 웨이블릿은 희소(sparsity) 장점을 준다. 그러나 시계열에서의 웨이블릿 사용은 희소와는 큰 관련이 없다. 다만 다음과 같이 변환하였을 때, \\[X(t) \\stackrel{\\omega}{\\rightarrow}d_{jk}(t) \\text{ j: scale, k: location}\\] \\(d\\)는 \\(\\omega\\)와 \\(t\\)의 성질을 동시에 갖으며 이를 통해 비정상 시계열을 다룰 수 있지 않을까 하는 기대감 때문이다. 이러한 분석 방법을 time (k) - scale (j) analysis라고 한다. 6.4 압축 표본화되지 않은 이산 웨이블릿(non-decimated discrete wavelets) 일반적인 웨이블릿은 시계열 자료를 다루기에는 information이 너무 작아지는 경향이 있는데, 압축 표본화되지 않은 웨이블릿(non-decimated wavelets)은 그렇지 않아 temporal resolution에 좋다고 한다. (G. P. Nason, Sachs, and Kroisandt 2000)은 컴팩트 지지 웨이블릿(compactly supported wavelets)이라는 것을 만들었다. References "],
["admultiscale.html", "Chapter 7 고급 다중척도 방법론 7.1 2세대 웨이블릿 변환(second-generation wavelet transform) 7.2 리프팅 스킴(lifting scheme) 7.3 2차원 자료의 리프팅 스킴(lifting in two dimensions)", " Chapter 7 고급 다중척도 방법론 이 장에서는 앞 장에서 다룬 다중척도 방법론 중 심화된 웨이블릿 방법론 및 최신 동향에 대해 다룬다. 이 분야의 대표적인 참고문헌에는 (Jansen and Oonincx 2005)가 있다. 7.1 2세대 웨이블릿 변환(second-generation wavelet transform) 푸리에 변환을 가지고 만들어진 웨이블릿을 1세대 웨이블릿(first-generation wavelet)이라고 부른다. 이와 다르게 푸리에 변환을 이용하지 않고 만들어진 웨이블릿을 2세대 웨이블릿 변환(second-generation wavelet)이라고 부른다. 이 2세대 웨이블릿 변환은 다음에 소개될 리프팅 스킴이라는 것에 의해 만들어진다. 7.2 리프팅 스킴(lifting scheme) 리프팅 스킴(lifting scheme)은 확장(enhancement)이라는 개념이다. 지금 존재하는 웨이블릿에 우리가 원하는 성질들을 추가하는 것이다. Haar 웨이블릿을 새로운 관점에서 보도록 하자. \\(s_{j+1}\\)을 \\(j+1\\) 스케일에서의 투입값이라고 하자. 그러면 Haar 변환은 이것들을 \\(j\\) 척도의 평균 \\(s_{j,k}\\)과 차이(detail) \\(d_{j,k}\\)로 바꿔준다. \\begin{equation} s_{j,k}=\\frac{s_{j+1,2k}+s_{j+1,2k+1}}{2} \\end{equation}\\begin{equation} d_{j,k}=s_{j+1,2k+1}+s_{j+1,2k} \\end{equation} 위 식들의 역변환(inverse transform)은 다음과 같다. \\begin{equation} s_{j+1,2k+1}=s_{j,k}+\\frac{d_{j,k}}{2} \\end{equation}\\begin{equation} s_{j+1,2k}=s_{j,k}-\\frac{d_{j,k}}{2}\\label{eq:four} \\end{equation} 식 (\\ref{eq:four})를 이항하여 정리하면 다음과 같은 식을 얻는다. \\begin{equation} s_{j,k}=s_{j+1,2k}+\\frac{d_{j,k}}{2}. \\end{equation} 웨이블릿 변환에서는 \\(s_{j,k}\\)와 \\(d_{j,k}\\)를 동시에 얻지만 리프팅 스킴에서는 \\(d_{j,k}\\)를 얻은 후 순차적으로 \\(s_{j,k}\\)를 얻는다. Figure 7.1: Forward lifting scheme using Haar transform. 정리하면 \\[\\text{차이(difference)=홀(odd)-짝(even)} \\qquad{\\text{듀얼 리프팅(dual lifting)}}\\] \\[\\text{평균(average)=짝(even)+0.5차이} \\qquad{\\text{프라이멀 리프팅(primal lifting)}}\\] 이며 여기서 짝과 홀을 어떻게 정하느냐에 따라 계산값이 달라진다. 리프팅 스킴의 계산 절차를 다음과 같이 세 단계로 요약할 수 있다. 분할(split): 관찰값들을 짝과 홀 두 개의 분리 집합(disjoint set)으로 분할(partition)한다(꼭 짝과 홀로 나누지 않아도 된다). 에측(predict): 홀로 색인(index)된 투입값을 이 값과 짝의 데이터만을 이용해 예측된 값으로 대체한다. (듀얼 리프팅) 갱신(update): \\(s_{j,k}=s_{j+1,2k}+\\frac{d_{j,k}}{2}\\) (프라이멀 리프팅) 일반적인 리프팅 스킴의 프라이멀 리프팅과 듀얼 리프팅의 정변환(forward transform)은 다음과 같다. \\[\\text{차이(difference)=홀(odd)-p짝(even)} \\qquad{\\text{듀얼 리프팅(dual lifting)}}\\] \\[\\text{평균(average)=짝(even)+u차이} \\qquad{\\text{프라이멀 리프팅(primal lifting)}}\\] 이 때 \\(p=1\\), \\(u=0.5\\)인 경우를 특별히 Haar 웨이블릿 변환(Haar wavelet transformation)이라고 부르는 것이다. Figure 7.2: Standard lifting scheme using primal and dual lifting. 리프팅의 역변환(backward transform)은 다음과 같다. \\[\\text{짝(even)=평균(average)-u차이(difference)} \\] \\[\\text{홀(odd)=차이(difference)+p짝} \\] Figure 7.3: Backward lifting scheme. 리프팅 스킴 방법은 데이터가 일정한 간격(equally space)으로 있어야 한다는 가정이 불필요하며 \\(n=2^{J}\\)일 필요도 없다. 그러나 \\(p\\)와 \\(u\\)를 바꿀 경우 직교성(orthogonality)이 안되기 시작하며 2차원 자료인 경우에도 잘 작동하지 않는다. 만약 시공간 자료(spatio-temporal data)에 리프팅을 적용할 수 있다면 군집 분석(clustering analysis)을 할 때 군집의 크기(clustering size)를 고민할 필요가 없다는 장점이 생긴다. 다음과 같은 벡터 \\[\\mathbf{z}_{3}=(1,2,3,4,5,6,7,8), n=8, J=3.\\] 가 있다고 하자. 이 벡터에 리프팅 스킴을 적용하면 다음과 같다. Spilt: \\(\\mathbf{z}_{3}\\)을 \\(\\mathbf{y}=(1,3,5,7)\\) (홀에 해당)와 \\(\\mathbf{x}=(2,4,6,8)\\) (짝에 해당)로 나눈다. Predict: \\(\\mathbf{x}\\)의 주변값의 평균을 이용해 \\(\\hat{\\mathbf{y}}\\)를 만든다. 첫 번째 원소를 예측할 때에는 첫 번째 \\(\\mathbf{x}\\)값만 쓰기로 한다. 그러면 \\(\\hat{\\mathbf{y}}=(2,3,5,7)\\)가 되고 \\(\\mathbf{e}_{2}=\\mathbf{y}-\\hat{\\mathbf{y}}=(-1,0,0,0)\\)가 된다. Update: 평균을 맞춰주는 작업을 진행해 \\(\\mathbf{z}_{2}=\\bar{\\mathbf{x}}=\\mathbf{x}+\\mathbf{e}_{2}/2 = (1.5,4,6,8)\\)를 만든다. 이제 \\(\\mathbf{z}_{2}\\)를 가지고 같은 작업을 반복한다. 그러면 Spilt: \\(\\mathbf{y}=(1.5,6)\\), \\(\\mathbf{x}=(4, 8)\\). Predict: \\(\\hat{\\mathbf{y}}=(4,6)\\), \\(\\mathbf{e}_{1}=(-2.5,0)\\). Update: \\(\\mathbf{z}_{1}=\\bar{\\mathbf{x}}=(2.75,8)\\). 한 번 더 반복한다. Spilt: \\(\\mathbf{y}=(2.75)\\), \\(\\mathbf{x}=8\\) Predict: \\(\\hat{\\mathbf{y}}=(8)\\), \\(\\mathbf{e}_{0}=(-5.25)\\) Update: \\(\\mathbf{z}_{0}=\\bar{\\mathbf{x}}=(5.375)\\). 최종적으로 남는 detail과 global은 다음과 같다. 0이 많아져 리프팅 스킴으로 좋은 결과를 얻었다고 말할 수 있다고 한다. \\[\\mathbf{e}_{2}=(-1,0,0,0), \\mathbf{e}_{1}=(-2.5,0), \\mathbf{e}_{0}=(-5.25), \\mathbf{z}_{0}=(5.375).\\] 다중척도 방법의 특징은 저장된 계수들을 가지고 원래 신호를 복원(reconstruction)할 수 있어야 한다는 것이다. 앞선 예제의 detail과 global 계수들을 가지고 신호복원을 해보자. \\(\\mathbf{x}=\\mathbf{z}_{0}-\\mathbf{e}_{0}/2=8\\), \\(\\mathbf{y}=\\mathbf{e}_{0}+\\hat{\\mathbf{y}}=2.75\\), \\(\\mathbf{z}_{1}=(2.75, 8)\\). \\(\\mathbf{x}=\\mathbf{z}_{1}-\\mathbf{e}_{1}/2=(4,8)\\), \\(\\hat{\\mathbf{y}}=(4,6)\\), \\(\\mathbf{y}=\\mathbf{e}_{1}+\\hat{\\mathbf{y}}=(1.5, 6)\\), \\(\\mathbf{z}_{2}=(1.5, 5, 6, 8)\\). \\(\\mathbf{x}=\\mathbf{z}_{2}-\\mathbf{e}_{2}/2=(2,4,6,8)\\), \\(\\hat{\\mathbf{y}}=(2,3,5,7)\\), \\(\\mathbf{y}=\\mathbf{e}_{2}+\\hat{\\mathbf{y}}=(1,3,5,7)\\). 따라서 \\(\\mathbf{z}_{3}=(1,2,3,4,5,6,7,8)\\) 신호를 성공적으로 복원할 수 있다. 7.3 2차원 자료의 리프팅 스킴(lifting in two dimensions) 이 절의 내용은 (Jang 2012)의 서술을 참고하였다. 2차원 자료에서는 1차원 자료와 다르게 짝과 홀을 정할 수 없다는 문제점이 있다. 불규칙한 격자점에서, 이웃(neighborhood)은 삼각분할(triangulation)을 통해 정의된다. 리프팅 스킴은 모든 해상도 수준(resolution level)에서 다중척도 들로네 삼각분할(multiscale Delaunay triangulation)이라 부르는 꼭지점들의 이웃 구조를 사용한다. 삼각형 격자 위에 있는 자료를 다중척도 표현으로 분해(decomposition)하는 것은 현재 수준 \\(l\\)에서 \\(l+1\\)로 넘어갈 때 빼길 원하는 점 주변의 국소적 재삼각분할(local retriangulation)을 필요로 한다. 이 알고리즘은 수준이 고정되었을 때 이웃은 변하지 않는다라는 가정 하에서 진행된다. Figure 7.4: 2D lifting scheme example. 위 그림에서 빨간색 점들로 검은색 점을 포함하는 보로노이 다이어그램(Voronoi diagram)을 그린다. (a), (b), (c)는 각각 첫 번째, 두 번째 및 세 번째 리프팅 스킴에 대응된다. (d)는 투입된 자료를 네 가지 수준으로 구분한 그림이다. 빨간색 점들은 \\(s_{t,2k+1}\\)에 대응되며 검은색 점들은 \\(s_{t,2k}\\)에 대응된다. 갱신 단계에서, 우리는 검은색 지점의 상세 이미지(detail image) \\(d_{t+1,k}\\)를 얻을 수 있고, 빨간색 지점의 갱신된 근사 이미지(approximation image) \\(s_{t+1,k}\\)를 얻을 수 있다. 산재된 자료(scattered data)의 경우 척도(scale)은 ‘연속(continuous)’ 개념에 대응된다. 다시 말하면, 모든 자료는 각자 고유의 척도를 가지고 있고, 이 척도는 주변과의 거리에 관계되어 결정되는 것이다. References "],
["spatial.html", "Chapter 8 공간통계학 8.1 공간자료의 종류(classes of spatial data)", " Chapter 8 공간통계학 이 장에서는 공간통계학과 관련된 내용을 다룬다. 주된 내용은 2015년 공간통계학 특강 수업 내용이다. 8.1 공간자료의 종류(classes of spatial data) 결과적으로 공간 자료의 종류는 크게 세 가지로 나눌 수 있다. 연속자료(continuous data; 이것을 다루는 분야가 geostatistics) 이산자료(discrete data, lattice data, areal data) 점 패턴 데이터(point pattern data) Elevation data Davis (1972)는 geoR이라는 R 패키지를 만들었다. 이 안에 있는 자료 elevation은 52개 지역의 표면 높이(surface elevation)를 측정한 자료이다. 기본 distance unit은 50 feet이며, 높이의 기본 distance unit은 10 feet이다. 이러한 자료를 plot하기 위해서는 geodata로 바꿔줘야 한다. Rongelape Island data Diggle et al. (1998)이 만든 R geoRglm 패키지의 롱겔라프 환초 자료(Rongelape Island data, rongelap)은 1954년 미국이 수소탄 실험을 한 곳의 방사능을 측정한 자료이다. 이 자료는 공간 이산 자료(spatial discrete data)의 예이다. 이 자료는 다른 자료들과 달리 sampling design이 되어있는 자료이다. 즉 자료를 200m마다 하나씩 일정하게 뽑은 것이다. 추가로 4개의 지역에 대해서는 50m마다 subsampling을 하였다. 200m마다 측정한 자료로는 200m보다 작은 variation을 구할 수 없다. 그래서 microscale variation을 보기 위해 추가로 subsampling을 한 것이다. Scottish lip cancer data 다음 자료는 1975-1980 동안 스코틀랜드 지역 남성의 lip cancer case 숫자를 county별로 센 scotland 자료이다. 이 자료는 SpatialEpi 패키지에 있다. 이 자료는 spatial dependency가 clear해서 많이 쓰인다고 한다. 이 자료는 앞 자료들과 달리 point 자료가 아닌 지역별 자료이다. 이런 자료를 공간 집적 자료(areal aggregation data)라고 한다. 이런 경우의 자료 분석은 first neighbor, second neighbor에 어떤 자료가 있는지 살펴보는 것을 많이 한다. Inventory data of the Zurichberg Forest, Switzerland 이 자료는 spBayes에 있는 취리히 숲 자재 자료(Zurichberg forest inventory data, Zurich.dat)이다. 이 자료의 특징은 숲이 어디 생길지 모르기 때문에 위치 자체가 random이 된다는 것이다. 앞선 자료들의 위치가 고정되어 있었던 것과는 다른 상황이다. 나무의 size를 잴 때에는 사람 가슴 정도 높이의 trunk를 잰다고 한다. 산림학에서의 관심사는 나무와 나무 사이의 상호작용(interaction)이 있는지 보는 것이다. 나무의 종류에 따라 같이 자랄 수도 있고, 또는 한 나무만 살아남을 수도 있는데 이러한 현상에 관심을 갖는 것이다. Location 자체가 random인 자료들은 point process로 모델링한다. 여기서 \\[\\mathbf{x}=(x_{1}, \\cdots , x_{n})\\] 이라고 하면 \\(\\mathbf{x}\\)와 \\(n\\)이 모두 random이다. 또 mark라고 추가정보가 들어올 수도 있다고 한다. \\[M|\\mathbf{x}=(m_{x_{1}}, \\cdots , m_{x_{n}}).\\] "],
["spatialprocess.html", "Chapter 9 공간과정 9.1 공간자료의 정상성(stationary in spatial data) 9.2 순정상성(strictly stationary) 9.3 약정상성(second order stationary, weakly stationary) 9.4 내재정상성(intrinsic stationary) 9.5 정상성들 사이의 관계(relationship between stationarity) 9.6 에르고딕 과정(ergodic process)", " Chapter 9 공간과정 연속자료의 공간과정(spatial process)은 다음과 같이 나타낸다. \\[\\{Z (\\mathbf{s}), \\mathbf{s} \\in \\mathcal{D} \\subset \\mathbb{R}^{d} \\} .\\] 여기서 \\(\\mathbf{s}\\)는 위치(location)이며 \\(d\\)차원 자료이다. \\(d\\)는 2차원일 수도 있고 고도를 고려하면 3차원일 수도 있다. \\(Z(\\mathbf{s})\\)는 \\(\\mathbf{s}\\)에서의 확률변수를 나타낸다. 만약 이산자료라면 \\[\\{Z (\\mathbf{s}), \\mathbf{s} \\in \\mathcal{D} \\subset \\mathbb{Z}^{d} \\} \\] 로 바뀐다. 점 패턴 자료의 경우에는 굳이 표현하자면 \\[\\{ (\\mathbf{s}_{1},z(\\mathbf{s}_{1})), \\cdots , (\\mathbf{s}_{n},z(\\mathbf{s}_{n})) \\} \\] 으로 쓸 수 있으며 이 때 \\(s\\), \\(Z\\), \\(n\\) 모두 무작위라고 한다. 다음은 \\(Z(\\mathbf{s})\\)의 평균과 분산을 생각해보자. \\[\\text{mean function}:\\mu(\\mathbf{s})=E(z(\\mathbf{s}))\\] \\[\\text{covariance function}: C(\\mathbf{s}_{1},\\mathbf{s}_{2})=E(Z(\\mathbf{s}_{1}-\\mu(\\mathbf{s}_{1})))(Z(\\mathbf{s}_{2}-\\mu(\\mathbf{s}_{2}))).\\] 우리의 관심사는 \\(C(\\mathbf{s}_{1},\\mathbf{s}_{2})\\)이다(공간 변화 모델링, spatial variation modeling). 그렇다면 우리는 왜 이러한 공간 변화(sptaial variation)를 고려해야 하는가? 간단한 회귀분석의 예를 들면 \\[ \\begin{equation} \\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}, \\qquad{\\epsilon_{i} \\sim (0, \\sigma^{2}\\Sigma_{0})} \\end{equation} \\] 으로 식을 쓸 수 있다. 그런데 여기서 오차항을 \\(\\epsilon_{i} \\stackrel{\\text{iid}}{\\sim} (0, \\sigma^{2})\\)이라고 놓고 모델링을 하면 일치성(consistency)은 만족하나 효율성(efficiency)이 깨지는 결과를 얻는다. 즉 추론(inference)를 하기 위해서 변동(variability)을 구할 때 이것이 커지므로 신뢰구간(confidence interval)도 커지고 부정확한 검정(test) 결과를 주는 것이다. 9.1 공간자료의 정상성(stationary in spatial data) 그렇다면 우리는 왜 정상성(stationary)을 고려하는가? 그 이유는 정상성이 아니면 점근(asymptotic) 결과가 거의 없어 모델링하기 어렵기 때문이다. 이론적 배경을 얘기해 줄 때 정상성이 필요한 것이다. 여기서는 세 가지 정상성 개념이 등장한다. 9.2 순정상성(strictly stationary) 이것은 \\[(Z(\\mathbf{s}_{1}), \\cdots , Z(\\mathbf{s}_{k})) \\stackrel{\\mathcal{D}}{=} (Z(\\mathbf{s}_{1}+\\mathbf{h}), \\cdots , Z(\\mathbf{s}_{k}+\\mathbf{h}))\\] 으로 \\(\\mathbf{h}\\)만큼 장소가 변해도 분포가 동일하다는 것이다. 또한 \\(\\mathbf{s}_{1}, \\cdots , \\mathbf{s}_{k}, \\mathbf{s}_{1}+\\mathbf{h}, \\cdots , \\mathbf{s}_{k}+\\mathbf{h} \\in \\mathcal{D}\\) 라는 가정이 필요하다. 9.3 약정상성(second order stationary, weakly stationary) 이것은 \\[\\mu(\\mathbf{s})=\\mu \\text{ (mean function이 상수)}\\] \\[C(\\mathbf{s}_{1},\\mathbf{s}_{2})=C_{0}(\\mathbf{s}_{1}-\\mathbf{s}_{2})\\] 두 가지 조건을 만족하는 것이다. 두 번째 조건은 covariance function이 distribution에 depend하는 어떤 함수 \\(C_{0}\\)로 표현된다는 것이다. \\(C \\in \\mathbb{R}^{d}\\times \\mathbb{R}^{d}\\), \\(C_{0} \\in \\mathbb{R}^{d}\\)라는 점에서 두 함수는 다른 함수이다. 이것을 만족하는 예는 가우스 과정(Gaussian process, GP), 가우스 임의장(Gaussian random field, GRF)이 있다. 보통 가 1차원이면 주로 과정(process), 2차원이면 주로 장(field)이라고 부른다. 9.4 내재정상성(intrinsic stationary) 이것은 차이(difference)를 가지고 정의한다. \\[E(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))=0, \\forall \\mathbf{h}\\] \\[\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))=2\\gamma(\\mathbf{h}), \\forall \\mathbf{h}\\] 여기서 \\(2\\gamma(\\mathbf{h})\\)를 변동도(variogram), \\(\\gamma(\\mathbf{h})\\)를 준변동도(semivariogram)라고 한다. 9.5 정상성들 사이의 관계(relationship between stationarity) 일반적으로 순정상성이 가장 강한 조건이며 그 다음이 약정상성, 내재정상성이 가장 약한 조건을 가진다. 그러나 특별한 경우에는 역방향도 성립할 수 있다. 9.5.1 순정상성과 약정상성간의 관계(relationship between strong and weak stationary) \\(C(\\mathbf{h})=\\text{Cov}(Z(\\mathbf{s}+\\mathbf{h}), Z(\\mathbf{h}))\\)일 때, \\(C(\\mathbf{s},\\mathbf{s}) &lt; \\infty\\)인 경우에는 순정상성이면 무조건 약정상성이 된다. 한편 가우스 과정이나 가우스 임의장인 경우는 처음 두 적률(moment)만 알면 모든 정보를 알 수 있어 약정상성이면 순정상성 또한 성립한다고 한다(우리가 가우스 과정이나 가우스 임의장을 많이 쓰는 이유이기도 하다). 9.5.2 약정상성와 내재정상성간의 관계(relationship between weak and intrinsic stationary) \\[2\\gamma(\\mathbf{h})=2(C(\\mathbf{0})-c(\\mathbf{h}))\\] 관계식을 살펴보면, \\(C\\)가 주어지면 \\(\\gamma\\)를 정의할 수 있다. 그렇다는 얘기는 약정상성이면 내재정상성도 된다는 것이다. 그러나 \\(\\gamma(\\mathbf{h})\\)만 알 경우 \\(C(\\mathbf{0})\\)과 \\(C(\\mathbf{h})\\)를 동시에 specify하지 못한다. 일반적으로 \\(\\lim_{\\mathbf{h}\\rightarrow \\infty}\\gamma(\\mathbf{h})=C(\\mathbf{0})\\)로 정의하여 \\(C(\\mathbf{0})\\)과 \\(C(\\mathbf{h})\\)를 정의한다. 이 말인즉 거리가 멀면 \\(C(\\mathbf{h})\\)는 \\(\\mathbf{0}\\)이 될 것이라고 생각하는 것이다. 9.5.3 내재정상성이나 약정상성이 안 되는 예(counterexample of intrinsic stationary but not weak stationary) 내재정상성이나 약정상성이 안 되는 경우의 예로는 브라운 운동(Brownian motion)이 있다. 브라운 운동 \\(B(t)\\)는 어떤 과정(process)이며 \\(B(t)=0\\) when \\(t=0\\), \\(B(t)\\)는 almost surely continuous \\(B(t)\\) has independent increments with \\(B(t)-B(s) \\sim \\mathcal{N}(0,\\sigma^{2}(t-s)), t&gt;s\\) 를 만족시킨다고 한다. 이 때 \\(\\sigma^{2}\\)을 확산계수(diffusion coefficient)라고 한다. 편의상 1차원에서 생각하자. 그러면 \\(E(B(t+h)-B(t))=0\\), \\(\\text{Var}(B(t+h)-B(t))=2|h|\\)이므로 내재정상성이다. 그러나 \\(h &lt; 0\\)인 경우에 공분산을 계산하면 \\[ \\begin{eqnarray} \\text{Cov}(B(t+h),B(t)) &amp;=&amp; E(B(t+h)B(t))\\nonumber\\\\ &amp;=&amp;E[B(t+h)\\{B(t)-B(t+h)+B(t+h)\\}]\\nonumber\\\\ &amp;=&amp;E[B^{2}(t+h)]+E[B(t+h)]E[B(t)-B(t+h)]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}(t+h)+0=\\sigma^{2}(t+h). \\end{eqnarray} \\] 가 된다. 같은 방법으로 \\(h &gt; 0\\)인 경우도 증명할 수 있으며 결국 \\(Cov(B(t+h),B(t))=\\min (t+h,t)\\)가 된다. 이는 \\(h\\)의 함수가 아니고 차이로 표현 못한다는 뜻이므로 약정상성이 아니다. 9.6 에르고딕 과정(ergodic process) \\[\\lim_{\\|\\mathbf{h}\\| \\rightarrow \\infty}C(\\mathbf{h}) \\rightarrow \\mathbf{0}\\] 인 과정(process)을 에르고딕 과정(ergodic process)이라고 한다. 이 성질은 충분히 긴 (정상상태)과정에서 앙상블평균(통계적평균)과 시간평균이 같다는 것이다. \\[\\bar{Z}_{n}:\\frac{1}{n}\\sum_{i=1}^{n}Z(t_{i}) \\rightarrow \\mu .\\] “앙상블 평균은 시간을 고정시켜 놓고 표본 함수를 무한 개로 하여 계산한 것이고 시간평균은 표본 함수를 고정시켜 놓고 시간을 무한대로 하여 계산한 것이다.” "],
["covfct.html", "Chapter 10 공분산함수 10.1 스펙트럴 표현 정리(spectral representation theorem) 10.2 콜모고로프 존재 정리(Kolmogorov’s existence theorem) 10.3 공분산함수의 성질(properties of covariance functions) 10.4 등방성(isotropy) 10.5 동질성(homogeneous) 10.6 이등방성(anisotropy) 10.7 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process 10.8 Bartlett의 정리(Bartlett’s theorem) 10.9 Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver)", " Chapter 10 공분산함수 공분산함수(covariance function) \\(C(\\mathbf{h})\\)는 \\[C(\\mathbf{h})=\\text{Cov}(Z(\\mathbf{s}+\\mathbf{h}), (Z(\\mathbf{s})$, $C(\\cdot); \\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\] 이다. 우리가 공분산함수를 고려할 때 생각해 볼 점은 공분산함수가 되기 위한 조건은 무엇이냐는 것이다. 공분산함수가 되려면 그 함수가 양정치함수(positive definite function)이어야 한다. 이것은 대응되는 행렬이 비음정치(non-negative definite) 또는 양반정치(positive semi-definite)이어야 한다는 것이다. 함수가 순양정치함수(strictly positive definite function)이어야 한다는 것은 대응되는 행렬이 양정치(positive definite)이어야 한다는 것이다. 그러나 저 둘을 보이는 것은 모든 유한한 표본에 대해 항상 성립해야 하는 것을 보여야하기 때문에 어렵다. 이를 좀 더 쉽게 보일 수 있는 방법으로 Bochner (1933, 1955)의 정리를 이용하는 것이다. 10.1 스펙트럴 표현 정리(spectral representation theorem) 스펙트럴 표현 정리(spectral representation theorem)는 다른 말로 Bochner의 정리(Bochner’s theorem)이라고 부른다. 실수값을 갖는 연속 함수 \\(C\\)가 양정치함수라는 것은 그 함수가 symmetric nonnegative measure \\(F\\)로부터 \\(\\mathcal{R}^{d}\\)로 가는 Fourier transform인 경우이다. 즉 \\[ \\begin{eqnarray} C(\\mathbf{h})&amp;=&amp;\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x})\\nonumber\\\\ &amp;=&amp;\\int_{\\mathbb{R}^{d}}\\cos(\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x}) \\text{ (spectral representation)} \\end{eqnarray} \\] 인 경우이다. 여기서 \\(F\\)를 스펙트럼 측도(spectral measure)라고 한다. 만약 \\(F\\)가 좋은 성질을 갖고 있어서 (르베그) 밀도함수((Lebesgue) density function) \\(f\\)가 있으면, 즉 \\(F(\\mathbf{x})=f(\\mathbf{x})d\\mathbf{x}\\)이면 \\(f(\\mathbf{x})\\)를 스펙트럼 밀도(spectral density)라고 하며 이것의 스펙트럼 표현(spectral representation)은 \\[ C(\\mathbf{h})=\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x}=\\int_{\\mathbb{R}^{d}}\\cos(\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x} \\] 로 쓸 수 있다. 이 정리를 쓰면 상대적으로 쉽게 어떤 함수가 양정치함수임을 보일 수 있다. 즉 \\[ C(\\mathbf{h}) =\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x}) =\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x} \\] 임을 보이고 \\(f(\\mathbf{x})\\)가 양(positive)임을 보이기만 하면 된다. 10.2 콜모고로프 존재 정리(Kolmogorov’s existence theorem) 정상성이 아닌 경우에는 콜모고로프 존재 정리(Kolmogorov’s existence theorem)으로 해결할 수 있다고 한다. 10.3 공분산함수의 성질(properties of covariance functions) 여기서부터는 공분산함수(covariance function) \\(C(\\mathbf{h})\\)의 성질에 대해 다루겠다. \\(C(\\mathbf{0}) \\geq 0\\) (분산이 음이 아님) \\(C(\\mathbf{-h})=C(\\mathbf{h})\\) \\(|C(\\mathbf{h})| \\leq C(\\mathbf{0})\\) (코쉬-슈바르츠 부등식) \\(C_{1}\\), \\(C_{2}\\)가 양정치함수이면 \\(a_{1}C_{1}+a_{2}C_{2}\\) 또한 양정치함수이다. (\\(a_{1}\\), \\(a_{2} \\geq 0\\)) 추가적으로 \\(C_{1}C_{2}\\) 또한 양정치함수이다. \\(C_{1}, C_{2}, \\ldots\\)가 양정치함수의 수열이고 이것의 극한 \\(\\lim_{n \\rightarrow \\infty}C_{n}\\)이 존재하면 이것 또한 양정치함수이다. 10.4 등방성(isotropy) \\(C(\\mathbf{h})=C_{0}(\\|\\mathbf{h}\\|)\\)인(or \\(\\gamma(\\mathbf{h})=\\gamma_{0}(\\|\\mathbf{h}\\|)\\)) \\(C_{0}\\)가 존재할 경우 이 공간과정이 \\(Z(\\mathbf{s})\\)를 등방성(isotropy)을 갖는다라고 한다. 이것은 \\(d\\)차원에서 정의한 \\(\\mathbf{h}\\) 1차원인 거리에만 의존하는 공분산함수로 표현 가능하다는 것이다. 10.5 동질성(homogeneous) (잘 쓰이지는 않지만) \\(Z(\\mathbf{s})\\)가 내재적(intrinsic)이고 등방성(isotrophic)인 경우 \\(Z(\\mathbf{s})\\)를 동질성(homogeneous)을 갖는다라고 한다. 10.6 이등방성(anisotropy) 이등방성(anisotropy)란 말 그대로 등방성이 아닌 경우를 얘기하지만 이런 경우가 너무 많아 범위를 나누어 생각한다. 10.6.1 기하학적 이등방성(Geometric anisotropy) 이것은 \\[\\gamma(\\mathbf{h})=\\gamma_{0}(\\|A\\mathbf{h}\\|)\\] 즉 어떤 선형변환행렬(linear transfomation matrix) \\(A\\)를 통해 변환한 후 등방성인 경우를 얘기한다. 보통 \\(A\\)는 대칭인 양정치행렬(symmetric positive definite matrix)을 다루며 이 때 \\(\\|A\\mathbf{h}\\|\\)는 타원형 등고선(ellipse contour)이 된다. 10.6.2 띠모양 이등방성(zonal anisotropy) 이것은 기하학적 이등방성의 일반화된 버전이라고 생각하면 된다. \\[\\gamma(\\mathbf{h})=\\sum_{i=1}^{K}\\gamma_{0}(\\|A_{i}\\mathbf{h}\\|)\\] 로 \\(A\\) 대신 \\(A_{i}\\)들로 표현할 수 있는 경우를 얘기한다. 즉 \\(\\gamma(\\mathbf{h})\\)에 대응되는 공간과정을 \\(Z(\\mathbf{s})\\)라 할 때 각 변동도(variogram)에 해당하는 독립인 공간과정(independent spatial process)의 합으로 즉 \\[Z(\\mathbf{s})=Z_{1}(\\mathbf{s})+ \\cdots Z_{k}(\\mathbf{s})\\] 로 표현할 수 있다는 것이다. 여기서 각각은 변동도인데 이들의 합도 변동도인가?라는 궁금증을 가질 수 있다. 이것은 맞다고 한다. 앞서 ’공분산함수가 되려면 이 함수가 양정치함수이어야 한다’라는 것에 대해 배운 적이 있다. 변동도에 대해서도 똑같이 생각해 볼 수 있을 것이다. 유효한 변동도(valid variogram)이란 무엇인가? 이를 확인해 보기 위해 다음 정의를 살펴보자. \\(\\gamma(\\mathbf{h})=C(\\mathbf{0})-C(\\mathbf{h})\\)라고 하자. 이 때 \\(C(\\mathbf{h})\\)는 양정치함수이나 \\(C(\\mathbf{0})\\)의 존재로 \\(\\gamma(\\mathbf{h})\\)가 양정치함수라고 말할 수는 없다. 대신 \\(\\sum a_{i}=0\\)을 만족하는(\\(C(\\mathbf{0})\\) 때문에 붙는 조건이다) 임의의 \\(a_{1}, \\cdots a_{n}\\)에 대해 \\[\\sum_{i}\\sum_{j}a_{i}a_{j}\\gamma(\\mathbf{s}_{i},\\mathbf{s}_{j}) \\leq 0\\] 인 경우 \\(\\gamma(\\mathbf{h})\\)를 조건부 음정치(conditionally negative definite)라고 한다. 우리는 종속구조(dependent structure)를 모델링하기 위해 공분산 준변동도(covariance semivariogram)를 생각하고 있다. 그런데 공간 데이터를 가지고 있으면 이를 어떤 표면(surface)으로 표현 가능하고 이 과정이 부드러운(smooth)지, 즉 과정이 연속인지 또는 미분가능한지 체크해 보고 싶을 수 있다. 이를 공분산을 가지고 체크해 볼 수 있다고 한다. 즉 공분산은 종속구조를 측정하는 것 이외의 또 다른 기능을 갖고 있는 것이다. 10.7 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process 우선 연속성(continuity)과 미분가능성(differentiability)의 정의를 해야 한다. 여러 버전이 있으나 여기서는 두 가지만 소개하기로 한다. 10.7.1 경로연속(path-continuity) 또는 경로 미분가능성(path-differentiability) 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 경로연속(path-continuity) (또는 K번 미분가능(K-times differentiability))하다는 것은 그것의 실현(realization)이 연속(K번 미분가능)한 것이다. 말은 쉬우나 보이기는 어렵다. \\(\\{ Z(\\mathbf{s}) , S \\in \\mathcal{D}\\}\\)를 확률변수의 모임(collection)이라고 할 때 \\((\\omega \\in \\Omega, \\mathcal{F}, P)\\)에서 \\(\\omega\\) 하나당 \\(\\{ Z(\\mathbf{s})\\}\\)가 나오고 이것을 실현한 것이다. 10.7.2 평균제곱연속(mean-square continuity) 또는 평균제곱 미분가능성(mean-square differentiability) 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 \\(\\mathbf{s}\\)에서 평균제곱연속(mean-square continuity)이라는 것(다른 말로 \\(L_{2}\\)-수렴)은 \\[E[(Z(\\mathbf{s}+\\mathbf{h}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 인 경우이다. 같은 방법으로 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 미분계수 \\(Z&#39;(\\mathbf{s})\\)에 대해 평균제곱 미분가능(mean-square differentiable)이라는 것은 \\[E[(\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}-Z&#39;(\\mathbf{s}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 인 경우이다. 사실 \\(\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}\\)의 극한으로 \\(Z&#39;(\\mathbf{s})\\)를 정의하는 것이다. 고차원의 경우도 반복적으로 정의가 가능하다. 이것의 생김새로 보아 변동도와 관련이 있을 것이다라는 생각도 해 볼 수 있을 것이다. 그렇다면 이 둘 사이에는 어떤 관계가 있을까? 안타깝께도 내재하는 관계는 없다. 즉 경로연속이나 평균제곱연속은 아닌 경우도 있고 그 반대도 있다. 한편 공분산과 평균제곱연속(또는 미분가능성)은 어떤 연관성이 있는 걸까? \\[E[(Z(\\mathbf{s}+\\mathbf{h}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 으로부터 \\[2\\gamma(\\mathbf{h})=2(C(\\mathbf{0})-C(\\mathbf{h})) \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 즉 \\(C\\)라는 함수가 \\(\\mathbf{0}\\)에서 연속이라는 것을 알 수 있다. (약)정상성을 가정할 경우 (\\(\\mathbf{s} \\in \\mathbb{R}^{d}\\), \\(\\mathbf{s}\\)는 고정되어있지 않다) \\[ \\mathbf{s} \\text{에서 평균제곱연속} \\leftrightarrow C(\\cdot)\\text{이 } \\mathbf{0}\\text{에서 연속}\\] \\[C(\\cdot)\\text{이 } \\mathbf{0}\\text{에서 연속} \\rightarrow Z(\\mathbf{s})\\text{가 모든 지점에서 평균제곱연속}\\] 위 결과로부터, \\(Z(\\mathbf{s})\\)는 모든 지점에서 평균제곱연속이거나 어떠한 지점에서도 평균제곱연속하지 못함을 알 수 있다. (어떤 특정한 지역에서만 평균제곱연속할 수 없다) 또한 다음과 같은 사실도 알 수 있다. \\[C(\\cdot) \\text{가 원점에서 연속이면} \\rightarrow C(\\cdot) \\text{는 모든 지점에서 연속이다}.\\] 이것의 증명에 대해서는 각자 생각해보기로 한다. 10.8 Bartlett의 정리(Bartlett’s theorem) 이 정리는 다음 두 가지를 일컫는 말이다. 공분산함수 \\(\\rho(\\mathbf{u})\\)를 갖는 정상확률과정이 k-번 평균제곱 미분가능하다는 것은 \\(\\rho(\\mathbf{u})\\)가 원점에서 2k-번 미분가능하다는 것과 동치이다. (k=0일 경우 위에서 말했던 관계와 같다.) k번 미분가능한 평균제곱 미분계수(k-th mean-square derivative) \\(Z^{(k)}(\\mathbf{s})\\)의 공분산함수는 \\((-1)^{k}C^{(2k)}\\)이다. 위 설명 내용은 나중에 공분산 모형으로 체크해 볼 수 있다. 지수 공분산 모형(exponential covariance model)은 미분가능하지 않다. 한편 Matern 계급(class)을 이용할 경우 모수를 조절해서 어느 정도까지 공분산 모형이 미분가능할 지 조절할 수 있다. 한편 비모수적인 방법으로 경험적 변동도(empirical variogram)를 쓸 수 있는데, 이 방법으로는 공분산 모형이 몇 번 미분가능한지 구분할 수 없다. (1-d인 경우) \\(Z_{h}(\\mathbf{s})=\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}\\)로 정의하고 이것의 공분산함수 \\(C_{h}(\\mathbf{t})\\)를 계산하면 \\[C_{h}(\\mathbf{t})=\\text{Cov}(Z_{h}(\\mathbf{s}+\\mathbf{t}),Z_{h}(\\mathbf{s}))=\\frac{1}{h^{2}}\\{2C(\\mathbf{t})-C(\\mathbf{t}+\\mathbf{h})-C(\\mathbf{t}-\\mathbf{h})\\}\\] 이다. 만약 \\(C(\\cdot)\\)이 두 번 미분가능하다면 \\[C_{h}(\\mathbf{t}) \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} -C^{&#39;&#39;}(\\mathbf{t})\\] 이다. 한편 평균제곱 연속성은 프랙탈 분석(fractal analysis) (표면의 부드러움 체크 관련), first index estimation 등과도 관련된다고 한다. 10.9 Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver) 평균제곱연속이나 경로연속이지 않은 경우(path-continuous 조건이 mean-square continuous조건보다 strong하다)는 것을 보이는 (Kent 1989)의 정리를 소개하였다. 여기서는 간단하게 2-d version으로 정리한다. \\(\\rho(\\mathbf{u})\\)를 상관함수(correlation function)라고 하자. 이 함수를 테일러 전개로 \\[\\rho(\\mathbf{u})=\\rho_{m}(\\mathbf{u})+r_{m}(\\mathbf{u}), \\mathbf{u} \\in \\mathbb{R}^{2},\\] 여기서 \\(\\rho_{m}(\\mathbf{u})\\)는 \\(\\mathbf{u}=\\mathbf{0}\\)에서 전개한 테일러 급수의 m차 다항함수, \\(r_{m}(\\mathbf{u})\\)는 나머지(remainder)이다. 이 때, 경로연속이 성립할 충분조건은 \\(\\rho (\\cdot)\\)이 두번 연속 미분가능하고 \\(|r_{2}(\\mathbf{u})|\\)가 다음의 순서로 \\[|r_{2}(\\mathbf{u})|=O(\\frac{|\\mathbf{u}|^{2}}{(\\log |\\mathbf{u}|)^{3+\\gamma}}) \\text{ as } |u| \\rightarrow 0 \\text{ for some } \\gamma &gt;0\\] 0으로 가는 것이다. 정상 가우스 과정일 때 경로 연속에 대한 충분조건은 \\[\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=O(\\frac{1}{(\\log |\\mathbf{u}|)^{1+\\epsilon}})=o(1) \\text{ as } |\\mathbf{u}| \\rightarrow 0 \\text{ for some } \\epsilon &gt; 0\\] 이다. 왼쪽의 \\(\\rho(\\mathbf{0})-\\rho(\\mathbf{u})\\)은 가우스 과정일 경우 나머지의 차수(order)가 아닌 상관(correlation)의 차수로 표현 가능하다는 뜻이다. 분산 1일 시 \\[\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=\\gamma(\\mathbf{u}) \\stackrel{\\|\\mathbf{u}\\| \\rightarrow 0}{\\rightarrow} 0\\] 이며 이 화살표는 평균제곱 연속성의 정의와 같다. (\\(\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=o(1)\\)) 한편 corollary의 차수보다 천천히 0으로 가면 평균제곱 연속은 되지만 경로연속이 되지는 않는다. References "],
["covmodel.html", "Chapter 11 공분산모형 11.1 덩어리 효과(nugget effect) 11.2 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case)) 11.3 유효 범위(effective range) 11.4 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models) 11.5 기타 다른 상황에서의 변동도들(variograms in other situation)", " Chapter 11 공분산모형 이 장에서는 공분산모형에는 어떤 것들이 있는지 살펴볼 것이다. 여기에 등장하는 그림은 주로 (Montero, Fernández-Avilés, and Mateu 2015) 책에 있는 그림들을 가져온 것이다. 11.1 덩어리 효과(nugget effect) 덩어리 효과(nugget effect)란 공간자료분석에서 흔히 나타나는 특징 중 하나이다. 일반적으로 공간자료 모델링은 다음과 같이 한다. \\[Y(\\mathbf{s})=\\mu(\\mathbf{s})+Z(\\mathbf{s})+\\epsilon(\\mathbf{s}).\\] 여기서 \\(Y(\\mathbf{s})\\)는 관측과정(observed process), \\(\\mu(\\mathbf{s})=E(Y(\\mathbf{s}))\\)는 결정적 평균 함수(deterministic mean function), \\(Z(\\mathbf{s})\\)는 순수 공간과정(spatial process), 마지막으로 \\(\\epsilon(\\mathbf{s})\\)는 나머지 설명되지 않은 항(remaining unexplained term)이다. 일종의 백색잡음(white noise)이라고 생각해도 좋다. 그리고 \\(Z(\\mathbf{s}) \\bot \\epsilon(\\mathbf{s})\\)이다. \\(\\epsilon(\\mathbf{s})\\)에 대해 공분산을 생각해보면, \\[ \\text{COV}(\\epsilon(\\mathbf{s}),\\epsilon(\\mathbf{s}+\\mathbf{h}))= \\left\\{ \\begin{array}{ll} \\sigma_{\\epsilon}^{2} &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ 0 &amp; \\textrm{o.w.} \\end{array} \\right. \\] 인데 여기서 \\(\\sigma_{\\epsilon}^{2}\\)을 덩어리(nugget)이라고 한다. 그리고 이러한 \\(\\epsilon(\\mathbf{s})\\)가 있는게 덩어리 효과(nugget effect)가 있다고 한다. 그렇다면 왜 이름이 덩어리인지 생각해 볼 필요가 있다. ‘Nugget’은 ’조그만 덩어리’ 라는 이름의 영어로, 금광 채굴 문제에서 유래했다고 한다. 유전 발굴과 같은 경우에는 변동성이 작아 어느 지점에서 유전이 발견되면 그 주변에서도 높은 확률로 유전이 발견되지만, 금광의 경우는 microscale variability가 있어 어느 지점에서 발견되었다 하더라도 주변에는 금광이 없을 수도 있다는 것이다. 또 다른 관점으로도 덩어리 효과를 표현할 수 있다. 극한값에서 생각을 해 보면 \\[C(\\mathbf{0})-C(\\mathbf{h})=\\gamma(\\mathbf{h})=\\frac{1}{2}\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))\\] 로 표현할 수 있는데, 덩어리 효가 존재할 경우 \\[\\lim_{\\mathbf{h}\\rightarrow \\mathbf{0}}\\gamma(\\mathbf{h})=\\gamma_{0} &gt;0\\] 일 수도 있다는 것이다. 이떄 이 \\(\\gamma_{0}\\)가 덩어리 효에 대응되는 것이다. 평균제곱과정(mean square process)인 경우 공분산이 \\(\\mathbf{h}=\\mathbf{0}\\)일 때 연속이어야 한다. 따라서 평균제곱이며 연속인 공간과정 \\(Z(\\mathbf{s})\\)의 경우 이러한 현상이 존재 할 수 없고 \\(\\epsilon(\\mathbf{s})\\)는 \\(\\mathbf{h}=\\mathbf{0}\\)일 때 불연속일 수 있고 \\(Z(\\mathbf{s}) \\bot \\epsilon(\\mathbf{s})\\)이므로 덩어리 효과가 존재할 수 있는 것이다. 그런데 앞서 또 다른 관점으로 설명할 때 \\(\\sigma_{\\epsilon}^{2}\\) 대신 \\(\\gamma_{0}\\)이라는 표기를 썼는데 그 이유는 측정 오류(measurement error)가 존재할 수도 있기 때문이다. 엄밀히 얘기하면 \\[\\sigma_{\\epsilon}^{2}=\\text{measurement error variance } + \\text{ 순수한 microscale error variance}\\] 로 표현할 수 있는데 공간자료의 경우 반복관찰이 없어 집합이 1개이므로 이 둘을 따로 구분해서 측정하는 것이 불가능하다. 여러 번 측정할 수 있으면 둘을 구분해서 추정할 수 있다고 한다. (독립인 자료의 경우) 그렇다면 언제 반복이 있느냐? 시공간 자료(spatio-temporal data)에서 시간(temporal) 정보를 다 빼냈을 경우 반복이 있는 자료라 생각할 수 있고 이 둘을 구분하여 추정할 수 있다고 한다. 11.2 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case)) 그렇다면 일반적으로 생각하는 변동도의 모양은 어떠한가? Figure 11.1: Variogram example. 새로 등장하는 용어인 문턱(sill)과 범위(range) 그리고 덩어리(nugget)의 의미를 꼭 알아두자. 변동도의 모양은 공분산이 얼마나 빨리 줄어드는지, 그 때 형태(shape)는 어떠한지에 따라 달라진다. 정상과정인 경우 \\[\\gamma(\\mathbf{h})=C(\\mathbf{0})-C(\\mathbf{h})\\] 가 성립한다. 그런데 \\(\\mathbf{h}\\)를 키우면 \\(C(\\mathbf{h})\\)가 줄어드므로 종속성(dependency)이 감소할 것이다. 그리고 정상과정인 경우 \\(C(\\mathbf{0})\\neq \\infty\\)이므로 \\(\\mathbf{h} \\rightarrow \\infty\\)일 때 \\(\\gamma(\\mathbf{h})\\)가 수렴할 것이다. (그렇지 않다면 비정상과정이다) 11.3 유효 범위(effective range) 앞서 말한 성질들에 variogram은 \\(\\mathbf{h} \\rightarrow \\infty\\)어떤 값으로 수렴한다. 그런데 어느정도 지난 \\(\\mathbf{h}\\)이후에 수렴값과 맞닿으면 range를 정의할 수 있으나 점근해서 수렴하는 경우는 range가 \\(\\infty\\)가 되는 문제가 발생한다. 이럴 때 대신 쓰는 정의로 유효 범위(effective range)라는 것이 있다. 이 것의 정의는 다음과 같다. \\[\\gamma(\\mathbf{h}) \\text{가 sill의 95% 접근하는 smallest } \\|\\mathbf{h}\\| .\\] 11.4 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models) 여기서는 총 9가지의 변동도 모형을 소개한다. 그러나 실제로 널리 쓰이는 모델은 ‘exponential’과 ’matern’ 두 개이다. Linear \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\| &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] \\(\\lim_{\\|\\mathbf{h}\\| \\rightarrow \\mathbf{0}}\\gamma(\\mathbf{h}) \\rightarrow \\infty\\)이므로 이것은 비정상과정의 변동도라는 것을 알 수 있다. (대신 내재정상과정이다) Spherical Figure 11.2: Spherical model. \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}[\\frac{3}{2}(\\frac{\\| \\mathbf{h}\\|}{R})-\\frac{1}{2}(\\frac{\\| \\mathbf{h}\\|}{R})^{3}] &amp; \\textrm{if $0 \\leq \\|\\mathbf{h}\\|\\leq R$}\\\\ c_{0}+c_{1} &amp; \\textrm{if $\\|\\mathbf{h}\\|&gt;R$}\\\\ \\end{array} \\right. \\] 이것은 지구통계학(geostatstics)에서는 많이 쓰나 모형이 복잡해 분포 가정 후 우도 추정(likelihood estimation)을 할 때 어렵고 잘 안되므로 통계학자들은 잘 안 좋아하는 모형이라고 한다. 참고로 이 변동도는 \\(\\mathbf{h} \\in \\mathbb{R}^{d}, d=1,2,3\\)에서만 유효하다. 이것 이외에 나머지 변동도의 예들은 다 모든 양수 \\(d\\)에서 유효한 변동도를 갖는다고 한다. Exponential Figure 11.3: Exponential model. \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-\\frac{\\|\\mathbf{h}\\|}{R}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] 이것은 평균제곱연속이나 평균제곱 미분 가능하지는 않다고 한다. 그리고 다른 애들보다 독립적인 편이라고(공분산이 약함) 한다. 특히 뒤에 나오는 가우스 변동도(Gaussian variogram)와 비교했을 때 덜 부드럽다. Gaussian Figure 11.4: Gaussian model. \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-(\\frac{\\|\\mathbf{h}\\|}{R})^{2}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] 가우스분포랑은 관련이 없으며 2차항이 있기 때문에 이러한 이름이 붙은 것이다. 이것은 무한번 평균제곱 미분 가능(infinitely mean-square differentiable)하다. 그렇다는 얘기는 엄청 부드러운 데이터 모델링 시 사용할 수 있다는 것이다. 그러나 이런 경우는 드물므로 많이 보기는 힘들다. Exponential power \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-(\\frac{\\|\\mathbf{h}\\|}{R})^{p}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$, $0 &lt; p \\leq 2$}\\\\ \\end{array} \\right. \\] 이것은 exponential과 Gaussian의 중간쯤 되는 변동도로 \\(p\\)를 추정하는 것이 어려워 보통 미리 정하고 사용한다고 한다. Rational quadratic \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\|^{2}/(1+\\frac{\\|R\\|^{2}}{R}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] Wave \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-\\frac{R}{\\|\\mathbf{h}\\|}\\sin(\\frac{\\|\\mathbf{h}\\|}{R})) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] Power-law \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\|^{\\lambda} &amp; \\textrm{if $\\mathbf{h}\\neq 0$, $0\\leq \\lambda &lt; 2$}\\\\ \\end{array} \\right. \\] 이것은 선형모형(linear model)의 일반화(generalization)이며 \\(\\lambda=0\\)일 때에는 선형모형이다. 이것은 Local whittle model에서 감소율(decay rate) (\\(\\| \\mathbf{h} \\| \\rightarrow \\infty\\), \\(\\| \\mathbf{h} \\| \\rightarrow 0\\) 부근) 결정시 부분적으로 사용한다고 한다. (공분산을 일종의 준모수 모델링하는 것 같다) Matern (변동도가 복잡하므로 공분산 형태로 썼다) \\[C(\\mathbf{h})=\\sigma^{2}\\frac{2^{1-\\alpha}}{\\Gamma(\\alpha)}(\\frac{\\|\\mathbf{h}\\|}{\\phi})^{\\alpha}K_{\\alpha}(\\frac{\\|\\mathbf{h}\\|}{\\phi})\\] 참고로 \\[\\lim_{\\mathbf{h} \\rightarrow 0}C(\\mathbf{h}) = \\sigma^{2}\\] 이다. 이 때 \\(K_{\\alpha}\\)는 차수 \\(\\alpha\\)를 갖는 변형된 이형(second kind) 베셀 함수(Bessel function)라고 한다. 그리고 \\((\\phi&gt;0,\\alpha&gt;0)\\)는 각각 척도모수(scale parameter), 평활모수(smoothness parameter) (shape parameter, random process의 smootheness와 연결)라고 부르는데, 이름으로 그 역할들을 짐작할 수 있을 것이다. Matern 공분산함수(Matern covariance function)는 두 개의 모수가 있으므로 다양한 형태의 변동도 모델링이 가능하다. 이들은 때때로 정의가 바뀌기도 하므로 R-package를 쓸 때 체크가 필요하다. Matern 공분산함수를 갖는 \\(Z(\\mathbf{s})\\)는 \\(\\lceil \\alpha \\lceil -1\\)번 평균제곱 미분가능하다. (\\(0&lt;\\alpha \\leq 1\\)인 경우에는 평균제곱연속) 이 말은 즉 \\(\\alpha\\)를 조절해 공분산 함수의 평활도(smoothness)를 조절할 수 있다는 뜻이다. \\(\\alpha=0.5\\)인 경우에는 지수모형(exponential model), \\(\\alpha \\rightarrow \\infty\\)인 경우에는 가우스모형(Gaussian model)이 된다. (\\(\\alpha\\)가 커질수록 differentiability 또한 올라간다) \\(\\alpha\\)가 half-integer인 경우 explicit form이 있다고 한다. \\(\\phi=1\\)일 때 \\[ \\begin{array}{ll} \\alpha=0.5: &amp; e^{-\\|\\mathbf{h}\\|}\\\\ \\alpha=1.5: &amp; (1+\\|\\mathbf{h}\\|)e^{-\\|\\mathbf{h}\\|}\\\\ \\alpha=2.5: &amp; (1+\\|\\mathbf{h}\\| + \\frac{\\|\\mathbf{h}\\|^{2}}{3})e^{-\\|\\mathbf{h}\\|}\\\\ \\end{array} \\] 11.4.1 변형된 이형 베셀에 대한 보충 설명(addtional explanation for K_alpha) 변형 베셀 함수(modified Bessel function)은 변형 베셀 방정식(modified Bessel equation)의 해이다. 변형 베셀 방정식(modified Bessel equation)은 다음의 \\[x^{2}y&#39;&#39;+xy&#39;-(x^{2}+\\alpha^{2})y=0\\] 미분방정식의 해이다. (original은 \\(x^{2}+\\alpha^{2}\\) 부분이 다르다고 한다) 이 해는 두 개가 존재한다. \\[\\begin{array}{ll} I_{\\alpha}(x): &amp; \\text{exponentially growing}\\\\ K_{\\alpha}(x): &amp; \\text{exponentially decaying}\\\\ \\end{array} \\] 우리는 감소하는 형태의 함수를 필요로 하므로 \\(K_{\\alpha}(x)\\)를 사용하는 것이다. 추가적으로 \\(x &gt;&gt; [\\alpha^{2}-\\frac{1}{4}]\\)이면 \\[K_{\\alpha}(x) \\sim \\sqrt{\\frac{\\pi}{2x}}e^{-x}\\] 가 되고, \\(0&lt;x&lt;&lt;1\\)이면 \\[ K_{\\alpha}(x) \\sim \\left\\{ \\begin{array}{ll} -\\log (\\frac{x}{2}) - c &amp; \\textrm{if $\\alpha$=0}\\\\ (\\frac{\\Gamma(\\alpha)}{2})(\\frac{2}{x})^{\\alpha} &amp; \\textrm{if $\\alpha$&gt;0}\\\\ \\end{array} \\right. \\] \\(\\alpha\\)를 추정하는 것은 쉽지 않다. 공간통계학에서는 보통 우도로 모수 추정을 하는데 이것을 하기 위해서는 공분산행렬의 역행렬 등을 계산해야 한다. 그런데 \\(\\alpha\\)가 클 경우 수치적 특이점(numerical singularity)이 많이 나와 \\(\\alpha\\) 추정이 어렵다. 그래서 보통 작은 숫자의 \\(\\alpha\\)를 고정한 후 많이 사용한다. 11.5 기타 다른 상황에서의 변동도들(variograms in other situation) 지금까지는 일변량(univariate)이고 정상성을 가지며 시간 구조(temporal structure)는 고려 안한 변동도만 살펴보았다. 그러나 그렇지 않은 경우에는 어떻게 할 것인가? 다변량(multivariate)인 경우 한 장소에서의 관측값이 여러 개 일 수도 있다. 즉 한 지점에서 미세먼지 농도만 관측한 것이 아니라 강수량도 같이 관측한 경우도 있을 수 있다. 이런 경우의 변동도는 상호의존성(interdependence)을 고려해야 한다. 같은 장소와 다른 장소 사이의 dependence와 관측값 사이의 dependence 등도 고려해야 한다. 이런 경우의 변동도들을 공변동도(covariogram)이라고 부른다. 시공간자료(spatio-temporal data)인 경우 가장 간단한 모델은 분리가능한(separable) 모델이다. 이 모델은 시간과 공간 공분산이 독립이라는 것이다. \\[C(h,t;\\theta)=C_{s}(h)C_{t}(t).\\] 그러나 이 모델은 어느 위치에 있던지 시간 의존성이 같다는 매우 강한 가정을 갖는 것이다. 그렇지 않은 모형들을 분리 가능하지 않은(non-separable) 모델이라고 부른다. 스케일(scale)이 커지는 경우 전 지구적 수준의 데이터를 다룰 경우 구면좌표계를 사용할 필요가 있다. 구(sphere)나 다양체(manifold) 상에서 정의됨을 고려해야 하며 어떤 거리(distance)를 쓸 것인지에 대해서도 생각해 봐야 한다. References "],
["variogramest.html", "Chapter 12 변동도의 추정 12.1 경험변동도(empirical variogram) 12.2 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram) 12.3 R 예제(R-variogramest)", " Chapter 12 변동도의 추정 이 문서에서는 variogram을 어떻게 추정할 것인지에 대해 다룰 것이다. 크게 두 가지가 있다. Empirical model (nonparametric) Parametric fit 그리고 (Gelfand et al. 2010)의 33쪽부터, (N. A. C. Cressie 1993)의 69쪽부터 참고했다. 12.1 경험변동도(empirical variogram) 이것은 변동도를 비모수 추정하는 것이다. 다시 한 번 변동도의 정의를 살펴보면 \\[2\\gamma(\\mathbf{h})=\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))\\] 으로 lag \\(\\mathbf(h)\\)에만 의존하는 함수이다. 그런데 내재정상성(instinsic stationary)에서는 평균이 0이므로 \\[2\\gamma(\\mathbf{h})=E[Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))^{2}]\\] 이 된다. 다음은 추정량을 구하기 위한 방법들이다. 적률 추정(Metohd of moment (MoM) estimation (Matheron, 1962)) \\(E(Z(\\mathbf{s}))=\\mu\\)라는 상수 평균(constant mean) 가정하에 적률추정량(MoM estimator)은 \\[2\\hat{\\gamma}(\\mathbf{h})=\\frac{1}{|N(\\mathbf{h})|}\\sum_{(s_{i},s_{j})\\in N(\\mathbf{h})}(Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j}))^{2}, \\mathbf{h}\\in \\mathbb{R}^{d}\\] 이다. 여기서 \\(N(\\mathbf{h})\\)는 거리가 \\(\\mathbf{h}\\)가 되는 \\((\\mathbf{s}_{i},\\mathbf{s}_{j})\\)들의 집합이다. 즉 \\[N(\\mathbf{h})=\\{ (\\mathbf{s}_{i},\\mathbf{s}_{j}), \\mathbf{s}_{i}-\\mathbf{s}_{j}=\\mathbf{h} \\}\\] 이다. \\(N(\\mathbf{h}) \\neq N(\\mathbf{-h})\\)임에 주의하자. 이 추정량의 문제는 정규 격자(regular grid) 자료에만 잘 적용된다는 점이다. Irregular한 자료에서는 \\(\\mathbf{h}\\)에 대응되는 \\(N(\\mathbf{h})\\)가 공집합(empty set)일 수도 있다. 그런 상황을 해결하기 위해 \\(\\mathbf{h}\\)의 적당한 근방(neighborhood) \\(T(\\mathbf{h})\\)을 생각하여 \\(N(\\mathbf{h})\\)를 정의하기도 한다. \\[N(\\mathbf{h})=\\{ (\\mathbf{s}_{i},\\mathbf{s}_{j}), \\mathbf{s}_{i}-\\mathbf{s}_{j}=T(\\mathbf{h}) \\} .\\] 그렇다면 이 근방의 size는 어떻게 정해야 할 것인가라는 질문이 생길 수도 있다. 이것은 띠너비 선택(bandwidth selection) 문제와 유사하다. Practically하게 (Journel and Huijbregts 2003)는 \\(| \\cup \\{N(\\mathbf{h}): \\mathbf{h} \\in T(\\mathbf{h}) \\} |\\)에 들어가는 distinct pair들이 적어도 30개 이상이 되도록 잡는 것이 좋다고 하였다. 그러나 이 경우도 역시 데이터의 사이즈가 적을 경우 문제가 된다. 또한 \\(\\mathbf{h}\\)의 방향도 고려할 경우 자료가 더 부족해지고, \\(\\mathbf{h}\\)에 따라 pair 갯수 또한 차이가 난다. 그리고 자료로 인해 관측할 수 있는 \\(\\mathbf{h}\\)의 minimum과 maximum length가 존재한다. 역시 practically하게 \\(\\mathbf{h}\\)는 observation location들의 maximum length의 절반 정도를 고르도록 권장하고 있다. 마지막으로 이 추정량의 성질에 대해 알아보자. 우선 unbiased하다(특히 grid 자료인 경우). 그러나 outlier에 로버스트하지는 않다. \\(Z(\\mathbf{s})\\)가 Gaussian distribution이면 \\[(Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j}))^{2} \\sim 2 \\gamma(\\mathbf{h})\\chi_{1}^{2}\\] 이다. 그런데 카이제곱 분포는 매우 skewed된 분포인데 이 분포를 sample mean을 이용해 추정했으므로 finite sample 이용시 variation이 클 수 있다. 로버스트 추정량(robust estimator (Cressie and Howkins, 1980)) 이 문제를 해결하기 위해 Cressie와 Howkins는 robust한 통계량을 제시하였다. \\[2 \\bar{\\gamma}(\\mathbf{h})=\\frac{1}{0.457+\\frac{0.494}{|N(\\mathbf{h})|}}\\{\\frac{1}{|N(\\mathbf{h})|}\\sum_{(\\mathbf{s}_{i},\\mathbf{s}_{j} \\in N(\\mathbf{h}))} |Z(\\mathbf{s}_{i}-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\}^{4}.\\] 앞의 \\(\\frac{1}{0.457+\\frac{0.494}{|N(\\mathbf{h})|}}\\)는 bias correction term이다. 이 추정량의 아이디어는 다음과 같다. 어떤 확률변수 \\(X \\sim \\chi_{1}^{2}\\)때 \\(X^{\\frac{1}{4}}\\)는 거의 symmetric임을 보일 수 있다고 한다. 즉 \\(|Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{2}\\)보다는 \\(|Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\)이 더 symmetric하게 행동할 수 있을 것이다. 따라서 이것을 이용하고자 하는 것이다. \\(\\mathbf{X}_{n}\\)을 \\(X_{n}\\equiv\\frac{1}{|N(\\mathbf{h})|}\\sum_{(\\mathbf{s}_{i},\\mathbf{s}_{j} \\in N(\\mathbf{h}))} |Z(\\mathbf{s}_{i}-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\) 이라고 하자. 다음은 \\(X \\sim \\chi_{1}^{2}\\)시 몇 가지 계산 결과이다. \\[E(X^{\\frac{1}{4}})=0.82216, \\text{Var}(X^{\\frac{1}{4}})=0.12192, E(X_{n})=0.82216 \\equiv \\nu\\] \\[\\text{Var}(X_{n})=\\frac{0.12192}{|N(\\mathbf{h}|)} \\text{(cross-covariance 무시할 경우)} .\\] 그 다음 \\(f(x)=x^{4}\\)에 대해 \\(\\nu\\) 근방에서 테일러 전개를 해보자. 그러면 \\[f(X_{n})\\circeq f(\\nu) +f&#39;(\\nu)(X_{n}-\\nu)+\\frac{1}{2}f&#39;&#39;(\\nu)(X_{n}-\\nu)^{2} .\\] 여기서 \\(X_{n}\\)만 random이다. 기댓값을 취하면 \\[ \\begin{aligned} E(X_{n})^{4}&amp;\\circeq f(\\nu) + f&#39;(\\nu)E(X_{n}-\\nu) +\\frac{1}{2}f&#39;&#39;(\\nu)E(X_{n}-\\nu)^{2}\\\\ &amp;\\circeq 0.457 + 0 +\\frac{0.494}{|N(\\mathbf{h})|} \\text{(second order까지 bias correction)}\\\\ \\end{aligned} \\] 따라서 robust estimator 앞에 bias correction을 위한 숫자항이 붙는 것이다. 또 다른 로버스트 추정량(another robust estimator) 특별한 이름은 없으며 앞 estimator에서 약간 변형한 형태이다. \\[ \\begin{aligned} 2\\tilde{\\gamma}(\\mathbf{h})&amp;=\\frac{1}{0.457}\\text{Median}\\{ |Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{2} , (\\mathbf{s}_{i},\\mathbf{s}_{j})\\in N(\\mathbf{h}) \\}\\\\ &amp;=\\frac{1}{0.457}\\{\\text{Median} \\{ |Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}} \\}^{4} \\} \\end{aligned} \\] Median을 쓸 경우 제곱근을 한 다음 네제곱을 하거나 그냥 제곱을 하거나 차이가 없다고 한다. 12.2 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram) 경험변동도(empirical variogram) 자료들을 가지고 왜 또 모수적(parametric)인 적합(fitting)을 하려고 할까? 지구통계학(geostatistics)에서는 종속구조(dependence structure) \\(\\boldsymbol{\\sigma}\\)를 이용한 예측(prediction)에 관심이 있다. 그런데 예측을 하려면 \\(\\boldsymbol{\\sigma}^{-1}\\)이 필요하다. 그런데 경험변동도로 하면 \\(\\boldsymbol{\\sigma}\\)가 비음정치(non-negative definite)가 아니거나 수치적 특이성(numerical singularity)이 생긴다. 일반적으로 다음과 같은 모수 모델 \\[\\hat{\\boldsymbol{\\gamma}}(h;\\hat{\\boldsymbol{\\theta}})\\] 는 양정치함수(positive definite function)임을 보장한다(물론 numerical singular한 경우도 있을 수는 있다). 이러한 이유로 공간통계학에서는 모수를 이용한 모델링을 선호하는 것이다. 다음과 같이 \\(\\hat{\\gamma}(\\mathbf{h}_{1}), \\cdots , \\hat{\\gamma}(h_{m})\\)이 available하다고 하자(이들을 새로운 자료로 생각해도 좋다). 여기서 \\(m\\)은 고정시킨다. 우리의 목표는 \\(\\boldsymbol{\\gamma}(h;\\boldsymbol{\\theta})\\)가 true model일 때 \\(\\hat{\\boldsymbol{\\gamma}}(h;\\hat{\\boldsymbol{\\theta}})\\)를 만들고자 한다. 추정 방법은 크게 세 가지가 있다. 12.2.1 LS method \\[ \\begin{aligned} \\hat{\\boldsymbol{\\theta}}_{LS}&amp;=\\text{argmin}_{\\boldsymbol{\\theta}}\\sum_{j=1}^{m}\\{ \\hat{\\gamma}(h_{j})-\\gamma(h_{j};\\boldsymbol{\\theta})\\}^{2}\\\\ &amp;=\\text{argmin}_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^{T}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\\\\ \\end{aligned} \\] 이 방법은 \\(\\hat{\\gamma}(\\mathbf{h}_{i})\\)들의 종속성(dependency)을 무시한다. 12.2.2 GLS method 최소자승법의 약점을 보완하기 위한 방법이다. \\[\\hat{\\boldsymbol{\\theta}}_{GLS}=\\text{argmin}_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^{T}V^{-1}(\\boldsymbol{\\theta})(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\\] 여기서 \\(V^{-1}(\\boldsymbol{\\theta})=\\text{Var}(\\hat{\\boldsymbol{\\gamma}})\\)이다. 일반적인 GLS는 \\((\\hat{y}-X\\beta)^{T}V^{-1}(\\theta)(\\hat{y}-X\\beta)\\)꼴처럼 \\(\\beta\\)와 \\(\\theta\\)가 다르지만, 이 경우는 두 개가 \\(\\theta\\)로 같다. 12.2.3 WLS method \\[\\hat{\\boldsymbol{\\theta}}_{GLS}=\\text{argmin}_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^{T}W(\\boldsymbol{\\theta})(\\boldsymbol{\\theta})(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\\] 여기서 \\(W(\\boldsymbol{\\theta})=\\text{diag}(V(\\boldsymbol{\\theta}))\\)이다. 이것은 highly nonlinear한 object function이라 \\(\\boldsymbol{\\theta}\\)가 많을수록 적합이 어려워진다. GLS처럼 \\(\\beta\\)와 \\(\\theta\\)가 같으므로 two-stage iteration을 통해 적합한다. Iteration procedure는 다음과 같다. \\[\\hat{\\boldsymbol{\\theta}}^{(k+1)}=\\text{argmin}_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))^{T}V^{-1}(\\hat{\\boldsymbol{\\theta}}^{(k)})(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\\] OLS로 \\(\\boldsymbol{\\gamma}(\\boldsymbol{\\theta})\\) 부분을 먼저 고정시키고 iteration을 돌린다는 것 같다. 12.2.4 근사 WLS (approximated WLS) (N. Cressie 1985)는 WLS의 계산의 어려움을 피하기 위해 다음과 같은 근사 WLS 방법을 쓰기도 한다. \\[\\hat{\\boldsymbol{\\theta}}=\\text{argmin}_{\\boldsymbol{\\theta}}\\sum_{j}\\frac{|N(\\mathbf{h}_{j})}{\\gamma^{2}(\\mathbf{h}_{j};\\boldsymbol{\\theta})}\\{ \\hat{\\gamma}(\\mathbf{h}_{j})-\\gamma(\\mathbf{h}_{j};\\boldsymbol{\\theta}) \\}^{2}\\] 이 근사가 가능한 이유는 \\(\\text{Var}(\\hat{\\gamma(\\mathbf{h}_{j})}) \\cong \\frac{8 \\gamma^{2}(\\mathbf{h}_{j}l\\boldsymbol{\\theta})}{|N(\\mathbf{h}_{j})|}\\)이기 때문이다(8은 상수라 무시해도 수렴함). 그러나 여전히 두 군데에 \\(\\boldsymbol{\\theta}\\)가 있어 two-stage iteration을 해야 한다. 이렇게 계산한 AWLS 추정량이 (어떤 조건 하에) asymptotic normal이 됨을 보일 수 있다고 한다. 그런데 이런 경우에도 \\(\\hat{\\gamma}(\\mathbf{h}_{j}) \\stackrel{n \\rightarrow}{\\rightarrow} \\gamma(\\mathbf{h};\\boldsymbol{\\theta})\\)를 따로 보여야 한다고 한다. 몇 가지 기본 가정들은 다음과 같다. -\\(\\mathbf{\\gamma}(\\mathbf{h}_{1}), \\cdots , \\mathbf{\\gamma}(\\mathbf{h}_{m})\\) 이 \\(\\mathbf{h}_{1}, \\cdots , \\mathbf{h}_{m}\\)에서 계산되어있다. 여기서 \\(m\\)은 고정되어 있다고 가정한다. -\\(|N(\\mathbf{h}_{j})|=O(n)\\). 좀 더 자세하게는 \\(N(\\mathbf{h}_{j})=n\\cdot \\phi_{j,n}\\)이고 \\(\\lim_{n \\rightarrow \\infty}\\phi_{j,n}=\\phi_{j} &lt; \\infty\\)이다(그렇지 않으면 \\(O(n)\\)의 order가 올라갈 것이다). 말로 설명하자면 데이터가 커질 때마다 밀도가 일정해야 하므로 region도 커져야 한다는 것이다. 참고로 (N. Cressie 1985)에서는 다음과 같이 적혀 있다. “\\(|N(\\mathbf{h}_{j})| \\rightarrow \\infty\\) for each \\(j=1, \\cdots, k\\) as \\(N \\rightarrow \\infty\\) as \\(|D| \\rightarrow \\infty\\) such that \\(N/|D|\\), the sampling rate per unit area is constant.” 그리고 data가 evenly spaced 되어야 이 방법이 잘 맞는다고 한다. \\(\\text{Cov}(\\hat{\\gamma}(\\mathbf{h}_{j}), \\hat{\\gamma}(\\mathbf{h}_{k}))=O(\\frac{1}{n})\\). 여기서 \\(\\text{Cov}(\\hat{\\gamma}(\\mathbf{h}_{j}), \\hat{\\gamma}(\\mathbf{h}_{k})) \\sim \\frac{U_{jk}(\\boldsymbol{\\theta})}{n}\\)을 만족하는 \\(m \\times m\\) 행렬 \\(U(\\boldsymbol{\\theta})\\)가 존재한다. Decay 속도가 충분히 빠른 variogram model들 (ex. exponential)이 이 조건을 만족한다고 한다. 이 세 가지 조건을 만족할 경우 \\[\\sqrt{n}(\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta})) \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(\\mathbf{0}, U(\\boldsymbol{\\theta}))\\] \\(m\\)이 커지면 \\((\\hat{\\boldsymbol{\\gamma}}-\\boldsymbol{\\gamma}(\\boldsymbol{\\theta}))\\)의 dimension 또한 커져 복잡한 문제가 되므로 \\(m\\)을 고정하는 것이다. \\[\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}) \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(\\mathbf{0}, H^{-1}R^{T}URH^{-1})\\] 여기서 \\(R\\)은 \\(m \\times p\\) (p: number of parameter) 행렬인데 \\[R_{ij}=\\frac{2\\phi_{i}}{\\gamma^{2}(\\mathbf{h}_{i};\\boldsymbol{\\theta})}\\cdot \\frac{2\\gamma}{2\\boldsymbol{\\theta}_{j}}(\\mathbf{h}_{j};\\boldsymbol{\\theta})\\] 가 성립한다. 한편 \\(H\\)는 목적함수 \\(S_{n}(\\boldsymbol{\\theta}*)\\)의 헤시안의 확률수렴 값이다. \\[H:\\frac{\\nabla^{2}S_{n}(\\boldsymbol{\\theta}*)}{n} \\stackrel{P}{\\rightarrow} H(\\boldsymbol{\\theta})\\] 이다. 참고로 목적함수의 asymptotic은 보통 다음과 같이 한다. Consistency는 따로 보이고 이것을 만족하면 그 다음 테일러 전개로 \\[S_{n}&#39;(\\boldsymbol{\\theta})=S_{n}&#39;(\\hat{\\boldsymbol{\\theta}})+S_{n}&#39;&#39;(\\boldsymbol{\\theta}*)(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta})\\] 를 만든다 (mean value theorem에 의해 등호 성립). 이때 \\(S_{n}&#39;(\\hat{\\boldsymbol{\\theta}})\\)은 minimization 문제이므로 0이 된다. \\(S_{n}&#39;&#39;(\\boldsymbol{\\theta}*)\\)는 \\(H\\)에 해당된다. 따라서 \\(S_{n}&#39;(\\boldsymbol{\\theta})\\)의 분포만 알아내면 되는 것이다. 이러한 방법을 “Sandwich method”라고 한다. 자세한 내용은 (Fuentes, 2007) 강좌의 4장을 보면 된다. 12.3 R 예제(R-variogramest) 이제 실제 R 예제에 대해 살펴보자. data(s100) s100.v1 &lt;- variog(s100, option=&quot;cloud&quot;) &gt; variog: computing omnidirectional variogram s100.v2 &lt;- variog(s100, max.dist=1, estimator.type=&quot;classical&quot;) &gt; variog: computing omnidirectional variogram s100.v3 &lt;- variog(s100, max.dist=1, estimator.type=&quot;modulus&quot;) &gt; variog: computing omnidirectional variogram par(mfrow=c(2,2)) plot(s100$coords[,1], s100$coords[,2], pch=&quot;x&quot;, xlab=&quot;&quot;, ylab=&quot;&quot;, main=&quot;locations&quot;) plot(s100.v1, main=&quot;Variogram cloud&quot;) plot(s100.v2, main=&quot;MoM estimator&quot;) plot(s100.v3, main=&quot;Robust estimator&quot;) Figure 12.1: Various variogram estimation methods. true &lt;- 1-matern(seq(0,1,0.1),0.3,0.5) ols &lt;- variofit(s100.v2, ini=c(0.9,0.2), cov.model=&quot;mat&quot;, fix.kappa=F, kap=1.5, nug=0.2, weights=&quot;equal&quot;) &gt; variofit: covariance model used is matern &gt; variofit: weights used: equal &gt; variofit: minimisation function used: optim wls &lt;- variofit(s100.v2, ini=c(0.9,0.2), cov.model=&quot;mat&quot;, fix.kappa=F, kap=1.5, nug=0.2, weights=&quot;cressie&quot;) &gt; variofit: covariance model used is matern &gt; variofit: weights used: cressie &gt; variofit: minimisation function used: optim par(mfrow=c(1,1)) plot(s100.v2, main=&quot;&quot;,col=&quot;blue&quot;, lwd=2) lines(seq(0,1,0.1), true, col=&quot;red&quot;) lines(ols, lwd=1.5) lines(wls, lty=2, lwd=1.5) legend(&quot;bottomright&quot;, c(&quot;true&quot;, &quot;ols&quot;, &quot;wls&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;black&quot;), lty=c(1,1)) Figure 12.2: Comparison of variogram estimation methods. References "],
["extremevaluestat.html", "Chapter 13 극단값 통계학", " Chapter 13 극단값 통계학 통계학자 존 튜키(John Tukey)는 이렇게 말했다. “나는 모든 지구물리학자들이 실제 오차나 변동성의 분포들이 가우스(Gauss)나 라플라스(Laplace)가 만든 매끄러운 종 모양의 분포보다 훨씬 더 극단값 분포에 가까운 모양을 갖고 있음을 알고 있다고 확신한다.” "],
["uGEVtheory.html", "Chapter 14 일변량 극단값 이론 14.1 일반화 극단값 분포(generalized extreme value distribution) 14.2 최대안정성(max-stablity) 14.3 복귀 수준(return level) 14.4 극단값 분포에서의 추론(inference in extreme value statistics)", " Chapter 14 일변량 극단값 이론 이 장은 일변량 극단값 분포 이론과 모델링에 관한 내용을 담고 있다. 특히 \\[M_{n}=\\max\\{X_{1},\\ldots,X_{n}\\}\\] 의 통계적 움직임에 초점을 맞출 것이다. 여기서 \\(X_{1},\\ldots,X_{n}\\)은 \\(F\\)라는 분포함수를 갖는 독립 확률변수의 수열이다. 일반적으로 \\(X_{i}\\)는 일정한 시간 스케일(예: 시간별, 일별, 월별 등)을 갖고 측정된 어떤 (확률)과정의 값들을 나타낸다. 정리하면 \\(M_{n}\\)은 \\(n\\) 시간 단위의 관측값 중 최대값을 나타낸다. \\(n\\)이 일년동한 관찰한 관측값 수라고 할 경우, \\(M_{n}\\)은 월 최대값에 대응된다. \\(X_{i}\\)들이 독립이라고 가정하였으므로 \\(M_{n}\\)의 분포는 \\begin{eqnarray*} P\\{M_{n} \\leq z \\} &amp;=&amp; P\\{X_{1}\\leq z, \\ldots, X_{n} \\leq z\\} \\nonumber\\\\ &amp;=&amp;P\\{X_{1}\\leq z\\}\\times \\cdots \\times P\\{X_{n}z\\} \\nonumber\\\\ &amp;=&amp;\\{F(z)\\}^{n}. \\end{eqnarray*} 가 된다. 그러나 이 식은 실제 도움이 되지 않는다. 왜냐하면 \\(F\\)의 진짜 분포가 어떤지를 모르기 때문이다. 여기서 우리는 관찰 데이터를 통해 \\(F\\)를 어떤 통계적 방법을 이용해 추정하고 그것으로 \\(F\\)를 대체할 가능성이 있는지 고민해볼 수 있다. 그렇지만 \\(F\\)의 변화가 아주 작더라도 \\(F^{n}\\)은 매우 많이 변하게 된다. 또 다른 접근방법으로는 \\(F\\)가 알려져 있지 않다는 것에 동의하고 극단값 자료만 가지고 \\(F^{n}\\)에 적합한 근사 모형이 있는지 찾아보는 방법이 있다. 이것은 표본 평균을 중심극한정리(central limit theorem)를 가지고 정규분포로 근사하는 것과 같은 방법이다. 즉 우리는 극단값 자료 분석에서도 중심극한정리와 같은 것이 있는지 살펴볼 필요가 있다. \\(n \\rightarrow \\infty\\)일 때 \\(F^{n}\\)의 움직임에 대해 살펴보자. 논리 전개를 위해 다음과 같은 논리가 추가로 더 필요하다. \\(z_{+}\\)를 \\(F(z)=1\\)로 만드는 가장 작은 값이라고 생각하자. 그러면 모든 \\(z &lt; z_{+}\\)에 대해 \\(n \\rightarrow \\infty\\)일 때 \\(F^{n}(z) \\rightarrow 0\\)이다. 결국 \\(M_{n}\\)은 \\(z_{+}\\)에서 점질량(point mass)을 갖는 퇴화분포(degenearte distribution)를 따르게 된다. 퇴화분포는 다루기 어렵기 때문에 다음과 같이 \\(M_{n}\\)에 대해 재정규화(re-normalization)를 한다. \\[M_{n}^{*}=\\frac{M_{n}-b_{n}}{a_{n}}.\\] 여기서 \\(\\{a_{n} &gt;0\\}\\)과 \\(\\{b_{n}\\}\\)은 적당한 상수열이다. \\(\\{a_{n}\\}\\)과 \\(\\{b_{n}\\}\\)은 \\(n\\)이 커짐에 따라 \\(M_{n}^{*}\\)의 위치(location)와 척도(scale)을 안정화시킬 수 있도록 잘 잡아준다. 만약 앞서 말한 상수열 \\(\\{a_{n} &gt;0\\}\\)과 \\(\\{b_{n}\\}\\)이 존재해 \\[P\\{(M_{n}-b_{n})/a_{n}\\leq z\\} \\rightarrow G(z), \\qquad{n \\rightarrow \\infty}\\] 가 성립한다고 가정하자. 여기서 \\(G\\)는 퇴화분포가 아닌 어떤 분포함수이다. 이 때, \\(G\\)다음 세 개의 족(family) 중 하나를 따른다. \\[\\textbf{굼벨(Gumbel): }G(z)=\\exp\\{\\exp[-(\\frac{z-b}{a})\\}, \\qquad{-\\infty &lt; z &lt; \\infty}.\\] \\[\\textbf{프레셰(Frechet): }G(z)= \\begin{cases} 0, &amp; \\text{z $\\leq$ b}\\\\ \\exp\\{-(\\frac{z-b}{a})^{-\\alpha}\\}, &amp; \\text{z &gt; b} \\end{cases}\\] \\[\\textbf{와이블(Weibull): }G(z)= \\begin{cases} \\exp\\{-[-(\\frac{z-b}{a})^{-\\alpha}]\\}, &amp; \\text{z &lt; b}\\\\ 1, &amp; \\text{z $\\geq$ b} \\end{cases}\\] 이 때 \\(a&gt;0\\), \\(b\\)는 모수들이고 \\(\\alpha &gt;0\\)이다. Figure 14.1: Plot of three GEV distributions. 위 정리는 재정규화 시킨 표본 최대값들 \\(M_{n}^{*}=\\frac{M_{n}-b_{n}}{a_{n}}.\\)이 세 가지 분포족 중 하나로 분포수렴할 것임을 알려주고 있다. \\(M_{n}^{*}\\)의 극한 분포는 놀랍게도 \\(F\\)에 상관없이 항상 저 세가지 분포족 중 하나로 수렴한다. 세 가지 분포의 이름은 앞으로 자주 등장할 것이므로 기억해두자. 참고로 이 적당한 \\(a_{n}\\), \\(b_{n}\\)은 존재하지 않을 수도 있다. (예: 포아송 분포) \\(X_{1}, X_{2}, \\ldots\\)가 정규지수분포\\((=\\text{Exp}(1))\\)에서 추출된 확률변수의 수열이라고 하자. 참고로 \\(F(x)=1-\\exp^{x}\\) (\\(x&gt;0\\))이다. 이때 \\(a_{n}=1\\), \\(b_{n}=\\log n\\)이라고 하면 \\(n\\rightarrow \\infty\\)일 때 고정된 \\(z\\in\\mathbb{R}\\)에 대해 \\begin{eqnarray} P\\{(M_{n}-b_{n})/a_{n} \\leq z \\} &amp;=&amp; F^{n}(z+\\log n) \\nonumber\\\\ &amp;=&amp;[1-e^{-(z+\\log n)}]^{n} \\nonumber\\\\ &amp;=&amp;[1-n^{-1}e^{-z}]^{n} \\nonumber\\\\ &amp;\\rightarrow&amp; \\exp(-e^{-z}) \\end{eqnarray} 이다. 즉 \\(a_{n}\\)과 \\(b_{n}\\)을 위와 같이 선택하였을 때 \\(M_{n}\\)의 극한분포는 굼벨분포가 된다. \\(X_{1}, X_{2}, \\ldots\\)가 표준정규분포에서 추출된 확률변수의 수열이라고 하자. 이때 \\[a_{n}=(2\\log n)^{-0.5}, b_{n}=(2\\log n)^{0.5}-0.5(2\\log n)^{-0.5}(\\log\\log n +log 4\\pi)\\] 로 놓을 경우 재정규화된 \\(M_{n}\\) 또한 굼벨분포로 수렴함이 알려져 있다. 그러나 앞 예제와 비교하였을 때 수렴 속도는 현저히 느리다. \\(M_{n}\\)의 수렴속도를 체크하는 것은 중요한 일이다. 왜냐면 우리는 일반화 극단값 분포를 유한개의 표본 최대값들의 점근 분포로 생각하고 사용할 것이기 때문이다. 결국 일반화 극단값 분포를 사용하기 위해 얼마나 많은 데이터가 필요할 것인가라는 문제는 \\(M_{n}\\)의 수렴속도와 관계된다. 14.1 일반화 극단값 분포(generalized extreme value distribution) 앞서 말한 \\(G\\)의 극한값에서의 행동은 분포에 따라 달라진다. 예를 들면, 굼벨분포와 프레셰분포는 \\(z_{+}=\\infty\\)이나 와이블분포는 \\(z_{+}&lt;\\infty\\)이다. 그리고 굼벨분포에서는 \\(G\\)의 분포가 지수적으로 감소(exponentially decay)하나 프레셰분포에서는 다항식 차수로 감소(polynomially decay)한다. 즉 족이 달라지면 극단값의 움직임 또한 다르게 표현될 것임을 알려준다. 그런데 이렇게 족 별로 따로 나누어 분석하는 것은 두 가지 문제점을 지닌다. 첫째, 데이터를 보고 어떤 족으로 분석하는 것이 적절한지 미리 정해야 한다. 둘째, 이러한 결정 후에는 이 결정이 맞다는 전제 하에 추론이 진행되고 이 선택이 가지는 불확실성을 배제하게 된다. 따라서 일반적으로는 앞 정리에 나왔던 식을 재구성하여 다음과 같이 하나의 함수로 표현하여 분석하게 된다. \\[G(z)=\\exp\\{-[1+\\xi(\\frac{z-\\mu}{\\sigma})]^{-1/\\xi}\\}.\\] 이 때 \\(\\{z: 1+\\xi(z-\\mu)/sigma &gt;0\\}\\)이며 \\(-\\infty &lt; \\mu &lt; \\infty\\), \\(\\sigma &gt;0\\) 그리고 \\(-\\infty &lt;\\xi &lt;\\infty\\)를 만족한다. 이 분포를 일반화 극단값 분포(generalized extreme value distribution)이라고 부른다. 여기서 \\(\\mu\\)는 위치모수(location parameter), \\(\\sigma\\)는 척도모수(scale parameter), 그리고 \\(\\xi\\)는 형태모수(shape parameter)라고 한다. 특히 \\(\\xi\\)는 이 분포가 굼벨족(\\(\\xi=0\\)), 프레셰족(\\(\\xi&gt;0\\)) 또는 와이블족(\\(\\xi&lt;0\\))이 될지 결정하는 역할을 한다. 14.2 최대안정성(max-stablity) 모든 \\(n=2,3,\\ldots,\\)에서 상수들 \\(\\alpha_{n}&gt;0\\), \\(\\beta_{n}\\)이 존재해 \\[G^{n}(\\alpha_{n}z+\\beta_{n})=G(z)\\] 일 경우 분포 G를 최대안정(max-stable)하다고 부른다. 즉 \\(G\\)에 거듭제곱을 하는 것은 단지 위치모수와 척도모수의 변화만 야기한다는 것이다. 최대안정성과 일반화 극단값 분포 사이에는 다음과 같은 정리가 있다. 어떤 분포가 최대안정한 것은 분포가 일반화 극단값 분포임과 동치이다. (Coles 2001) 14.3 복귀 수준(return level) 앞서 극단값 분석은 극단적인 사건들을 모델링하고 적합한 위험 평가를 해 줄 수 있는 통계적 방법을 제공한다고 설명하였다. 그렇다면 우리는 어떤 방법을 통해 이 위험(risk)를 측정할 수 있을까? 한 가지 방법으로 복귀 수준(return level)이라는 것이 있다. n-복귀 수준은 우리가 매 \\(n\\)년마다 한 번꼴로 초과될 것으로 기대되는 값이다. 다시 말하면 어떤 특정한 해에 그런 강도의 사건을 마주칠 확률이 \\(\\frac{1}{n}\\) 정도 될 때 \\(n\\)-복귀 수준이 된다. 분위수(quantile)로 봤을 때에는 \\(1-\\frac{1}{n}\\)분위수에 대응되는 사건이다. 20년 복귀 수준은 매 20년마다 최대 수준이 한 번꼴로 초과할 것으로 기대되는 수준에 대응된다. 이것은 매 년 \\(\\frac{1}{20}=0.05\\)의 확률로 그러한 사건이 마주칠 것으로 기대되며 \\(1-\\frac{1}{20}=0.95\\)의 분위수에 대응된다. 일반적으로 많이 쓰이는 복귀 수준은 \\(20, 50, 100\\)년 정도이다. 이 복귀 수준이라는 개념은 통계학에서 말하는 분위수와 일맥상통한다. \\(0&lt;p&lt;1\\)이라 할 때, 일반화 극단값 분포의 분위수 \\(x_{p}\\)는 \\[x_{p}=\\mu-\\frac{\\sigma}{\\xi}[1-\\{-\\log(1-p)\\}^{-\\xi}]\\] 가 된다. 이것은 \\(G(x_{p})=1-p\\)라 놓고 \\(x_{p}\\)에 대해 풀어 얻을 수 있다. 이 때 \\(x_{p}\\)를 \\(1/p\\) 복귀 주기(return period)에 대응되는 복귀 수준이라 부른다. 14.4 극단값 분포에서의 추론(inference in extreme value statistics) \\(k\\)개의 연 최대값 \\(X_{1}, \\ldots , X_{k}\\)들이 주어져 있을 때, 일반화 극단값 분포의 모수들 \\((\\mu, \\sigma, \\xi)\\)을 추론하는 문제를 생각해보자. 가능한 추론 방법들은 다음과 같다. 그래프 기반 방법: 전통적으로 중요했고, 지금도 기본적인 모형 파악에 유용하다. 적률 기반(moment-based) 추정량: 적률이 존재하지 않을 가능성이 있어 극단값 분석에 유용하지 않은 경우가 많다. 확률 가중 적률: 수문학(hydrology)에서 많이 쓰이나 복잡한 자료로 확장하기 어렵다. 가능도(likelihood) 기반 방법: 통계학에서 가장 많이 이용하는 방법이다. 복잡한 자료에 적용할 수 있고 일반적인 점근적 추정과 검정이론을 적용할 수 있다. 또한 베이지안 방법을 적용할 수도 있다. 일반화 극단값 분포를 가능도 기반 방법으로 분석할 때 정칙 조건(regularity condition)을 만족하는지 항상 확인해야 한다. 일반화 극단값 분포의 모수들로 표현된 값 \\(\\mu-\\sigma/\\xi\\)이 \\(\\xi\\neq 0\\)일때 분포의 끝점(end point)이 되기 때문에 정칙 조건을 만족하지 않을 수도 있다.(Smith 1985) 한편 형태모수 값과 최대가능도추정량의 존재 사이의 관계에 대해 다음과 같은 사실이 알려져 있다. \\(\\xi &gt; -0.5\\)인 경우 최대가능도추정량은 정칙조건을 만족하며 일반적인 점근적 성질을 갖는다. \\(-1 &lt; \\xi &lt; -0.5\\)인 경우 최대가능도추정량을 얻을 수는 있으나 일반적인 점근적 성질을 갖지는 않는다. \\(\\xi &lt; -1\\)인 경우 최대가능도추정량을 얻지 못할 가능성이 높다. References "],
["spatextremes.html", "Chapter 15 공간 극단값 이론과 최대안정과정 15.1 최대안정과정(max-stable process)", " Chapter 15 공간 극단값 이론과 최대안정과정 이 절은 최대안정과정(max-stable process) 및 이것의 공간 극단값(spatial extremes)이론으로의 응용에 초점을 맞춰 서술한다. 이 절의 서술 내용은 (Coles 2001)과 (Dey and Yan 2015)를 참고하였다. 15.1 최대안정과정(max-stable process) 앞서 나왔던 최대안정(max-stable)의 정의를 다시 살펴보자. 이번엔 (Dey and Yan 2015) 버전이다. \\(Z_{1}, Z_{2},\\ldots\\)를 확률과정 \\(\\{Z(x):x\\in\\mathcal{X} \\}\\)의 독립인 copy들의 수열이라고 하자. 만약 각각의 \\(n, (n\\geq 1)\\)에 대해 다음과 같은 \\(a_{n}&gt;0\\), \\(b_{n}\\in\\mathbb{R}\\)이 존재해 다음 \\begin{equation} \\frac{\\max_{i=1,\\ldots ,n}Z_{i}-b_{n}}{a_{n}}\\stackrel{d}{=}Z \\end{equation} 을 만족한다면 \\(\\{Z(x):x\\in\\mathcal{X} \\}\\)를 최대안정(max-stable)하다고 말한다. 다음은 (De Haan 1984)에 나오는 정리이다. \\(Y_{1}, Y_{2},\\ldots\\)를 continuous sample path를 갖는 \\(\\{Y(x):x\\in\\mathcal{X} \\}\\)의 독립인 copy들의 수열이라고 하자. 만약 연속함수 \\(c_{n}&gt;0, d_{n}\\in\\mathbb{R}\\)이 존재해 극한과정(limiting process) \\(\\{Z(x): x\\in\\mathcal{X}\\}\\)에 수렴한다면, 즉 \\begin{equation} \\frac{\\max_{i=1,\\ldots, n}Y_{i}(x)-d_{n}(x)}{c_{n}(x)}\\rightarrow Z(x), x\\in\\mathcal{X}, n\\rightarrow \\infty \\label{eq:dehaan} \\end{equation} 이고 \\(Z(x)\\)가 non-degenerate라면 \\(\\{Z(x):x\\in\\mathcal{X} \\}\\) 최대안정과정(max-stable process)이어야 한다. (\\ref{eq:dehaan})의 수렴은 \\(\\mathcal{X}\\)의 연속함수 공간에서의 약수렴(weak convergence)을 의미한다. 이 정리의 의미는 다음과 같다. 일변량 극단값 이론과 연관성을 생각해보면 \\(\\{Z(x):x\\in\\mathcal{X} \\}\\)는 일반화 극단값 분포여야 한다는 것이다. 공간 극단값 이론에서 최대안정과정을 고려하는 이유는 다음과 같다. \\(n\\)개의 독립 반복에서 극한과정(limiting process) \\(\\{Z(x): x\\in\\mathcal{X}\\}\\)이 \\(n\\)이 충분히 클 때 부분 최대 과정(partial maxima process)을 모델링 할 좋은 후보라고 여겨지기 때문이다. (margin이 unit Fréchet \\((F(z)=\\exp(-1/z))\\)이라고 가정하자.) 이러한 경우에 \\begin{equation} \\max_{i=1,\\ldots , n}n^{-1}Z_{i}(\\cdot) \\stackrel{d}{=} Z(\\cdot), n\\geq 1 \\end{equation} 일 경우 과정(process) \\(\\{Z(x)\\}\\)를 최대안정과정(max-stable process)이라고 부른다. 이 때 놀랍게도 최대안정과정에서 spectral characterization (뜻 찾아보기)를 얻는 것이 가능하다고 한다. ((De Haan 1984)의 characterization) \\(\\{ (\\xi_{i}, U_{i})\\}_{i \\geq 1}\\)을 \\((0,\\infty ] \\times \\mathbb{R}^{d}\\)에서 (intensity \\(d\\Gamma (\\xi, u)=\\xi^{-2}d\\xi\\nu(du), \\nu\\)는 \\(\\mathbb{R}^{d}\\)에서 \\(\\sigma\\)-finite measure)의 Poisson point process의 점들이라고 하자. 그리고 \\(\\{Z(x) \\}_{x\\in\\mathbb{R}^{d}}\\)는 unit Fréchet margin을 가지는 최대안정과정이라고 가정하고 continuous sample path에서 \\begin{equation} \\{Z(x)\\}_{x\\in\\mathbb{R}^{d}}\\stackrel{d}{=}\\{\\max_{i\\geq 1}\\xi_{i}f_{x}(U_{i})\\}_{x\\in\\mathbb{R}^{d}} \\end{equation} 를 만족한다. 그러면 다음과 같은은 음이 아닌 연속함수 \\[\\{ f_{x}(y):x,y\\in\\mathbb{R}^{d}\\}\\] 가 존재해 \\begin{equation} \\int_{\\mathbb{R}^{d}}f_{x}(y)\\nu(dy)=1, \\forall x \\in \\mathbb{R}^{d} \\end{equation} 를 만족한다. References "],
["references.html", "Chapter 16 References", " Chapter 16 References "]
]
