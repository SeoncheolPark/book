[
["index.html", "통계공부와 관련된 글들 Chapter 1 일러두기", " 통계공부와 관련된 글들 Seoncheol Park 2016-07-08 Chapter 1 일러두기 각 장은 독립된 구성으로 되어 있으며, 한 권 이상의 책들을 참고문헌으로 하여 그들의 정의 및 표현을 따라가는 방식으로 구성되어 있다. 따라서 각 장마다 표현 및 한국어 용어 번역이 상이할 수 있다. "],
["multiscale.html", "Chapter 2 다중척도 방법론 2.1 다중척도 변환(multiscale transform) 2.2 역(inverse) 2.3 희소성(sparsity) 2.4 R 예제(R-multiscale)", " Chapter 2 다중척도 방법론 이 장에서는 통계학에서의 다중척도 방법론을 다룬다. 주된 내용은 2015년 지도교수님의 특강 수업 내용이다. 이 문서에 담겨있는 그림들은 (Nason 2010)을 참고하였다. 2.1 다중척도 변환(multiscale transform) 다음과 같은 형태의 벡터 자료를 생각해보자. \\[ \\mathbf{y}=(y_{1},\\ldots,y_{n}), n=2^{J}\\] 여기서 \\(n=2^{J}\\)는 굉장히 강하고 불편한 조건이다. 예를 들어, 자료의 길이가 800개 또는 900개 정도라면 자료의 길이가 2의 배수라는 조건에 맞게 데이터를 일부를 버려야 한다. 또한 \\(\\mathbf{y}\\)는 등간격 자료(equally spaced data)여야 한다. 예를 들어, 시계열 자료의 경우 오늘 10시, 내일 10시에 관측된 값이 자료에 있으면 그 다음 값은 모레 10시에 관측된 값이여야 하며, 11시에 관측된 값이 와서는 안 된다는 것이다. 다중척도 방법론에서 알고 싶어하는 가장 중요한 정보는 각기 다른 척도(scale)와 위치(location)에서의 \\(\\mathbf{y}\\)의 상세(detail)이다. 여기서 척도는 수준(level), 해상도(resolution) 등으로 불리기도 하며 통계학 용어로 번역하자면 분산, 파워(power), 도수(frequency) 등으로 말할 수 있다. 장소라는 것은 관측값을 관찰한 정의역(domain)을 의미하며, 시간 자료면 시간, 공간 자료면 공간이 로케이션이 된다. 한편 상세의 정의는 다음과 같다. 주어진 자료 \\(\\mathbf{y}\\)의 상세(detail) \\(d_{k}\\)는 \\[d_{k}=y_{2k}-y_{2k-1},\\qquad{k=1,2,\\ldots,\\frac{n}{2}}\\] 이다. 다음과 같이 길이 8인 자료 \\(\\textbf{y}=(y_{1},y_{2},\\ldots,y_{8})\\)가 있다고 하자. 그러면 이 자료의 상세는 \\[d_{1}=y_{2}-y_{1}, d_{2}=y_{4}-y_{3}, d_{3}=y_{6}-y_{5}, d_{4}=y_{8}-y_{7}\\] 와 같이 4개가 존재한다. 여기서 특이한 점은 \\(y_{3}-y_{2}\\)와 같은 값들은 고려하지 않는다는 것이다. 이는 어떻게 관측하느냐에 따라 \\(d_{k}\\)가 완전히 달라질 수도 있다는 말이다. 즉 상세는 평행 이동 불변하지 않다(not translation invariant). 한편, 상세와 유사한 개념으로 성김(coarser)을 정의한다. 주어진 자료 \\(\\mathbf{y}\\)의 성김(coarser) \\(c_{k}\\)은 \\[c_{k}=y_{2k}+y_{2k-1},\\qquad{k=1,2,\\ldots,\\frac{n}{2}}\\] 이다. 성김은 매끄러움(smooth)으로 불리기도 하며, +의 개념이다. 상세는 차이(difference)로 불리기도 하며, -의 개념이다. \\(c_{k}\\)와 \\(d_{k}\\)를 알고 있으면 원래 자료들의 원소 \\(y_{i}\\)들도 다 알아낼 수 있다. 이렇게 다중척도 변환은 원래 신호를 재구성(reconstruction)할 수 있어야 한다. 한편, 앞선 예제에서처럼 자료의 길이가 8일 때, (특정 수준에서) 얻을 수 있는 최대 상세는 4개이다. 이것을 가장 섬세한 상세(finest-detail)라고 한다. 그런데 우리가 \\(c_{k}\\)를 이용해서 \\(d_{k}(=d_{J-1})\\)보다 좀 더 엉성한 상세를 얻고 싶을 수 있다. 그러면 그 것보다 낮은 수준, 즉 \\(J-2\\) 수준을 생각하면 된다. \\(J-2\\) 수준에서의 상세는 \\begin{eqnarray*} d_{J-2,l}&amp;=&amp;c_{J-1,2l}-c_{J-1,2l-1},l=1,2,\\ldots,\\frac{n}{4}\\\\ &amp;=&amp;(y_{4l}+y_{4l-1})-(y_{4l-2}+y_{4l-3})\\\\ \\end{eqnarray*} 로 정의된다. 예를 들어, \\(d_{J-2,1}=(y_{4}+y_{3})-(y_{2}+y_{1})\\)이다. 마찬가지로 \\(J-2\\) 레벨에서의 성김은 \\[ c_{J-2,l}=c_{J-1,2l}+c_{J-1,2l-1},l=1,2,\\ldots,\\frac{n}{4} \\] 이다. 앞서 말한 다중척도 과정을 그림으로 요약하면 다음과 같다. Figure 2.1: Generic step in multiscale transform. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)를 생각해보자. 그러면 \\(J=0,1,2\\)에서의 상세와 성김은 Figure 2.2: Graphical depiction of a multiscale transform. 로 구할 수 있다. 이는 Mallat이 1998년 개발하였으며 피라미드 알고리즘(pyramid algorithm)이라 부른다. 다중척도 변환은 다음과 같이 같은 차원의 새로운 벡터를 정의하는 과정으로 볼 수 있다. \\[(1,1,7,9,2,8,8,6) \\rightarrow (42,6,14,4,0,2,6,-2).\\] 바뀐 벡터를 살펴보면, 42는 평균(global trend)에 해당되고, 6은 \\(J=0\\)일 때의 상세, \\((14,4)\\)는 \\(J=1\\)일 때의 상세, \\((0,2,6,-2)\\)는 \\(J=2\\)일 때의 상세이다. \\(d_{j,k}\\)는 웨이블릿 계수(wavelet coefficient)로, \\(c_{j,k}\\)는 압축 계수(scaling coefficient) 또는 부드러움 계수(smooth coefficient)라 부른다. 여기서 \\(j\\)는 수준(level), 척도(scale), 또는 해상도(resolution)을 나타내며, \\(k\\)는 위치(location)를 나타낸다. 2.2 역(inverse) 우리는 \\(\\{ d_{j,k} \\}\\)와 \\(\\{ c_{j,k} \\}\\)를 가지고 \\(\\mathbf{y}\\)를 구할 수 있다. 이를 위해서는 \\[c_{j-1,2k}=\\frac{(c_{j-1,2k}+d_{j-2,k})}{2}, c_{j-1,2k-1}=\\frac{(c_{j-1,2k}+-d_{j-2,k})}{2}\\] 이 두 가지만 알고 있으면 된다. 앞서 다룬 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)를 생각해보자. 이것의 다중척도 변환 결과는 \\[(c_{01},d_{0,1},d_{11},d_{12},d_{21},d_{22},d_{23},d_{24})=(42,6,14,4,0,2,6,-2)\\] 였다. 이를 가지고 역변환을 해 보면, \\[c_{12}=\\frac{(42+6)}{2}=24, c_{11}=\\frac{(42-6)}{2}=18,\\] \\[c_{24}=\\frac{(24+4)}{2}=14, c_{23}=\\frac{(24-4)}{2}=10, c_{22}=\\frac{(18+14)}{2}=16, c_{21}=\\frac{(18-14)}{2}=2,\\] \\[c_{38}=\\frac{(14-2)}{2}=6, c_{37}=\\frac{(14+2)}{2}=8, c_{36}=\\frac{(10+6)}{2}=8, c_{35}=\\frac{(10-6)}{2}=2, \\] \\[c_{34}=\\frac{(16+2)}{2}=9, c_{33}=\\frac{(16-2)}{2}=7, c_{32}=\\frac{(2+0)}{2}=1, c_{31}=\\frac{(2-0)}{2}=1.\\] 이다. 여기서 알 수 있는 사실 중 하나는 가장 섬세한 부드러움 계수는 데이터, 즉 자료라는 것이다. 그런데 데이터를 부드러움 계수로 다루는 것이 과역 적절한가? 라는 의문이 들 수 있다. 예를 들어, 자료에 잡음이 너무 많은 경우 부드러움 계수 또한 오류가 많이 생길 것이다. 통계학에서는 이를 보완하기 위해 \\(\\mathbf{y}\\) 또는 \\(d\\)에 적절한 추정을 한 \\(\\hat{y}\\) 또는 \\(\\hat{d}\\) 등을 고려하기도 한다. (통계학자들은 데이터를 언제나 잡음이 끼어있는 신호라고 생각하고 있음을 명심해야 한다. 이 점이 통계학자와 다른 분야의 학자들이 자료를 보는 관점의 가장 큰 차이점 중 하나이다.) 2.3 희소성(sparsity) Figure 2.3: Example of sparse function. 위 그림은 희소성을 갖는 함수의 전형적인 예 중 하나이다. 이 자료는 왼쪽은 항상 1, 오른쪽은 항상 2의 값을 갖고 있는 부드러운(smooth) 함수이며 변화가 없다. 그러나 가운데 지점에서는 함숫값이 1에서 2로 바뀌면서 급격한 점프가 일어난다. 이 지점은 다른 지역과는 달리 굉장히 다른 정보를 갖고 있는 것이다. 기존 회귀분석에서는 근본적인 함수(underlying function)들의 동질성(homogeneous) 가정을 바탕으로 분석한다. 이 말은 변동이 항상 일정하다는 뜻으로, 함수가 어떤 지역에서 두 번 미분 가능하면 다른 지역에서도 똑같이 두 번 미분 가능해야 한다는 것이다. 그러나 위 그림의 함수처럼 어떤 지역에서는 한 번만 미분 가능하거나 아예 미분 가능하지 않을 수도 있다. 이런 함수들을 다룰 때에는 다중척도 방법으로 접근하는 것이 필요하다. (Donoho and Johnstone 1994)의 논문 이전까지 통계학자들의 관심사는 부드러운 함수의 평균 추정에 집중되어 있었다. Donoho는 논문에서 몇 가지 혁신적인 개념들을 제시했는데, 임계화(thresholding), 희소성(sparsity) 등이 그것이다. 그의 아이디어는 당시에는 이해하기 힘든 것들이었다. 그러나 이 논문은 후대에 들에 고차원 자료 분석의 밑거름이 되게 해 주었고, least absolute sharinkage and selection operator (LASSO)와 거의 같은 개념을 먼저 제시하였다. Donoho의 제자인 Fan은 후에 스승의 아이디어를 알기 쉽게 해석하여 smoothly clipped absolute deviation (SCAD)라는 것을 제안하기도 하였다. 우리가 지금까지 일반적으로 배운 회귀분석 모형은 다음과 같이 나타낼 수 있다. \\[y=f+\\epsilon, \\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})\\] 다시 말하면, 우리가 지금까지 다뤘던 모든 자료에는 오차(\\(\\epsilon\\))와 \\(f\\)가 공존하는 형태의 모형이다. 이러한 모형에서는, 평균이 매우 중요하며 큰 의미를 갖게 된다. 그러나 성긴 모형에서는 상황이 조금 달라진다. 어떤 \\(y\\)는 \\(f\\)만 갖기도 하고, 또는 \\(\\epsilon\\)만 갖기도 한다. \\(\\mathbf{y}=(y_{1},y_{2},y_{3},y_{4})\\)라는 자료가 있을 때 이들 중 \\(y_{3}\\)만 \\(f\\)의 정보가 들어있는 진짜 신호이고 나머지 \\(y_{1},y_{2},y_{4}\\)는 잡음만 있을 수도 있는 것이다. 이런 자료에서 가장 좋은 추정량은 평균이 아니라 \\(y_{3}\\)이다. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,1,1,2,2,2,2)\\)를 생각해보자. 이 자료를 다중척도 변환해 보면 \\[(1,1,1,1,2,2,2,2) \\rightarrow (1,2,4,0,0,0,0,0)\\] 과 같이 0이 많은 벡터로 변환될 것이다. 위 예제와 같이 0이 많이 있는 자료들을 희소성(sparsity)이 있는 자료라 하며, 다중척도 변환은 희소성이 있는 자료를 다룰 때 많은 도움이 될 수 있다. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)을 다중척도 변환한 결과는 \\((42,6,14,4,0,2,6,-2)\\)였다. 이 벡터를 \\(\\mathbf{d}\\)라 하자. 그러면 \\[\\| \\mathbf{y} \\|^{2}=\\sum_{i=1}^{8}y_{i}^{2}=219, \\| \\mathbf{d} \\|^{2}=\\sum_{i=1}^{8}d_{i}^{2}=2056\\] 이다. 그러나 때때로 우리는 \\(\\| \\mathbf{y} \\|^{2}=\\| \\mathbf{d} \\|^{2}\\)가 되도록 만들고 싶어한다. 이를 해결해 줄 수 있는 것이 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform)이다. 2.4 R 예제(R-multiscale) 웨이블릿과 관련된 R 예제를 담고 있는 책은 (Nason 2010)이 있다. 이 책의 저자는 wavethresh란 R 패키지를 만들기도 했다. 또 다른 R 패키지로 waveslim이라는 것도 있다. 여기서는 (Nason 2010)의 예제를 일부 다뤄보기로 한다. wavethresh 라이브러리를 실행시킨 상황에서, 다음 벡터의 웨이블릿 변환을 실행해본다. y &lt;- c(1,1,7,9,2,8,8,6) ## wd: wavelet transform ywd &lt;- wd(y, filter.number=1, family=&quot;DaubExPhase&quot;) names(ywd) &gt; [1] &quot;C&quot; &quot;D&quot; &quot;nlevels&quot; &quot;fl.dbase&quot; &quot;filter&quot; &quot;type&quot; &gt; [7] &quot;bc&quot; &quot;date&quot; ## what filter produced a particular wavelet decomposition object ywd$filter &gt; $H &gt; [1] 0.7071068 0.7071068 &gt; &gt; $G &gt; NULL &gt; &gt; $name &gt; [1] &quot;Haar wavelet&quot; &gt; &gt; $family &gt; [1] &quot;DaubExPhase&quot; &gt; &gt; $filter.number &gt; [1] 1 ## level 2 detail coefficients accessD(ywd, level=2) &gt; [1] 0.000000 -1.414214 -4.242641 1.414214 ## plot wavelet decomposition coefficients plot(ywd) Figure 2.4: Wavelet decomposition coefficients. &gt; [1] 7 7 7 References "],
["wavelettransform.html", "Chapter 3 웨이블릿 변환 3.1 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform) 3.2 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념 3.3 섬세한 척도 근사(fine-scale approximation) 3.4 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale) 3.5 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름) 3.6 웨이블릿의 종류들(types of wavelets)", " Chapter 3 웨이블릿 변환 웨이블릿은 ’Wave’와 프랑스어 ’let’의 합성어로, ’let’은 ’small’이라는 뜻을 가지고 있다. 즉 웨이블릿은 ’small wave’라는 뜻으로, 컴팩트 받침(compactly supported)인 함수들을 일컷는 말이다 . 사인(Sine), 코사인(cosine) 기저(basis)는 \\((-\\infty, \\infty)\\)에서 정의되는 매우 큰 파동이므로 웨이블릿에 해당하지 않는다. 웨이블릿 변환(wavelet transform)이란 웨이블릿 기저함수를 이용해 데이터를 변환하는 것을 말한다. 여기서 웨이블릿 기저함수라는 건 적분하면 0이 되고, 진동하면서 진폭이 0으로 수렴하는 함수를 말한다. 3.1 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform) 가장 단순한 웨이블릿 변환으로 Haar 웨이블릿 변환(Haar wavelet transform)이 있다. 앞서 상세와 성김은 다음과 같이 구할 수 있었음을 상기하자. \\[d_{k}=y_{2k}-y_{2k-1}, c_{k}=y_{2k}+y_{2k-1}.\\] 에너지를 보존하기 위해 다음과 같이 \\(\\alpha\\)라는 상수를 고려하자. \\[d_{k}=\\alpha(y_{2k}-y_{2k-1}), c_{k}=\\alpha(y_{2k}+y_{2k-1}).\\] 그러면 \\begin{eqnarray*} d_{k}^{2}+c_{k}^{2}&amp;=&amp;\\alpha^{2}(y_{2k}^{2}-2y_{2k}y_{2k-1}+y_{2k-1}^{2}+\\alpha^{2}(y_{2k}^{2}+2y_{2k}y_{2k-1}+y_{2k-1}^{2})\\\\ &amp;=&amp;2\\alpha^{2}(y_{2k}^{2}+y_{2k-1}^{2}) \\end{eqnarray*} 즉 \\(2\\alpha^{2}=1 \\Rightarrow \\alpha=\\frac{1}{\\sqrt{2}}\\)이면 \\(y\\)와 \\(d\\)의 에너지가 보존(conserved)된다. 이렇게 \\[d_{k}=\\frac{1}{\\sqrt{2}}(y_{2k}-y_{2k-1}), c_{k}=\\frac{1}{\\sqrt{2}}(y_{2k}+y_{2k-1}).\\] 하는 것을 표준화(normalization)라고 말하기도 한다. 정리하면 Haar 웨이블릿 변환(Haar wavelet transform)의 이산 웨이블릿 계수(discrete wavelet coefficient) \\(d_{k}\\)는 \\[d_{k}=g_{0}y_{2k}+g_{1}y_{2k-1}=\\sum_{l=-\\infty}^{\\infty}g_{l}y_{2k-l}\\] 이며 여기서 \\[ g_{l} = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=0$} \\\\ -\\frac{1}{\\sqrt{2}} &amp; \\text{if $l=1$}\\\\ 0 &amp; \\text{o.w.}\\\\ \\end{cases} \\] 여기서 \\(g_{l}\\)을 고역 필터(high-pass filter)라고 부른다. 마찬가지로 성김에 대해서도 \\[ c_{k}= \\sum_{l=-\\infty}^{\\infty}h_{l}y_{2k-l}, h_{l} = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=0$}\\\\ \\frac{1}{\\sqrt{2}} &amp; \\text{if $l=1$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] 로 나타낼 수 있고 \\(h_{l}\\)을 저역 필터(low-pass filter)라 부른다. 세부와 성김은 앞서 언급한 피라미드 알고리즘으로 구할 수도 있지만 여기서는 \\(\\mathbf{d}=\\mathbf{Wy}\\)처럼 통계학자들에게 익숙한 행렬 꼴로 바꾸어 표현한다. 행렬을 이용해 웨이블릿 계수를 계산해보자. 다음과 같은 자료 \\(\\mathbf{y}=(1,1,7,9,2,8,8,6)\\)에 행렬 \\(W\\)을 다음과 같이 정의하면 \\[ W = \\begin{bmatrix} \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4}\\\\ \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}} &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{\\sqrt{2}} &amp; -\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{2} &amp; \\frac{1}{2} &amp; -\\frac{1}{2} &amp; -\\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2} &amp; \\frac{1}{2} &amp; -\\frac{1}{2} &amp; -\\frac{1}{2}\\\\ \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; \\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4} &amp; -\\frac{\\sqrt{2}}{4}\\\\ \\end{bmatrix} \\] \\(\\mathbf{d}=(\\frac{21\\sqrt{2}}{2},0,-\\sqrt{2},-3\\sqrt{2},\\sqrt{2},-7,-2,\\frac{3\\sqrt{2}}{2})\\)를 얻을 수 있다. (이 예제에서는 (Nason 2010)의 정의를 따라갔다.) 위 예제의 \\(W\\)처럼 Haar 웨이블릿의 \\(W\\)는 정규직교(orthonormal)라는 성질을 갖는데, 정규직교의 의미는 \\(W^{T}W=\\mathbf{I}\\)이다. 그러나 모든 웨이블릿의 \\(W\\)가 정규직교인 것은 아니다. 그리고 \\[\\| \\mathbf{d} \\|^{2}=\\mathbf{d}^{T}\\mathbf{d}=(W\\mathbf{y})^{T}(W\\mathbf{y})=\\mathbf{y}^{T}W^{T}W\\mathbf{y}=\\| \\mathbf{y} \\|^{2}\\] 이 식은 Parseval 등식(Parseval’s identity)에 대응된다. 정규직교인 웨이블릿의 \\(W\\)은 다음과 같은 장점을 갖는다. 다음과 같이 원래 자료와 추정량에 대한 공식이 다음과 같이 주어졌을 때, \\[y=f+\\epsilon, \\epsilon \\sim (\\cdot, \\sigma^{2}I) \\rightarrow d=\\theta +e, Wy=d, Wf=\\theta, W\\epsilon=e\\] 정규직교인 \\(W\\)이면 \\[Var(W\\epsilon)=WVar(\\epsilon)W^{T}=\\sigma^{2}I=Var(\\epsilon)\\] 이다. 즉 원래 자료와 추정량의 분산 구조가 같다. 3.2 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념 \\(p(x)\\)라는 함수가 주어졌을 때, 이것의 압축 및 전이된 버전(scaled and translated version)은 다음과 같이 정의된다. \\[ \\| p_{j,k}(x) \\|^{2}=\\int_{\\infty}^{\\infty}p_{j,k}^{2}(x)dx=\\int_{\\infty}^{\\infty}2^{j}p^{2}(2^{j}x-k)dx=\\int_{\\infty}^{\\infty}p^{2}(y)dy=\\| p(y) \\|^{2} \\] \\[p_{j,k}(x)=2^{\\frac{j}{2}}p(2^{j}x-k)\\] 3.3 섬세한 척도 근사(fine-scale approximation) 다음과 같이 Haar 함수(Haar function)를 정의한다. \\[ \\phi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,1]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] Figure 3.1: Plot of Haar function. 이 때 \\(\\phi(x)=\\phi_{0,0}(x)\\)이다. 즉 척도도 바꾸지 않고 어떤 전이(translation)도 없을 때의 \\(\\phi\\)인 것이다. Figure 3.2: Scaling and translation version of Haar function. 가장 성긴 레벨의 (Haar) 척도 웨이블릿(scaling wavelet, father wavelet)은 \\[c_{J,k}=\\int f(x) \\phi_{J,k}(x)dx=\\int f(x) 2^{\\frac{J}{2}}\\phi(2^{J}x-k)dx\\] 이다. 이것은 앞서 말한 \\(p\\)를 \\(\\phi\\)로 바꾸면 되며 또한 데이터와 같음을 알고 있다. 그리고 앞의 정의들을 이용하면 \\[ \\phi_{J,k} = \\begin{cases} 2^{\\frac{J}{2}} &amp; \\text{if $x \\in [2^{-J}k,2^{-J}(k+1)]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] 이다. 여기서 정의하는 간격을 \\(I_{J,k}\\)라 한다. 우리는 다양한 척도에서 \\(f\\)를 근사할 수 있다. 가장 섬세한 척도로는 \\[f_{J}=\\sum_{k=0}^{2^{J}-1}c_{J,k}\\phi_{J,k}(x)\\] 가 있으며, 가장 성긴 척도로 근사하고 싶으면 \\[f_{0}=\\sum_{k=0}^{2^{0}-1}c_{0,k}\\phi_{J,k}(x)\\] 로 \\(f\\)를 근사한다. 3.4 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale) 앞에서 말한대로, 우리는 섬세한 척도의 계수들로부터 좀 더 성긴 척도의 계수들을 구할 수 있다. \\begin{eqnarray*} c_{J-1,k}&amp;=&amp;\\int_{2^{-(J-1)}k}^{2^{-(J-1)}(k+1)}f(x)\\phi_{J-1,k}(x)dx\\\\ &amp;=&amp;\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{(\\frac{J-1}{2})}\\phi(2^{J-1}x-k)dx\\\\ &amp;=&amp;2^{-\\frac{1}{2}}\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J-1}x-k)dx\\\\ &amp;=&amp;2^{-\\frac{1}{2}}[\\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J}x-2k)dx + \\int_{2^{-J}(2k)}^{2^{-J}(2k+2)}f(x)2^{\\frac{J}{2}}\\phi(2^{J}x-2k-1)dx]\\\\ &amp;=&amp;2^{-\\frac{1}{2}}(c_{J,2k}+c_{J,2k+1}).\\\\ \\end{eqnarray*} 즉 \\({J-1}\\)척도 계수는 \\(J\\)척도 계수로부터 구할 수 있다. 여기서 중간에 \\[\\phi(x)=\\phi(2x)+\\phi(2x-1)\\] 이라는 사실을 이용하였는데, 이것은 매우 중요하다. 이것을 척도방정식(two-scale relationship) 또는 팽창방정식(dilation relationship)이라고 한다. Figure 3.3: Relationship between Haar scale function. 3.5 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름) 척도 근사들의 차이에서 도출된 함수들이 작은 파도(small wave) 형태를 띄므로 이것을 웨이블릿이라 부른다. 앞서 근사식 \\(f_{J}=\\sum_{k=0}^{2^{J}-1}c_{J,k}\\phi_{J,k}(x)\\)과 \\(p_{j,k}(x)=2^{\\frac{j}{2}}p(2^{j}x-k)\\)으로부터 \\(J=1\\)일 때에는 \\[f_{1}(x)=c_{10}\\phi_{10}(x)+c_{11}\\phi_{11}(x)=c_{10}2^{\\frac{1}{2}}\\phi(2x)+c_{11}2^{\\frac{1}{2}}\\phi(2x-1) \\] 이다. 여기서 \\(f_{0}(x)=c_{00}\\phi_{00}(x)\\)라 하고 \\(c_{J-1,k}=\\frac{c_{J,2k}+c_{J,2k-1}}{\\sqrt{2}}\\)를 이용하면 \\begin{eqnarray*} f_{1}(x)-f_{0}(x)&amp;=&amp;c_{10}\\phi_{10}(x)+c_{11}\\phi_{11}(x)-c_{00}\\phi_{00}(x)\\\\ &amp;=&amp;c_{10}2^{\\frac{1}{2}}\\phi(2x)+c_{11}2^{\\frac{1}{2}}\\phi(2x-1)-c_{00}(\\phi(2x)+\\phi(2x-1))\\\\ &amp;=&amp;(c_{10}2^{\\frac{1}{2}}-c_{00})\\phi(2x)+(c_{11}2^{\\frac{1}{2}}-c_{00})\\phi(2x-1)\\\\ &amp;=&amp;(\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\phi(2x)-(\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\phi(2x-1)\\\\ &amp;=&amp;(\\frac{c_{10}-c_{11}}{\\sqrt{2}})(\\phi(2x)-\\phi(2x-1))\\\\ &amp;=&amp;d_{00}\\psi(x).\\\\ \\end{eqnarray*} 여기서 \\((\\frac{c_{10}-c_{11}}{\\sqrt{2}})\\)은 웨이블릿 상수에 해당하고, \\(\\psi(x)=\\phi(2x)-\\phi(2x-1)\\)은 Haar 모웨이블릿(Haar mother wavelet)이라고 한다. \\[ \\psi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,\\frac{1}{2})$}\\\\ -1 &amp; \\text{if $x \\in [\\frac{1}{2},1)$}\\\\ 0 &amp; \\textrm{o.w.} \\end{cases} \\] Figure 3.4: Haar mother wavelet function. 우리는 웨이블릿을 가지고 어떤 함수를 분해(decompose)할 수 있다. 앞의 결과는 \\[f_{1}(x)=f_{0}(x)+d_{00}\\psi(x)=c_{00}\\phi(x)+d_{00}\\psi(x)\\] 로 쓸 수 있으며 \\(f_{1}\\)을 좀 더 성긴 척도함수인 \\(f_{0}\\)과 차이(difference)에 해당하는 \\(\\psi(x)\\)로 분해할 수 있음을 보여준다. 일반적으로 \\(f_{j+1}(x)\\)는 다음과 같이 쓸 수 있다. \\begin{eqnarray*} f_{j+1}(x)&amp;=&amp;\\sum_{k=0}^{2^{j}-1}c_{jk}\\phi_{jk}(x)+\\sum_{k=0}^{2^{j}-1}d_{jk}\\psi_{jk}(x)\\\\ &amp;=&amp;f_{j}(x)+g_{j}(x)\\\\ &amp;=&amp;f_{j-1}(x)+g_{j-1}(x)+g_{j}(x)\\\\ &amp;=&amp; \\vdots \\\\ &amp;=&amp;f_{0}(x)+\\sum_{l=0}^{j}g_{l}(x).\\\\ \\end{eqnarray*} 즉 \\(f_{j+1}(x)\\)은 가장 성긴 근사함수인 \\(f_{0}\\)와 각 수준에서의 차이인 \\(g_{l}\\)들의 합으로 표현할 수 있다. 3.6 웨이블릿의 종류들(types of wavelets) 3.6.1 Haar 웨이블릿(Haar wavelet) 다음과 같이 Haar 함수(Haar function)의 정의를 다시 상기하자. \\[ \\phi(x) = \\begin{cases} 1 &amp; \\text{if $x \\in [0,1]$}\\\\ 0 &amp; \\text{o.w.} \\end{cases} \\] \\(x\\)를 물리적 영역(physical domain) 또는 시간 영역(time domain, t)이라 생각하면, 시간에 대해 컴팩트 받침(compactly supported)인 함수이다. 우리의 궁금점은 이 함수과 과연 주파수 영역(frequency domain)에서도 컴팩트 받침인가이다. 주어진 함수의 각진동수(angular frequency)를 이용한 유니터리 푸리에 변환(Fourier transform with unitary and angular frequency)은 다음과 같다. \\[\\hat{f}(\\omega)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}f(x)e^{-i\\omega x}dx\\] 여기서 \\(\\frac{1}{\\sqrt{2\\pi}}\\)는 이 변환을 유니터리 푸리에 변환으로 만들기 위해 곱해지는 상수이다. 유니터리 변환과 푸리에 변환에 대한 보다 자세한 나용은 인터넷을 참조하기 바란다. 앞서 나온 푸리에 변환을 이용해 Haar 함수를 푸리에 변환한 결과는 다음과 같다. \\[\\hat{\\phi}(\\omega)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{i\\omega}{2}}\\text{sinc}(\\frac{\\omega}{2}).\\] 여기서 \\[ \\text{sinc}(\\omega)= \\begin{cases} \\frac{\\sin (\\omega)}{\\omega} &amp; \\text{if $\\omega \\neq 0$}\\\\ 1 &amp;\\text{if $\\omega = 0$} \\end{cases} \\] 이다. \\(\\hat{\\phi}(\\omega)\\)는 \\(| \\omega |^{-1}\\)만큼의 감쇠(decay)를 가지며 꼬리가 굉장히 긴 함수이다. 즉 주파수 영역에서 이 함수는 컴팩트 받침과 거리가 먼 함수가 된다. 시간 영역과 주파수 영역 사이에 불확정성 원리(uncertainty principle)이 있다는 것은 알려진 사실이다. Figure 3.5: Haar function (left) and Haar function in frequency domain after Fourier transform (right). 3.6.2 Shannon 웨이블릿(Shannon wavelet) Haar와 반대로 주파수 영역에서 컴팩트 받침인 웨이블릿을 생각할 수 있다. 이것의 대표적인 예가 Shannon 웨이블릿(Shannon wavelet)이다. 다음과 같은 모웨이블릿(mother wavelet)을 생각하자. 그리고 그것의 푸리에 변환은 \\[\\psi(x)=\\frac{\\sin(2\\pi x)-\\cos(\\pi x)}{\\pi (x - \\frac{1}{2})} \\rightarrow \\hat{\\psi}(\\omega)=-e^{-\\frac{i\\omega}{2}}I_{[-2\\pi,-\\pi]\\cup[\\pi,2\\pi]}(\\omega).\\] 따라서 주파수 영역에서는 컴팩트 받침이다. Shannon의 부웨이블릿(father wavelet)와 대응되는 푸리에 변환은 다음과 같다. \\[\\phi(x)=\\text{sinc}(\\pi x) \\rightarrow \\hat{\\phi}(\\omega)=1.\\] 즉 주파수 영역에서는 컴팩트 받침이다. 한편 시간 영역에서 \\(\\phi(x)\\)의 그림은 다음과 같다고 한다(출처: 위키, 체크 필요) Figure 3.6: Real Shannon wavelet function. 3.6.3 Meyer 웨이블릿(Meyer wavelet) Shannon 웨이블릿과 유사하며, 주파수 영역에서 상자 함수(box function)를 약간 부드럽게 한 형태의 웨이블릿이 Meyer 웨이블릿(Meyer wavelet)이다. Figure 3.7: Scaling function and wavelet function of Meyer wavelet on time domain. Figure 3.8: Meyer scaling function on frequency domain. References "],
["admultiscale.html", "Chapter 4 고급 다중척도 방법론 4.1 2세대 웨이블릿 변환(second-generation wavelet transform) 4.2 리프팅 스킴(lifting scheme) 4.3 2차원 자료의 리프팅 스킴(lifting in two dimensions)", " Chapter 4 고급 다중척도 방법론 이 장에서는 앞 장에서 다룬 다중척도 방법론 중 심화된 웨이블릿 방법론 및 최신 동향에 대해 다룬다. 이 분야의 대표적인 참고문헌에는 (Jansen and Oonincx 2005)가 있다. 4.1 2세대 웨이블릿 변환(second-generation wavelet transform) 푸리에 변환을 가지고 만들어진 웨이블릿을 1세대 웨이블릿(first-generation wavelet)이라고 부른다. 이와 다르게 푸리에 변환을 이용하지 않고 만들어진 웨이블릿을 2세대 웨이블릿 변환(second-generation wavelet)이라고 부른다. 이 2세대 웨이블릿 변환은 다음에 소개될 리프팅 스킴이라는 것에 의해 만들어진다. 4.2 리프팅 스킴(lifting scheme) 리프팅 스킴(lifting scheme)은 확장(enhancement)이라는 개념이다. 지금 존재하는 웨이블릿에 우리가 원하는 성질들을 추가하는 것이다. Haar 웨이블릿을 새로운 관점에서 보도록 하자. \\(s_{j+1}\\)을 \\(j+1\\) 스케일에서의 투입값이라고 하자. 그러면 Haar 변환은 이것들을 \\(j\\) 척도의 평균 \\(s_{j,k}\\)과 차이(detail) \\(d_{j,k}\\)로 바꿔준다. \\begin{equation} s_{j,k}=\\frac{s_{j+1,2k}+s_{j+1,2k+1}}{2} \\end{equation}\\begin{equation} d_{j,k}=s_{j+1,2k+1}+s_{j+1,2k} \\end{equation} 위 식들의 역변환(inverse transform)은 다음과 같다. \\begin{equation} s_{j+1,2k+1}=s_{j,k}+\\frac{d_{j,k}}{2} \\end{equation}\\begin{equation} s_{j+1,2k}=s_{j,k}-\\frac{d_{j,k}}{2}\\label{eq:four} \\end{equation} 식 (\\ref{eq:four})를 이항하여 정리하면 다음과 같은 식을 얻는다. \\begin{equation} s_{j,k}=s_{j+1,2k}+\\frac{d_{j,k}}{2}. \\end{equation} 웨이블릿 변환에서는 \\(s_{j,k}\\)와 \\(d_{j,k}\\)를 동시에 얻지만 리프팅 스킴에서는 \\(d_{j,k}\\)를 얻은 후 순차적으로 \\(s_{j,k}\\)를 얻는다. Figure 4.1: Forward lifting scheme using Haar transform. 정리하면 \\[\\text{차이(difference)=홀(odd)-짝(even)} \\qquad{\\text{듀얼 리프팅(dual lifting)}}\\] \\[\\text{평균(average)=짝(even)+0.5차이} \\qquad{\\text{프라이멀 리프팅(primal lifting)}}\\] 이며 여기서 짝과 홀을 어떻게 정하느냐에 따라 계산값이 달라진다. 리프팅 스킴의 계산 절차를 다음과 같이 세 단계로 요약할 수 있다. 분할(split): 관찰값들을 짝과 홀 두 개의 분리 집합(disjoint set)으로 분할(partition)한다(꼭 짝과 홀로 나누지 않아도 된다). 에측(predict): 홀로 색인(index)된 투입값을 이 값과 짝의 데이터만을 이용해 예측된 값으로 대체한다. (듀얼 리프팅) 갱신(update): \\(s_{j,k}=s_{j+1,2k}+\\frac{d_{j,k}}{2}\\) (프라이멀 리프팅) 일반적인 리프팅 스킴의 프라이멀 리프팅과 듀얼 리프팅의 정변환(forward transform)은 다음과 같다. \\[\\text{차이(difference)=홀(odd)-p짝(even)} \\qquad{\\text{듀얼 리프팅(dual lifting)}}\\] \\[\\text{평균(average)=짝(even)+u차이} \\qquad{\\text{프라이멀 리프팅(primal lifting)}}\\] 이 때 \\(p=1\\), \\(u=0.5\\)인 경우를 특별히 Haar 웨이블릿 변환(Haar wavelet transformation)이라고 부르는 것이다. Figure 4.2: Standard lifting scheme using primal and dual lifting. 리프팅의 역변환(backward transform)은 다음과 같다. \\[\\text{짝(even)=평균(average)-u차이(difference)} \\] \\[\\text{홀(odd)=차이(difference)+p짝} \\] Figure 4.3: Backward lifting scheme. 리프팅 스킴 방법은 데이터가 일정한 간격(equally space)으로 있어야 한다는 가정이 불필요하며 \\(n=2^{J}\\)일 필요도 없다. 그러나 \\(p\\)와 \\(u\\)를 바꿀 경우 직교성(orthogonality)이 안되기 시작하며 2차원 자료인 경우에도 잘 작동하지 않는다. 만약 시공간 자료(spatio-temporal data)에 리프팅을 적용할 수 있다면 군집 분석(clustering analysis)을 할 때 군집의 크기(clustering size)를 고민할 필요가 없다는 장점이 생긴다. 다음과 같은 벡터 \\[\\mathbf{z}_{3}=(1,2,3,4,5,6,7,8), n=8, J=3.\\] 가 있다고 하자. 이 벡터에 리프팅 스킴을 적용하면 다음과 같다. Spilt: \\(\\mathbf{z}_{3}\\)을 \\(\\mathbf{y}=(1,3,5,7)\\) (홀에 해당)와 \\(\\mathbf{x}=(2,4,6,8)\\) (짝에 해당)로 나눈다. Predict: \\(\\mathbf{x}\\)의 주변값의 평균을 이용해 \\(\\hat{\\mathbf{y}}\\)를 만든다. 첫 번째 원소를 예측할 때에는 첫 번째 \\(\\mathbf{x}\\)값만 쓰기로 한다. 그러면 \\(\\hat{\\mathbf{y}}=(2,3,5,7)\\)가 되고 \\(\\mathbf{e}_{2}=\\mathbf{y}-\\hat{\\mathbf{y}}=(-1,0,0,0)\\)가 된다. Update: 평균을 맞춰주는 작업을 진행해 \\(\\mathbf{z}_{2}=\\bar{\\mathbf{x}}=\\mathbf{x}+\\mathbf{e}_{2}/2 = (1.5,4,6,8)\\)를 만든다. 이제 \\(\\mathbf{z}_{2}\\)를 가지고 같은 작업을 반복한다. 그러면 Spilt: \\(\\mathbf{y}=(1.5,6)\\), \\(\\mathbf{x}=(4, 8)\\). Predict: \\(\\hat{\\mathbf{y}}=(4,6)\\), \\(\\mathbf{e}_{1}=(-2.5,0)\\). Update: \\(\\mathbf{z}_{1}=\\bar{\\mathbf{x}}=(2.75,8)\\). 한 번 더 반복한다. Spilt: \\(\\mathbf{y}=(2.75)\\), \\(\\mathbf{x}=8\\) Predict: \\(\\hat{\\mathbf{y}}=(8)\\), \\(\\mathbf{e}_{0}=(-5.25)\\) Update: \\(\\mathbf{z}_{0}=\\bar{\\mathbf{x}}=(5.375)\\). 최종적으로 남는 detail과 global은 다음과 같다. 0이 많아져 리프팅 스킴으로 좋은 결과를 얻었다고 말할 수 있다고 한다. \\[\\mathbf{e}_{2}=(-1,0,0,0), \\mathbf{e}_{1}=(-2.5,0), \\mathbf{e}_{0}=(-5.25), \\mathbf{z}_{0}=(5.375).\\] 다중척도 방법의 특징은 저장된 계수들을 가지고 원래 신호를 복원(reconstruction)할 수 있어야 한다는 것이다. 앞선 예제의 detail과 global 계수들을 가지고 신호복원을 해보자. \\(\\mathbf{x}=\\mathbf{z}_{0}-\\mathbf{e}_{0}/2=8\\), \\(\\mathbf{y}=\\mathbf{e}_{0}+\\hat{\\mathbf{y}}=2.75\\), \\(\\mathbf{z}_{1}=(2.75, 8)\\). \\(\\mathbf{x}=\\mathbf{z}_{1}-\\mathbf{e}_{1}/2=(4,8)\\), \\(\\hat{\\mathbf{y}}=(4,6)\\), \\(\\mathbf{y}=\\mathbf{e}_{1}+\\hat{\\mathbf{y}}=(1.5, 6)\\), \\(\\mathbf{z}_{2}=(1.5, 5, 6, 8)\\). \\(\\mathbf{x}=\\mathbf{z}_{2}-\\mathbf{e}_{2}/2=(2,4,6,8)\\), \\(\\hat{\\mathbf{y}}=(2,3,5,7)\\), \\(\\mathbf{y}=\\mathbf{e}_{2}+\\hat{\\mathbf{y}}=(1,3,5,7)\\). 따라서 \\(\\mathbf{z}_{3}=(1,2,3,4,5,6,7,8)\\) 신호를 성공적으로 복원할 수 있다. 4.3 2차원 자료의 리프팅 스킴(lifting in two dimensions) 이 절의 내용은 (Jang 2012)의 서술을 참고하였다. 2차원 자료에서는 1차원 자료와 다르게 짝과 홀을 정할 수 없다는 문제점이 있다. 불규칙한 격자점에서, 이웃(neighborhood)은 삼각분할(triangulation)을 통해 정의된다. 리프팅 스킴은 모든 해상도 수준(resolution level)에서 다중척도 들로네 삼각분할(multiscale Delaunay triangulation)이라 부르는 꼭지점들의 이웃 구조를 사용한다. 삼각형 격자 위에 있는 자료를 다중척도 표현으로 분해(decomposition)하는 것은 현재 수준 \\(l\\)에서 \\(l+1\\)로 넘어갈 때 빼길 원하는 점 주변의 국소적 재삼각분할(local retriangulation)을 필요로 한다. 이 알고리즘은 수준이 고정되었을 때 이웃은 변하지 않는다라는 가정 하에서 진행된다. Figure 4.4: 2D lifting scheme example. 위 그림에서 빨간색 점들로 검은색 점을 포함하는 보로노이 다이어그램(Voronoi diagram)을 그린다. (a), (b), (c)는 각각 첫 번째, 두 번째 및 세 번째 리프팅 스킴에 대응된다. (d)는 투입된 자료를 네 가지 수준으로 구분한 그림이다. 빨간색 점들은 \\(s_{t,2k+1}\\)에 대응되며 검은색 점들은 \\(s_{t,2k}\\)에 대응된다. 갱신 단계에서, 우리는 검은색 지점의 상세 이미지(detail image) \\(d_{t+1,k}\\)를 얻을 수 있고, 빨간색 지점의 갱신된 근사 이미지(approximation image) \\(s_{t+1,k}\\)를 얻을 수 있다. 산재된 자료(scattered data)의 경우 척도(scale)은 ‘연속(continuous)’ 개념에 대응된다. 다시 말하면, 모든 자료는 각자 고유의 척도를 가지고 있고, 이 척도는 주변과의 거리에 관계되어 결정되는 것이다. References "],
["spatial.html", "Chapter 5 공간통계학 5.1 공간자료의 종류(classes of spatial data)", " Chapter 5 공간통계학 이 장에서는 공간통계학과 관련된 내용을 다룬다. 주된 내용은 2015년 공간통계학 특강 수업 내용이다. 5.1 공간자료의 종류(classes of spatial data) 결과적으로 공간 자료의 종류는 크게 세 가지로 나눌 수 있다. 연속자료(continuous data; 이것을 다루는 분야가 geostatistics) 이산자료(discrete data, lattice data, areal data) 점 패턴 데이터(point pattern data) Elevation data Davis (1972)는 geoR이라는 R 패키지를 만들었다. 이 안에 있는 자료 elevation은 52개 지역의 표면 높이(surface elevation)를 측정한 자료이다. 기본 distance unit은 50 feet이며, 높이의 기본 distance unit은 10 feet이다. 이러한 자료를 plot하기 위해서는 geodata로 바꿔줘야 한다. Rongelape Island data Diggle et al. (1998)이 만든 R geoRglm 패키지의 롱겔라프 환초 자료(Rongelape Island data, rongelap)은 1954년 미국이 수소탄 실험을 한 곳의 방사능을 측정한 자료이다. 이 자료는 공간 이산 자료(spatial discrete data)의 예이다. 이 자료는 다른 자료들과 달리 sampling design이 되어있는 자료이다. 즉 자료를 200m마다 하나씩 일정하게 뽑은 것이다. 추가로 4개의 지역에 대해서는 50m마다 subsampling을 하였다. 200m마다 측정한 자료로는 200m보다 작은 variation을 구할 수 없다. 그래서 microscale variation을 보기 위해 추가로 subsampling을 한 것이다. Scottish lip cancer data 다음 자료는 1975-1980 동안 스코틀랜드 지역 남성의 lip cancer case 숫자를 county별로 센 scotland 자료이다. 이 자료는 SpatialEpi 패키지에 있다. 이 자료는 spatial dependency가 clear해서 많이 쓰인다고 한다. 이 자료는 앞 자료들과 달리 point 자료가 아닌 지역별 자료이다. 이런 자료를 공간 집적 자료(areal aggregation data)라고 한다. 이런 경우의 자료 분석은 first neighbor, second neighbor에 어떤 자료가 있는지 살펴보는 것을 많이 한다. Inventory data of the Zurichberg Forest, Switzerland 이 자료는 spBayes에 있는 취리히 숲 자재 자료(Zurichberg forest inventory data, Zurich.dat)이다. 이 자료의 특징은 숲이 어디 생길지 모르기 때문에 위치 자체가 random이 된다는 것이다. 앞선 자료들의 위치가 고정되어 있었던 것과는 다른 상황이다. 나무의 size를 잴 때에는 사람 가슴 정도 높이의 trunk를 잰다고 한다. 산림학에서의 관심사는 나무와 나무 사이의 상호작용(interaction)이 있는지 보는 것이다. 나무의 종류에 따라 같이 자랄 수도 있고, 또는 한 나무만 살아남을 수도 있는데 이러한 현상에 관심을 갖는 것이다. Location 자체가 random인 자료들은 point process로 모델링한다. 여기서 \\[\\mathbf{x}=(x_{1}, \\cdots , x_{n})\\] 이라고 하면 \\(\\mathbf{x}\\)와 \\(n\\)이 모두 random이다. 또 mark라고 추가정보가 들어올 수도 있다고 한다. \\[M|\\mathbf{x}=(m_{x_{1}}, \\cdots , m_{x_{n}}).\\] "],
["spatialprocess.html", "Chapter 6 공간과정 6.1 공간자료의 정상성(stationary in spatial data) 6.2 순정상성(strictly stationary) 6.3 약정상성(second order stationary, weakly stationary) 6.4 내재정상성(intrinsic stationary) 6.5 정상성들 사이의 관계(relationship between stationarity) 6.6 에르고딕 과정(ergodic process)", " Chapter 6 공간과정 연속자료의 공간과정(spatial process)은 다음과 같이 나타낸다. \\[\\{Z (\\mathbf{s}), \\mathbf{s} \\in \\mathcal{D} \\subset \\mathbb{R}^{d} \\} .\\] 여기서 \\(\\mathbf{s}\\)는 위치(location)이며 \\(d\\)차원 자료이다. \\(d\\)는 2차원일 수도 있고 고도를 고려하면 3차원일 수도 있다. \\(Z(\\mathbf{s})\\)는 \\(\\mathbf{s}\\)에서의 확률변수를 나타낸다. 만약 이산자료라면 \\[\\{Z (\\mathbf{s}), \\mathbf{s} \\in \\mathcal{D} \\subset \\mathbb{Z}^{d} \\} \\] 로 바뀐다. 점 패턴 자료의 경우에는 굳이 표현하자면 \\[\\{ (\\mathbf{s}_{1},z(\\mathbf{s}_{1})), \\cdots , (\\mathbf{s}_{n},z(\\mathbf{s}_{n})) \\} \\] 으로 쓸 수 있으며 이 때 \\(s\\), \\(Z\\), \\(n\\) 모두 무작위라고 한다. 다음은 \\(Z(\\mathbf{s})\\)의 평균과 분산을 생각해보자. \\[\\text{mean function}:\\mu(\\mathbf{s})=E(z(\\mathbf{s}))\\] \\[\\text{covariance function}: C(\\mathbf{s}_{1},\\mathbf{s}_{2})=E(Z(\\mathbf{s}_{1}-\\mu(\\mathbf{s}_{1})))(Z(\\mathbf{s}_{2}-\\mu(\\mathbf{s}_{2}))).\\] 우리의 관심사는 \\(C(\\mathbf{s}_{1},\\mathbf{s}_{2})\\)이다(공간 변화 모델링, spatial variation modeling). 그렇다면 우리는 왜 이러한 공간 변화(sptaial variation)를 고려해야 하는가? 간단한 회귀분석의 예를 들면 \\[ \\begin{equation} \\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}, \\qquad{\\epsilon_{i} \\sim (0, \\sigma^{2}\\Sigma_{0})} \\end{equation} \\] 으로 식을 쓸 수 있다. 그런데 여기서 오차항을 \\(\\epsilon_{i} \\stackrel{\\text{iid}}{\\sim} (0, \\sigma^{2})\\)이라고 놓고 모델링을 하면 일치성(consistency)은 만족하나 효율성(efficiency)이 깨지는 결과를 얻는다. 즉 추론(inference)를 하기 위해서 변동(variability)을 구할 때 이것이 커지므로 신뢰구간(confidence interval)도 커지고 부정확한 검정(test) 결과를 주는 것이다. 6.1 공간자료의 정상성(stationary in spatial data) 그렇다면 우리는 왜 정상성(stationary)을 고려하는가? 그 이유는 정상성이 아니면 점근(asymptotic) 결과가 거의 없어 모델링하기 어렵기 때문이다. 이론적 배경을 얘기해 줄 때 정상성이 필요한 것이다. 여기서는 세 가지 정상성 개념이 등장한다. 6.2 순정상성(strictly stationary) 이것은 \\[(Z(\\mathbf{s}_{1}), \\cdots , Z(\\mathbf{s}_{k})) \\stackrel{\\mathcal{D}}{=} (Z(\\mathbf{s}_{1}+\\mathbf{h}), \\cdots , Z(\\mathbf{s}_{k}+\\mathbf{h}))\\] 으로 \\(\\mathbf{h}\\)만큼 장소가 변해도 분포가 동일하다는 것이다. 또한 \\(\\mathbf{s}_{1}, \\cdots , \\mathbf{s}_{k}, \\mathbf{s}_{1}+\\mathbf{h}, \\cdots , \\mathbf{s}_{k}+\\mathbf{h} \\in \\mathcal{D}\\) 라는 가정이 필요하다. 6.3 약정상성(second order stationary, weakly stationary) 이것은 \\[\\mu(\\mathbf{s})=\\mu \\text{ (mean function이 상수)}\\] \\[C(\\mathbf{s}_{1},\\mathbf{s}_{2})=C_{0}(\\mathbf{s}_{1}-\\mathbf{s}_{2})\\] 두 가지 조건을 만족하는 것이다. 두 번째 조건은 covariance function이 distribution에 depend하는 어떤 함수 \\(C_{0}\\)로 표현된다는 것이다. \\(C \\in \\mathbb{R}^{d}\\times \\mathbb{R}^{d}\\), \\(C_{0} \\in \\mathbb{R}^{d}\\)라는 점에서 두 함수는 다른 함수이다. 이것을 만족하는 예는 가우스 과정(Gaussian process, GP), 가우스 임의장(Gaussian random field, GRF)이 있다. 보통 가 1차원이면 주로 과정(process), 2차원이면 주로 장(field)이라고 부른다. 6.4 내재정상성(intrinsic stationary) 이것은 차이(difference)를 가지고 정의한다. \\[E(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))=0, \\forall \\mathbf{h}\\] \\[\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))=2\\gamma(\\mathbf{h}), \\forall \\mathbf{h}\\] 여기서 \\(2\\gamma(\\mathbf{h})\\)를 변동도(variogram), \\(\\gamma(\\mathbf{h})\\)를 준변동도(semivariogram)라고 한다. 6.5 정상성들 사이의 관계(relationship between stationarity) 일반적으로 순정상성이 가장 강한 조건이며 그 다음이 약정상성, 내재정상성이 가장 약한 조건을 가진다. 그러나 특별한 경우에는 역방향도 성립할 수 있다. 6.5.1 순정상성과 약정상성간의 관계(relationship between strong and weak stationary) \\(C(\\mathbf{h})=\\text{Cov}(Z(\\mathbf{s}+\\mathbf{h}), Z(\\mathbf{h}))\\)일 때, \\(C(\\mathbf{s},\\mathbf{s}) &lt; \\infty\\)인 경우에는 순정상성이면 무조건 약정상성이 된다. 한편 가우스 과정이나 가우스 임의장인 경우는 처음 두 적률(moment)만 알면 모든 정보를 알 수 있어 약정상성이면 순정상성 또한 성립한다고 한다(우리가 가우스 과정이나 가우스 임의장을 많이 쓰는 이유이기도 하다). 6.5.2 약정상성와 내재정상성간의 관계(relationship between weak and intrinsic stationary) \\[2\\gamma(\\mathbf{h})=2(C(\\mathbf{0})-c(\\mathbf{h}))\\] 관계식을 살펴보면, \\(C\\)가 주어지면 \\(\\gamma\\)를 정의할 수 있다. 그렇다는 얘기는 약정상성이면 내재정상성도 된다는 것이다. 그러나 \\(\\gamma(\\mathbf{h})\\)만 알 경우 \\(C(\\mathbf{0})\\)과 \\(C(\\mathbf{h})\\)를 동시에 specify하지 못한다. 일반적으로 \\(\\lim_{\\mathbf{h}\\rightarrow \\infty}\\gamma(\\mathbf{h})=C(\\mathbf{0})\\)로 정의하여 \\(C(\\mathbf{0})\\)과 \\(C(\\mathbf{h})\\)를 정의한다. 이 말인즉 거리가 멀면 \\(C(\\mathbf{h})\\)는 \\(\\mathbf{0}\\)이 될 것이라고 생각하는 것이다. 6.5.3 내재정상성이나 약정상성이 안 되는 예(counterexample of intrinsic stationary but not weak stationary) 내재정상성이나 약정상성이 안 되는 경우의 예로는 브라운 운동(Brownian motion)이 있다. 브라운 운동 \\(B(t)\\)는 어떤 과정(process)이며 \\(B(t)=0\\) when \\(t=0\\), \\(B(t)\\)는 almost surely continuous \\(B(t)\\) has independent increments with \\(B(t)-B(s) \\sim \\mathcal{N}(0,\\sigma^{2}(t-s)), t&gt;s\\) 를 만족시킨다고 한다. 이 때 \\(\\sigma^{2}\\)을 확산계수(diffusion coefficient)라고 한다. 편의상 1차원에서 생각하자. 그러면 \\(E(B(t+h)-B(t))=0\\), \\(\\text{Var}(B(t+h)-B(t))=2|h|\\)이므로 내재정상성이다. 그러나 \\(h &lt; 0\\)인 경우에 공분산을 계산하면 \\[ \\begin{eqnarray} \\text{Cov}(B(t+h),B(t)) &amp;=&amp; E(B(t+h)B(t))\\nonumber\\\\ &amp;=&amp;E[B(t+h)\\{B(t)-B(t+h)+B(t+h)\\}]\\nonumber\\\\ &amp;=&amp;E[B^{2}(t+h)]+E[B(t+h)]E[B(t)-B(t+h)]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}(t+h)+0=\\sigma^{2}(t+h). \\end{eqnarray} \\] 가 된다. 같은 방법으로 \\(h &gt; 0\\)인 경우도 증명할 수 있으며 결국 \\(Cov(B(t+h),B(t))=\\min (t+h,t)\\)가 된다. 이는 \\(h\\)의 함수가 아니고 차이로 표현 못한다는 뜻이므로 약정상성이 아니다. 6.6 에르고딕 과정(ergodic process) \\[\\lim_{\\|\\mathbf{h}\\| \\rightarrow \\infty}C(\\mathbf{h}) \\rightarrow \\mathbf{0}\\] 인 과정(process)을 에르고딕 과정(ergodic process)이라고 한다. 이 성질은 충분히 긴 (정상상태)과정에서 앙상블평균(통계적평균)과 시간평균이 같다는 것이다. \\[\\bar{Z}_{n}:\\frac{1}{n}\\sum_{i=1}^{n}Z(t_{i}) \\rightarrow \\mu .\\] “앙상블 평균은 시간을 고정시켜 놓고 표본 함수를 무한 개로 하여 계산한 것이고 시간평균은 표본 함수를 고정시켜 놓고 시간을 무한대로 하여 계산한 것이다.” "],
["covfct.html", "Chapter 7 공분산함수 7.1 스펙트럴 표현 정리(spectral representation theorem) 7.2 콜모고로프 존재 정리(Kolmogorov’s existence theorem) 7.3 공분산함수의 성질(properties of covariance functions) 7.4 등방성(isotropy) 7.5 동질성(homogeneous) 7.6 이등방성(anisotropy) 7.7 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process 7.8 Bartlett의 정리(Bartlett’s theorem) 7.9 Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver)", " Chapter 7 공분산함수 공분산함수(covariance function) \\(C(\\mathbf{h})\\)는 \\(C(\\mathbf{h})=\\text{Cov}(Z(\\mathbf{s}+\\mathbf{h}), (Z(\\mathbf{s})\\), \\(C(\\cdot); \\mathbb{R}^{d} \\rightarrow \\mathbb{R}\\)이다. 우리가 공분산함수를 고려할 때 생각해 볼 점은 공분산함수가 되기 위한 조건은 무엇이냐는 것이다. 공분산함수가 되려면 그 함수가 양정치함수(positive definite function)이어야 한다. 이것은 대응되는 행렬이 비음정치(non-negative definite) 또는 양반정치(positive semi-definite)이어야 한다는 것이다. 함수가 순양정치함수(strictly positive definite function)이어야 한다는 것은 대응되는 행렬이 양정치(positive definite)이어야 한다는 것이다. 그러나 저 둘을 보이는 것은 모든 유한한 표본에 대해 항상 성립해야 하는 것을 보여야하기 때문에 어렵다. 이를 좀 더 쉽게 보일 수 있는 방법으로 Bochner (1933, 1955)의 정리를 이용하는 것이다. 7.1 스펙트럴 표현 정리(spectral representation theorem) 스펙트럴 표현 정리(spectral representation theorem)는 다른 말로 Bochner의 정리(Bochner’s theorem)이라고 부른다. 실수값을 갖는 연속 함수 \\(C\\)가 양정치함수라는 것은 그 함수가 symmetric nonnegative measure \\(F\\)로부터 \\(\\mathcal{R}^{d}\\)로 가는 Fourier transform인 경우이다. 즉 \\[ \\begin{eqnarray} C(\\mathbf{h})&amp;=&amp;\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x})\\nonumber\\\\ &amp;=&amp;\\int_{\\mathbb{R}^{d}}\\cos(\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x}) \\text{ (spectral representation)} \\end{eqnarray} \\] 인 경우이다. 여기서 \\(F\\)를 스펙트럼 측도(spectral measure)라고 한다. 만약 \\(F\\)가 좋은 성질을 갖고 있어서 (르베그) 밀도함수((Lebesgue) density function) \\(f\\)가 있으면, 즉 \\(F(\\mathbf{x})=f(\\mathbf{x})d\\mathbf{x}\\)이면 \\(f(\\mathbf{x})\\)를 스펙트럼 밀도(spectral density)라고 하며 이것의 스펙트럼 표현(spectral representation)은 \\[ C(\\mathbf{h})=\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x}=\\int_{\\mathbb{R}^{d}}\\cos(\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x} \\] 로 쓸 수 있다. 이 정리를 쓰면 상대적으로 쉽게 어떤 함수가 양정치함수임을 보일 수 있다. 즉 \\[ C(\\mathbf{h}) =\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})dF(\\mathbf{x}) =\\int_{\\mathbb{R}^{d}}\\exp(i\\mathbf{h}^{T}\\mathbf{x})f(\\mathbf{x})d\\mathbf{x} \\] 임을 보이고 \\(f(\\mathbf{x})\\)가 양(positive)임을 보이기만 하면 된다. 7.2 콜모고로프 존재 정리(Kolmogorov’s existence theorem) 정상성이 아닌 경우에는 콜모고로프 존재 정리(Kolmogorov’s existence theorem)으로 해결할 수 있다고 한다. 7.3 공분산함수의 성질(properties of covariance functions) 여기서부터는 공분산함수(covariance function) \\(C(\\mathbf{h})\\)의 성질에 대해 다루겠다. \\(C(\\mathbf{0}) \\geq 0\\) (분산이 음이 아님) \\(C(\\mathbf{-h})=C(\\mathbf{h})\\) \\(|C(\\mathbf{h})| \\leq C(\\mathbf{0})\\) (코쉬-슈바르츠 부등식) \\(C_{1}\\), \\(C_{2}\\)가 양정치함수이면 \\(a_{1}C_{1}+a_{2}C_{2}\\) 또한 양정치함수이다. (\\(a_{1}\\), \\(a_{2} \\geq 0\\)) 추가적으로 \\(C_{1}C_{2}\\) 또한 양정치함수이다. \\(C_{1}, C_{2}, \\ldots\\)가 양정치함수의 수열이고 이것의 극한 \\(\\lim_{n \\rightarrow \\infty}C_{n}\\)이 존재하면 이것 또한 양정치함수이다. 7.4 등방성(isotropy) \\(C(\\mathbf{h})=C_{0}(\\|\\mathbf{h}\\|)\\)인(or \\(\\gamma(\\mathbf{h})=\\gamma_{0}(\\|\\mathbf{h}\\|)\\)) \\(C_{0}\\)가 존재할 경우 이 공간과정이 \\(Z(\\mathbf{s})\\)를 등방성(isotropy)을 갖는다라고 한다. 이것은 \\(d\\)차원에서 정의한 \\(\\mathbf{h}\\) 1차원인 거리에만 의존하는 공분산함수로 표현 가능하다는 것이다. 7.5 동질성(homogeneous) (잘 쓰이지는 않지만) \\(Z(\\mathbf{s})\\)가 내재적(intrinsic)이고 등방성(isotrophic)인 경우 \\(Z(\\mathbf{s})\\)를 동질성(homogeneous)을 갖는다라고 한다. 7.6 이등방성(anisotropy) 이등방성(anisotropy)란 말 그대로 등방성이 아닌 경우를 얘기하지만 이런 경우가 너무 많아 범위를 나누어 생각한다. 7.6.1 기하학적 이등방성(Geometric anisotropy) 이것은 \\[\\gamma(\\mathbf{h})=\\gamma_{0}(\\|A\\mathbf{h}\\|)\\] 즉 어떤 선형변환행렬(linear transfomation matrix) \\(A\\)를 통해 변환한 후 등방성인 경우를 얘기한다. 보통 \\(A\\)는 대칭인 양정치행렬(symmetric positive definite matrix)을 다루며 이 때 \\(\\|A\\mathbf{h}\\|\\)는 타원형 등고선(ellipse contour)이 된다. 7.6.2 띠모양 이등방성(zonal anisotropy) 이것은 기하학적 이등방성의 일반화된 버전이라고 생각하면 된다. \\[\\gamma(\\mathbf{h})=\\sum_{i=1}^{K}\\gamma_{0}(\\|A_{i}\\mathbf{h}\\|)\\] 로 \\(A\\) 대신 \\(A_{i}\\)들로 표현할 수 있는 경우를 얘기한다. 즉 \\(\\gamma(\\mathbf{h})\\)에 대응되는 공간과정을 \\(Z(\\mathbf{s})\\)라 할 때 각 변동도(variogram)에 해당하는 독립인 공간과정(independent spatial process)의 합으로 즉 \\[Z(\\mathbf{s})=Z_{1}(\\mathbf{s})+ \\cdots Z_{k}(\\mathbf{s})\\] 로 표현할 수 있다는 것이다. 여기서 각각은 변동도인데 이들의 합도 변동도인가?라는 궁금증을 가질 수 있다. 이것은 맞다고 한다. 앞서 ’공분산함수가 되려면 이 함수가 양정치함수이어야 한다’라는 것에 대해 배운 적이 있다. 변동도에 대해서도 똑같이 생각해 볼 수 있을 것이다. 유효한 변동도(valid variogram)이란 무엇인가? 이를 확인해 보기 위해 다음 정의를 살펴보자. \\(\\gamma(\\mathbf{h})=C(\\mathbf{0})-C(\\mathbf{h})\\)라고 하자. 이 때 \\(C(\\mathbf{h})\\)는 양정치함수이나 \\(C(\\mathbf{0})\\)의 존재로 \\(\\gamma(\\mathbf{h})\\)가 양정치함수라고 말할 수는 없다. 대신 \\(\\sum a_{i}=0\\)을 만족하는(\\(C(\\mathbf{0})\\) 때문에 붙는 조건이다) 임의의 \\(a_{1}, \\cdots a_{n}\\)에 대해 \\[\\sum_{i}\\sum_{j}a_{i}a_{j}\\gamma(\\mathbf{s}_{i},\\mathbf{s}_{j}) \\leq 0\\] 인 경우 \\(\\gamma(\\mathbf{h})\\)를 조건부 음정치(conditionally negative definite)라고 한다. 우리는 종속구조(dependent structure)를 모델링하기 위해 공분산 준변동도(covariance semivariogram)를 생각하고 있다. 그런데 공간 데이터를 가지고 있으면 이를 어떤 표면(surface)으로 표현 가능하고 이 과정이 부드러운(smooth)지, 즉 과정이 연속인지 또는 미분가능한지 체크해 보고 싶을 수 있다. 이를 공분산을 가지고 체크해 볼 수 있다고 한다. 즉 공분산은 종속구조를 측정하는 것 이외의 또 다른 기능을 갖고 있는 것이다. 7.7 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process 우선 연속성(continuity)과 미분가능성(differentiability)의 정의를 해야 한다. 여러 버전이 있으나 여기서는 두 가지만 소개하기로 한다. 7.7.1 경로연속(path-continuity) 또는 경로 미분가능성(path-differentiability) 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 경로연속(path-continuity) (또는 K번 미분가능(K-times differentiability))하다는 것은 그것의 실현(realization)이 연속(K번 미분가능)한 것이다. 말은 쉬우나 보이기는 어렵다. \\(\\{ Z(\\mathbf{s}) , S \\in \\mathcal{D}\\}\\)를 확률변수의 모임(collection)이라고 할 때 \\((\\omega \\in \\Omega, \\mathcal{F}, P)\\)에서 \\(\\omega\\) 하나당 \\(\\{ Z(\\mathbf{s})\\}\\)가 나오고 이것을 실현한 것이다. 7.7.2 평균제곱연속(mean-square continuity) 또는 평균제곱 미분가능성(mean-square differentiability) 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 \\(\\mathbf{s}\\)에서 평균제곱연속(mean-square continuity)이라는 것(다른 말로 \\(L_{2}\\)-수렴)은 \\[E[(Z(\\mathbf{s}+\\mathbf{h}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 인 경우이다. 같은 방법으로 어떤 확률과정 \\(Z(\\mathbf{s})\\)가 미분계수 \\(Z&#39;(\\mathbf{s})\\)에 대해 평균제곱 미분가능(mean-square differentiable)이라는 것은 \\[E[(\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}-Z&#39;(\\mathbf{s}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 인 경우이다. 사실 \\(\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}\\)의 극한으로 \\(Z&#39;(\\mathbf{s})\\)를 정의하는 것이다. 고차원의 경우도 반복적으로 정의가 가능하다. 이것의 생김새로 보아 변동도와 관련이 있을 것이다라는 생각도 해 볼 수 있을 것이다. 그렇다면 이 둘 사이에는 어떤 관계가 있을까? 안타깝께도 내재하는 관계는 없다. 즉 경로연속이나 평균제곱연속은 아닌 경우도 있고 그 반대도 있다. 한편 공분산과 평균제곱연속(또는 미분가능성)은 어떤 연관성이 있는 걸까? \\[E[(Z(\\mathbf{s}+\\mathbf{h}))^{2}] \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 으로부터 \\[2\\gamma(\\mathbf{h})=2(C(\\mathbf{0})-C(\\mathbf{h})) \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} 0\\] 즉 \\(C\\)라는 함수가 \\(\\mathbf{0}\\)에서 연속이라는 것을 알 수 있다. (약)정상성을 가정할 경우 (\\(\\mathbf{s} \\in \\mathbb{R}^{d}\\), \\(\\mathbf{s}\\)는 고정되어있지 않다) \\[ \\mathbf{s} \\text{에서 평균제곱연속} \\leftrightarrow C(\\cdot)\\text{이 } \\mathbf{0}\\text{에서 연속}\\] \\[C(\\cdot)\\text{이 } \\mathbf{0}\\text{에서 연속} \\rightarrow Z(\\mathbf{s})\\text{가 모든 지점에서 평균제곱연속}\\] 위 결과로부터, \\(Z(\\mathbf{s})\\)는 모든 지점에서 평균제곱연속이거나 어떠한 지점에서도 평균제곱연속하지 못함을 알 수 있다. (어떤 특정한 지역에서만 평균제곱연속할 수 없다) 또한 다음과 같은 사실도 알 수 있다. \\[C(\\cdot) \\text{가 원점에서 연속이면} \\rightarrow C(\\cdot) \\text{는 모든 지점에서 연속이다}.\\] 이것의 증명에 대해서는 각자 생각해보기로 한다. 7.8 Bartlett의 정리(Bartlett’s theorem) 이 정리는 다음 두 가지를 일컫는 말이다. 공분산함수 \\(\\rho(\\mathbf{u})\\)를 갖는 정상확률과정이 k-번 평균제곱 미분가능하다는 것은 \\(\\rho(\\mathbf{u})\\)가 원점에서 2k-번 미분가능하다는 것과 동치이다. (k=0일 경우 위에서 말했던 관계와 같다.) k번 미분가능한 평균제곱 미분계수(k-th mean-square derivative) \\(Z^{(k)}(\\mathbf{s})\\)의 공분산함수는 \\((-1)^{k}C^{(2k)}\\)이다. 위 설명 내용은 나중에 공분산 모형으로 체크해 볼 수 있다. 지수 공분산 모형(exponential covariance model)은 미분가능하지 않다. 한편 Matern 계급(class)을 이용할 경우 모수를 조절해서 어느 정도까지 공분산 모형이 미분가능할 지 조절할 수 있다. 한편 비모수적인 방법으로 경험적 변동도(empirical variogram)를 쓸 수 있는데, 이 방법으로는 공분산 모형이 몇 번 미분가능한지 구분할 수 없다. (1-d인 경우) \\(Z_{h}(\\mathbf{s})=\\frac{Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s})}{h}\\)로 정의하고 이것의 공분산함수 \\(C_{h}(\\mathbf{t})\\)를 계산하면 \\[C_{h}(\\mathbf{t})=\\text{Cov}(Z_{h}(\\mathbf{s}+\\mathbf{t}),Z_{h}(\\mathbf{s}))=\\frac{1}{h^{2}}\\{2C(\\mathbf{t})-C(\\mathbf{t}+\\mathbf{h})-C(\\mathbf{t}-\\mathbf{h})\\}\\] 이다. 만약 \\(C(\\cdot)\\)이 두 번 미분가능하다면 \\[C_{h}(\\mathbf{t}) \\stackrel{\\|\\mathbf{h}\\| \\rightarrow 0}{\\rightarrow} -C^{&#39;&#39;}(\\mathbf{t})\\] 이다. 한편 평균제곱 연속성은 프랙탈 분석(fractal analysis) (표면의 부드러움 체크 관련), first index estimation 등과도 관련된다고 한다. 7.9 Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver) 평균제곱연속이나 경로연속이지 않은 경우(path-continuous 조건이 mean-square continuous조건보다 strong하다)는 것을 보이는 (Kent 1989)의 정리를 소개하였다. 여기서는 간단하게 2-d version으로 정리한다. \\(\\rho(\\mathbf{u})\\)를 상관함수(correlation function)라고 하자. 이 함수를 테일러 전개로 \\[\\rho(\\mathbf{u})=\\rho_{m}(\\mathbf{u})+r_{m}(\\mathbf{u}), \\mathbf{u} \\in \\mathbb{R}^{2},\\] 여기서 \\(\\rho_{m}(\\mathbf{u})\\)는 \\(\\mathbf{u}=\\mathbf{0}\\)에서 전개한 테일러 급수의 m차 다항함수, \\(r_{m}(\\mathbf{u})\\)는 나머지(remainder)이다. 이 때, 경로연속이 성립할 충분조건은 \\(\\rho (\\cdot)\\)이 두번 연속 미분가능하고 \\(|r_{2}(\\mathbf{u})|\\)가 다음의 순서로 \\[|r_{2}(\\mathbf{u})|=O(\\frac{|\\mathbf{u}|^{2}}{(\\log |\\mathbf{u}|)^{3+\\gamma}}) \\text{ as } |u| \\rightarrow 0 \\text{ for some } \\gamma &gt;0\\] 0으로 가는 것이다. 정상 가우스 과정일 때 경로 연속에 대한 충분조건은 \\[\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=O(\\frac{1}{(\\log |\\mathbf{u}|)^{1+\\epsilon}})=o(1) \\text{ as } |\\mathbf{u}| \\rightarrow 0 \\text{ for some } \\epsilon &gt; 0\\] 이다. 왼쪽의 \\(\\rho(\\mathbf{0})-\\rho(\\mathbf{u})\\)은 가우스 과정일 경우 나머지의 차수(order)가 아닌 상관(correlation)의 차수로 표현 가능하다는 뜻이다. 분산 1일 시 \\[\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=\\gamma(\\mathbf{u}) \\stackrel{\\|\\mathbf{u}\\| \\rightarrow 0}{\\rightarrow} 0\\] 이며 이 화살표는 평균제곱 연속성의 정의와 같다. (\\(\\rho(\\mathbf{0})-\\rho(\\mathbf{u})=o(1)\\)) 한편 corollary의 차수보다 천천히 0으로 가면 평균제곱 연속은 되지만 경로연속이 되지는 않는다. References "],
["covmodel.html", "Chapter 8 공분산모형 8.1 덩어리 효과(nugget effect) 8.2 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case)) 8.3 유효 범위(effective range) 8.4 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models) 8.5 기타 다른 상황에서의 변동도들(variograms in other situation)", " Chapter 8 공분산모형 이 장에서는 공분산모형에는 어떤 것들이 있는지 살펴볼 것이다. 8.1 덩어리 효과(nugget effect) 덩어리 효과(nugget effect)란 공간자료분석에서 흔히 나타나는 특징 중 하나이다. 일반적으로 공간자료 모델링은 다음과 같이 한다. \\[Y(\\mathbf{s})=\\mu(\\mathbf{s})+Z(\\mathbf{s})+\\epsilon(\\mathbf{s}).\\] 여기서 \\(Y(\\mathbf{s})\\)는 관측과정(observed process), \\(\\mu(\\mathbf{s})=E(Y(\\mathbf{s}))\\)는 결정적 평균 함수(deterministic mean function), \\(Z(\\mathbf{s})\\)는 순수 공간과정(spatial process), 마지막으로 \\(\\epsilon(\\mathbf{s})\\)는 나머지 설명되지 않은 항(remaining unexplained term)이다. 일종의 백색잡음(white noise)이라고 생각해도 좋다. 그리고 \\(Z(\\mathbf{s}) \\bot \\epsilon(\\mathbf{s})\\)이다. \\(\\epsilon(\\mathbf{s})\\)에 대해 공분산을 생각해보면, \\[ \\text{COV}(\\epsilon(\\mathbf{s}),\\epsilon(\\mathbf{s}+\\mathbf{h}))= \\left\\{ \\begin{array}{ll} \\sigma_{\\epsilon}^{2} &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ 0 &amp; \\textrm{o.w.} \\end{array} \\right. \\] 인데 여기서 \\(\\sigma_{\\epsilon}^{2}\\)을 덩어리(nugget)이라고 한다. 그리고 이러한 \\(\\epsilon(\\mathbf{s})\\)가 있는게 덩어리 효과(nugget effect)가 있다고 한다. 그렇다면 왜 이름이 덩어리인지 생각해 볼 필요가 있다. ‘Nugget’은 ’조그만 덩어리’ 라는 이름의 영어로, 금광 채굴 문제에서 유래했다고 한다. 유전 발굴과 같은 경우에는 변동성이 작아 어느 지점에서 유전이 발견되면 그 주변에서도 높은 확률로 유전이 발견되지만, 금광의 경우는 microscale variability가 있어 어느 지점에서 발견되었다 하더라도 주변에는 금광이 없을 수도 있다는 것이다. 또 다른 관점으로도 덩어리 효과를 표현할 수 있다. 극한값에서 생각을 해 보면 \\[C(\\mathbf{0})-C(\\mathbf{h})=\\gamma(\\mathbf{h})=\\frac{1}{2}\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))\\] 로 표현할 수 있는데, 덩어리 효가 존재할 경우 \\[\\lim_{\\mathbf{h}\\rightarrow \\mathbf{0}}\\gamma(\\mathbf{h})=\\gamma_{0} &gt;0\\] 일 수도 있다는 것이다. 이떄 이 \\(\\gamma_{0}\\)가 덩어리 효에 대응되는 것이다. 평균제곱과정(mean square process)인 경우 공분산이 \\(\\mathbf{h}=\\mathbf{0}\\)일 때 연속이어야 한다. 따라서 평균제곱이며 연속인 공간과정 \\(Z(\\mathbf{s})\\)의 경우 이러한 현상이 존재 할 수 없고 \\(\\epsilon(\\mathbf{s})\\)는 \\(\\mathbf{h}=\\mathbf{0}\\)일 때 불연속일 수 있고 \\(Z(\\mathbf{s}) \\bot \\epsilon(\\mathbf{s})\\)이므로 덩어리 효과가 존재할 수 있는 것이다. 그런데 앞서 또 다른 관점으로 설명할 때 \\(\\sigma_{\\epsilon}^{2}\\) 대신 \\(\\gamma_{0}\\)이라는 표기를 썼는데 그 이유는 측정 오류(measurement error)가 존재할 수도 있기 때문이다. 엄밀히 얘기하면 \\[\\sigma_{\\epsilon}^{2}=\\text{measurement error variance } + \\text{ 순수한 microscale error variance}\\] 로 표현할 수 있는데 공간자료의 경우 반복관찰이 없어 집합이 1개이므로 이 둘을 따로 구분해서 측정하는 것이 불가능하다. 여러 번 측정할 수 있으면 둘을 구분해서 추정할 수 있다고 한다. (독립인 자료의 경우) 그렇다면 언제 반복이 있느냐? 시공간 자료(spatio-temporal data)에서 시간(temporal) 정보를 다 빼냈을 경우 반복이 있는 자료라 생각할 수 있고 이 둘을 구분하여 추정할 수 있다고 한다. 8.2 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case)) 그렇다면 일반적으로 생각하는 변동도의 모양은 어떠한가? Figure 8.1: Variogram example. 새로 등장하는 용어인 문턱(sill)과 범위(range) 그리고 덩어리(nugget)의 의미를 꼭 알아두자. 변동도의 모양은 공분산이 얼마나 빨리 줄어드는지, 그 때 형태(shape)는 어떠한지에 따라 달라진다. 정상과정인 경우 \\[\\gamma(\\mathbf{h})=C(\\mathbf{0})-C(\\mathbf{h})\\] 가 성립한다. 그런데 \\(\\mathbf{h}\\)를 키우면 \\(C(\\mathbf{h})\\)가 줄어드므로 종속성(dependency)이 감소할 것이다. 그리고 정상과정인 경우 \\(C(\\mathbf{0})\\neq \\infty\\)이므로 \\(\\mathbf{h} \\rightarrow \\infty\\)일 때 \\(\\gamma(\\mathbf{h})\\)가 수렴할 것이다. (그렇지 않다면 비정상과정이다) 8.3 유효 범위(effective range) 앞서 말한 성질들에 variogram은 \\(\\mathbf{h} \\rightarrow \\infty\\)어떤 값으로 수렴한다. 그런데 어느정도 지난 \\(\\mathbf{h}\\)이후에 수렴값과 맞닿으면 range를 정의할 수 있으나 점근해서 수렴하는 경우는 range가 \\(\\infty\\)가 되는 문제가 발생한다. 이럴 때 대신 쓰는 정의로 유효 범위(effective range)라는 것이 있다. 이 것의 정의는 다음과 같다. \\[\\gamma(\\mathbf{h}) \\text{가 sill의 95% 접근하는 smallest } \\|\\mathbf{h}\\| .\\] 8.4 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models) 여기서는 총 9가지의 변동도 모형을 소개한다. 그러나 실제로 널리 쓰이는 모델은 ‘exponential’과 ’matern’ 두 개이다. Linear \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\| &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] \\(\\lim_{\\|\\mathbf{h}\\| \\rightarrow \\mathbf{0}}\\gamma(\\mathbf{h}) \\rightarrow \\infty\\)이므로 이것은 비정상과정의 변동도라는 것을 알 수 있다. (대신 내재정상과정이다) Spherical \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}[\\frac{3}{2}(\\frac{\\| \\mathbf{h}\\|}{R})-\\frac{1}{2}(\\frac{\\| \\mathbf{h}\\|}{R})^{3}] &amp; \\textrm{if $0 \\leq \\|\\mathbf{h}\\|\\leq R$}\\\\ c_{0}+c_{1} &amp; \\textrm{if $\\|\\mathbf{h}\\|&gt;R$}\\\\ \\end{array} \\right. \\] 이것은 지구통계학(geostatics)에서는 많이 쓰나 모형이 복잡해 분포 가정 후 우도 추정(likelihood estimation)을 할 때 어렵고 잘 안되므로 통계학자들은 잘 안 좋아하는 모형이라고 한다. 참고로 이 변동도는 \\(\\mathbf{h} \\in \\mathbb{R}^{d}, d=1,2,3\\)에서만 유효하다. 이것 이외에 나머지 변동도의 예들은 다 모든 양수 \\(d\\)에서 유효한 변동도를 갖는다고 한다. Exponential \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-\\frac{\\|\\mathbf{h}\\|}{R}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] 이것은 평균제곱연속이나 평균제곱 미분 가능하지는 않다고 한다. 그리고 다른 애들보다 독립적인 편이라고(공분산이 약함) 한다. 특히 뒤에 나오는 가우스 변동도(Gaussian variogram)와 비교했을 때 덜 부드럽다. Gaussian \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-(\\frac{\\|\\mathbf{h}\\|}{R})^{2}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] 가우스분포랑은 관련이 없으며 2차항이 있기 때문에 이러한 이름이 붙은 것이다. 이것은 무한번 평균제곱 미분 가능(infinitely mean-square differentiable)하다. 그렇다는 얘기는 엄청 부드러운 데이터 모델링 시 사용할 수 있다는 것이다. 그러나 이런 경우는 드물므로 많이 보기는 힘들다. Exponential power \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-e^{-(\\frac{\\|\\mathbf{h}\\|}{R})^{p}}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$, $0 &lt; p \\leq 2$}\\\\ \\end{array} \\right. \\] 이것은 exponential과 Gaussian의 중간쯤 되는 변동도로 \\(p\\)를 추정하는 것이 어려워 보통 미리 정하고 사용한다고 한다. Rational quadratic \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\|^{2}/(1+\\frac{\\|R\\|^{2}}{R}) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] Wave \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}(1-\\frac{R}{\\|\\mathbf{h}\\|}\\sin(\\frac{\\|\\mathbf{h}\\|}{R})) &amp; \\textrm{if $\\mathbf{h}\\neq 0$}\\\\ \\end{array} \\right. \\] Power-law \\[ \\gamma(\\mathbf{h})= \\left\\{ \\begin{array}{ll} 0 &amp; \\textrm{if $\\mathbf{h}$=0}\\\\ c_{0}+c_{1}\\| \\mathbf{h}\\|^{\\lambda} &amp; \\textrm{if $\\mathbf{h}\\neq 0$, $0\\leq \\lambda &lt; 2$}\\\\ \\end{array} \\right. \\] 이것은 선형모형(linear model)의 일반화(generalization)이며 \\(\\lambda=0\\)일 때에는 선형모형이다. 이것은 Local whittle model에서 감소율(decay rate) (\\(\\| \\mathbf{h} \\| \\rightarrow \\infty\\), \\(\\| \\mathbf{h} \\| \\rightarrow 0\\) 부근) 결정시 부분적으로 사용한다고 한다. (공분산을 일종의 준모수 모델링하는 것 같다) Matern (변동도가 복잡하므로 공분산 형태로 썼다) \\[C(\\mathbf{h})=\\sigma^{2}\\frac{2^{1-\\alpha}}{\\Gamma(\\alpha)}(\\frac{\\|\\mathbf{h}\\|}{\\phi})^{\\alpha}K_{\\alpha}(\\frac{\\|\\mathbf{h}\\|}{\\phi})\\] 참고로 \\[\\lim_{\\mathbf{h} \\rightarrow 0}C(\\mathbf{h}) = \\sigma^{2}\\] 이다. 이 때 \\(K_{\\alpha}\\)는 차수 \\(\\alpha\\)를 갖는 변형된 이형(second kind) 베셀 함수(Bessel function)라고 한다. 그리고 \\((\\phi&gt;0,\\alpha&gt;0)\\)는 각각 척도모수(scale parameter), 평활모수(smoothness parameter) (shape parameter, random process의 smootheness와 연결)라고 부르는데, 이름으로 그 역할들을 짐작할 수 있을 것이다. Matern 공분산함수(Matern covariance function)는 두 개의 모수가 있으므로 다양한 형태의 변동도 모델링이 가능하다. 이들은 때때로 정의가 바뀌기도 하므로 R-package를 쓸 때 체크가 필요하다. Matern 공분산함수를 갖는 \\(Z(\\mathbf{s})\\)는 \\(\\lceil \\alpha \\lceil -1\\)번 평균제곱 미분가능하다. (\\(0&lt;\\alpha \\leq 1\\)인 경우에는 평균제곱연속) 이 말은 즉 \\(\\alpha\\)를 조절해 공분산 함수의 평활도(smoothness)를 조절할 수 있다는 뜻이다. \\(\\alpha=0.5\\)인 경우에는 지수모형(exponential model), \\(\\alpha \\rightarrow \\infty\\)인 경우에는 가우스모형(Gaussian model)이 된다. (\\(\\alpha\\)가 커질수록 differentiability 또한 올라간다) \\(\\alpha\\)가 half-integer인 경우 explicit form이 있다고 한다. \\(\\phi=1\\)일 때 \\[ \\begin{array}{ll} \\alpha=0.5: &amp; e^{-\\|\\mathbf{h}\\|}\\\\ \\alpha=1.5: &amp; (1+\\|\\mathbf{h}\\|)e^{-\\|\\mathbf{h}\\|}\\\\ \\alpha=2.5: &amp; (1+\\|\\mathbf{h}\\| + \\frac{\\|\\mathbf{h}\\|^{2}}{3})e^{-\\|\\mathbf{h}\\|}\\\\ \\end{array} \\] 8.4.1 변형된 이형 베셀에 대한 보충 설명(addtional explanation for K_alpha) 변형 베셀 함수(modified Bessel function)은 변형 베셀 방정식(modified Bessel equation)의 해이다. 변형 베셀 방정식(modified Bessel equation)은 다음의 \\[x^{2}y&#39;&#39;+xy&#39;-(x^{2}+\\alpha^{2})y=0\\] 미분방정식의 해이다. (original은 \\(x^{2}+\\alpha^{2}\\) 부분이 다르다고 한다) 이 해는 두 개가 존재한다. \\[\\begin{array}{ll} I_{\\alpha}(x): &amp; \\text{exponentially growing}\\\\ K_{\\alpha}(x): &amp; \\text{exponentially decaying}\\\\ \\end{array} \\] 우리는 감소하는 형태의 함수를 필요로 하므로 \\(K_{\\alpha}(x)\\)를 사용하는 것이다. 추가적으로 \\(x &gt;&gt; [\\alpha^{2}-\\frac{1}{4}]\\)이면 \\[K_{\\alpha}(x) \\sim \\sqrt{\\frac{\\pi}{2x}}e^{-x}\\] 가 되고, \\(0&lt;x&lt;&lt;1\\)이면 \\[ K_{\\alpha}(x) \\sim \\left\\{ \\begin{array}{ll} -\\log (\\frac{x}{2}) - c &amp; \\textrm{if $\\alpha$=0}\\\\ (\\frac{\\Gamma(\\alpha)}{2})(\\frac{2}{x})^{\\alpha} &amp; \\textrm{if $\\alpha$&gt;0}\\\\ \\end{array} \\right. \\] \\(\\alpha\\)를 추정하는 것은 쉽지 않다. 공간통계학에서는 보통 우도로 모수 추정을 하는데 이것을 하기 위해서는 공분산행렬의 역행렬 등을 계산해야 한다. 그런데 \\(\\alpha\\)가 클 경우 수치적 특이점(numerical singularity)이 많이 나와 \\(\\alpha\\) 추정이 어렵다. 그래서 보통 작은 숫자의 \\(\\alpha\\)를 고정한 후 많이 사용한다. 8.5 기타 다른 상황에서의 변동도들(variograms in other situation) 지금까지는 일변량(univariate)이고 정상성을 가지며 시간 구조(temporal structure)는 고려 안한 변동도만 살펴보았다. 그러나 그렇지 않은 경우에는 어떻게 할 것인가? 다변량(multivariate)인 경우 한 장소에서의 관측값이 여러 개 일 수도 있다. 즉 한 지점에서 미세먼지 농도만 관측한 것이 아니라 강수량도 같이 관측한 경우도 있을 수 있다. 이런 경우의 변동도는 상호의존성(interdependence)을 고려해야 한다. 같은 장소와 다른 장소 사이의 dependence와 관측값 사이의 dependence 등도 고려해야 한다. 이런 경우의 변동도들을 공변동도(covariogram)이라고 부른다. 시공간자료(spatio-temporal data)인 경우 가장 간단한 모델은 분리가능한(separable) 모델이다. 이 모델은 시간과 공간 공분산이 독립이라는 것이다. \\[C(h,t;\\theta)=C_{s}(h)C_{t}(t).\\] 그러나 이 모델은 어느 위치에 있던지 시간 의존성이 같다는 매우 강한 가정을 갖는 것이다. 그렇지 않은 모형들을 분리 가능하지 않은(non-separable) 모델이라고 부른다. 스케일(scale)이 커지는 경우 전 지구적 수준의 데이터를 다룰 경우 구면좌표계를 사용할 필요가 있다. 구(sphere)나 다양체(manifold) 상에서 정의됨을 고려해야 하며 어떤 거리(distance)를 쓸 것인지에 대해서도 생각해 봐야 한다. "],
["variogramest.html", "Chapter 9 변동도의 추정 9.1 경험변동도(empirical variogram) 9.2 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram)", " Chapter 9 변동도의 추정 이 문서에서는 variogram을 어떻게 추정할 것인지에 대해 다룰 것이다. 크게 두 가지가 있다. Empirical model (nonparametric) Parametric fit 그리고 (Gelfand et al. 2010)의 33쪽부터, (Cressie 1993)의 69쪽부터 참고했다. 9.1 경험변동도(empirical variogram) 이것은 변동도를 비모수 추정하는 것이다. 다시 한 번 변동도의 정의를 살펴보면 \\[2\\gamma(\\mathbf{h})=\\text{Var}(Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))\\] 으로 lag \\(\\mathbf(h)\\)에만 의존하는 함수이다. 그런데 내재정상성(instinsic stationary)에서는 평균이 0이므로 \\[2\\gamma(\\mathbf{h})=E[Z(\\mathbf{s}+\\mathbf{h})-Z(\\mathbf{s}))^{2}]\\] 이 된다. 다음은 추정량을 구하기 위한 방법들이다. 적률 추정(Metohd of moment (MoM) estimation (Matheron, 1962)) \\(E(Z(\\mathbf{s}))=\\mu\\)라는 상수 평균(constant mean) 가정하에 적률추정량(MoM estimator)은 \\[2\\hat{\\gamma}(\\mathbf{h})=\\frac{1}{|N(\\mathbf{h})|}\\sum_{(s_{i},s_{j})\\in N(\\mathbf{h})}(Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j}))^{2}, \\mathbf{h}\\in \\mathbb{R}^{d}\\] 이다. 여기서 \\(N(\\mathbf{h})\\)는 거리가 \\(\\mathbf{h}\\)가 되는 \\((\\mathbf{s}_{i},\\mathbf{s}_{j})\\)들의 집합이다. 즉 \\[N(\\mathbf{h})=\\{ (\\mathbf{s}_{i},\\mathbf{s}_{j}), \\mathbf{s}_{i}-\\mathbf{s}_{j}=\\mathbf{h} \\}\\] 이다. \\(N(\\mathbf{h}) \\neq N(\\mathbf{-h})\\)임에 주의하자. 이 추정량의 문제는 정규 격자(regular grid) 자료에만 잘 적용된다는 점이다. Irregular한 자료에서는 \\(\\mathbf{h}\\)에 대응되는 \\(N(\\mathbf{h})\\)가 공집합(empty set)일 수도 있다. 그런 상황을 해결하기 위해 \\(\\mathbf{h}\\)의 적당한 근방(neighborhood) \\(T(\\mathbf{h})\\)을 생각하여 \\(N(\\mathbf{h})\\)를 정의하기도 한다. \\[N(\\mathbf{h})=\\{ (\\mathbf{s}_{i},\\mathbf{s}_{j}), \\mathbf{s}_{i}-\\mathbf{s}_{j}=T(\\mathbf{h}) \\} .\\] 그렇다면 이 근방의 size는 어떻게 정해야 할 것인가라는 질문이 생길 수도 있다. 이것은 띠너비 선택(bandwidth selection) 문제와 유사하다. Practically하게 (Journel and Huijbregts 2003)는 \\(| \\cup \\{N(\\mathbf{h}): \\mathbf{h} \\in T(\\mathbf{h}) \\} |\\)에 들어가는 distinct pair들이 적어도 30개 이상이 되도록 잡는 것이 좋다고 하였다. 그러나 이 경우도 역시 데이터의 사이즈가 적을 경우 문제가 된다. 또한 \\(\\mathbf{h}\\)의 방향도 고려할 경우 자료가 더 부족해지고, \\(\\mathbf{h}\\)에 따라 pair 갯수 또한 차이가 난다. 그리고 자료로 인해 관측할 수 있는 \\(\\mathbf{h}\\)의 minimum과 maximum length가 존재한다. 역시 practically하게 \\(\\mathbf{h}\\)는 observation location들의 maximum length의 절반 정도를 고르도록 권장하고 있다. 마지막으로 이 추정량의 성질에 대해 알아보자. 우선 unbiased하다(특히 grid 자료인 경우). 그러나 outlier에 로버스트하지는 않다. \\(Z(\\mathbf{s})\\)가 Gaussian distribution이면 \\[(Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j}))^{2} \\sim 2 \\gamma(\\mathbf{h})\\chi_{1}^{2}\\] 이다. 그런데 카이제곱 분포는 매우 skewed된 분포인데 이 분포를 sample mean을 이용해 추정했으므로 finite sample 이용시 variation이 클 수 있다. 로버스트 추정량(robust estimator (Cressie and Howkins, 1980)) 이 문제를 해결하기 위해 Cressie와 Howkins는 robust한 통계량을 제시하였다. \\[2 \\bar{\\gamma}(\\mathbf{h})=\\frac{1}{0.457+\\frac{0.494}{|N(\\mathbf{h})|}}\\{\\frac{1}{|N(\\mathbf{h})|}\\sum_{(\\mathbf{s}_{i},\\mathbf{s}_{j} \\in N(\\mathbf{h}))} |Z(\\mathbf{s}_{i}-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\}^{4}.\\] 앞의 \\(\\frac{1}{0.457+\\frac{0.494}{|N(\\mathbf{h})|}}\\)는 bias correction term이다. 이 추정량의 아이디어는 다음과 같다. 어떤 확률변수 \\(X \\sim \\chi_{1}^{2}\\)때 \\(X^{\\frac{1}{4}}\\)는 거의 symmetric임을 보일 수 있다고 한다. 즉 \\(|Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{2}\\)보다는 \\(|Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\)이 더 symmetric하게 행동할 수 있을 것이다. 따라서 이것을 이용하고자 하는 것이다. \\(\\mathbf{X}_{n}\\)을 \\(X_{n}\\equiv\\frac{1}{|N(\\mathbf{h})|}\\sum_{(\\mathbf{s}_{i},\\mathbf{s}_{j} \\in N(\\mathbf{h}))} |Z(\\mathbf{s}_{i}-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}}\\) 이라고 하자. 다음은 \\(X \\sim \\chi_{1}^{2}\\)시 몇 가지 계산 결과이다. \\[E(X^{\\frac{1}{4}})=0.82216, \\text{Var}(X^{\\frac{1}{4}})=0.12192, E(X_{n})=0.82216 \\equiv \\nu\\] \\[\\text{Var}(X_{n})=\\frac{0.12192}{|N(\\mathbf{h}|)} \\text{(cross-covariance 무시할 경우)} .\\] 그 다음 \\(f(x)=x^{4}\\)에 대해 \\(\\nu\\) 근방에서 테일러 전개를 해보자. 그러면 \\[f(X_{n})\\circeq f(\\nu) +f&#39;(\\nu)(X_{n}-\\nu)+\\frac{1}{2}f&#39;&#39;(\\nu)(X_{n}-\\nu)^{2} .\\] 여기서 \\(X_{n}\\)만 random이다. 기댓값을 취하면 \\[ \\begin{aligned} E(X_{n})^{4}&amp;\\circeq f(\\nu) + f&#39;(\\nu)E(X_{n}-\\nu) +\\frac{1}{2}f&#39;&#39;(\\nu)E(X_{n}-\\nu)^{2}\\\\ &amp;\\circeq 0.457 + 0 +\\frac{0.494}{|N(\\mathbf{h})|} \\text{(second order까지 bias correction)}\\\\ \\end{aligned} \\] 따라서 robust estimator 앞에 bias correction을 위한 숫자항이 붙는 것이다. 또 다른 로버스트 추정량(another robust estimator) 특별한 이름은 없으며 앞 estimator에서 약간 변형한 형태이다. \\[ \\begin{aligned} 2\\tilde{\\gamma}(\\mathbf{h})&amp;=\\frac{1}{0.457}\\text{Median}\\{ |Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{2} , (\\mathbf{s}_{i},\\mathbf{s}_{j})\\in N(\\mathbf{h}) \\}\\\\ &amp;=\\frac{1}{0.457}\\{\\text{Median} \\{ |Z(\\mathbf{s}_{i})-Z(\\mathbf{s}_{j})|^{\\frac{1}{2}} \\}^{4} \\} \\end{aligned} \\] Median을 쓸 경우 제곱근을 한 다음 네제곱을 하거나 그냥 제곱을 하거나 차이가 없다고 한다. 9.2 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram) empirical variogram 자료들을 가지고 왜 또 parametric한 fitting을 하려고 할까? Geostatistics에서는 dependence structure \\(\\boldsymbol{\\sigma}\\)를 이용한 prediction에 관심이 있다. 그런데 prediction을 하려면 \\(\\boldsymbol{\\sigma}^{-1}\\)이 필요하다. 그런데 empirical variogram으로 하면 \\(\\boldsymbol{\\sigma}\\)가 non-negative definite가 아니거나 numerical singularity가 생긴다. 일반적으로 다음과 같은 모수 모델 \\[\\hat{\\boldsymbol{\\gamma}}(h;\\hat{\\boldsymbol{\\theta}})\\] 는 positive definite function을 guarantee한다(물론 numerical singular한 경우도 있을 수는 있다). 이러한 이유로 공간통계학에서는 모수를 이용한 모델링을 선호하는 것이다. 다음과 같이 \\(\\hat{\\gamma}(\\mathbf{h}_{1}), \\cdots , \\hat{\\gamma}(h_{m})\\)이 available하다고 하자(이들을 새로운 자료로 생각해도 좋다). 여기서 \\(m\\)은 고정시킨다. 우리의 목표는 \\(\\boldsymbol{\\gamma}(h;\\boldsymbol{\\theta})\\)가 true model일 때 \\(\\hat{\\boldsymbol{\\gamma}}(h;\\hat{\\boldsymbol{\\theta}})\\)를 만들고자 한다. 추정 방법은 크게 세 가지가 있다. References "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
