<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>통계공부와 관련된 글들</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다.">
  <meta name="generator" content="bookdown 0.0.73 and GitBook 2.6.7">

  <meta property="og:title" content="통계공부와 관련된 글들" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  <meta name="github-repo" content="SeoncheolPark/book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="통계공부와 관련된 글들" />
  
  <meta name="twitter:description" content="통계공부를 하면서 몇 가지 내용들을 gitbook 형식으로 정리하였다." />
  

<meta name="author" content="Seoncheol Park">

<meta name="date" content="2016-07-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="wavelettransform.html">
<link rel="next" href="advwaveletshrinkage.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">통계공부와 관련된 글들</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 일러두기</a></li>
<li class="part"><span><b>Multiscale Methods in Statistics</b></span></li>
<li class="chapter" data-level="2" data-path="multiscale.html"><a href="multiscale.html"><i class="fa fa-check"></i><b>2</b> 다중척도 방법론</a><ul>
<li class="chapter" data-level="2.1" data-path="multiscale.html"><a href="multiscale.html#-multiscale-transform"><i class="fa fa-check"></i><b>2.1</b> 다중척도 변환(multiscale transform)</a></li>
<li class="chapter" data-level="2.2" data-path="multiscale.html"><a href="multiscale.html#inverse"><i class="fa fa-check"></i><b>2.2</b> 역(inverse)</a></li>
<li class="chapter" data-level="2.3" data-path="multiscale.html"><a href="multiscale.html#sparsity"><i class="fa fa-check"></i><b>2.3</b> 희소성(sparsity)</a></li>
<li class="chapter" data-level="2.4" data-path="multiscale.html"><a href="multiscale.html#-filter-in-signal-processing"><i class="fa fa-check"></i><b>2.4</b> 신호처리에서의 필터(filter in signal processing)</a></li>
<li class="chapter" data-level="2.5" data-path="multiscale.html"><a href="multiscale.html#r-r-multiscale"><i class="fa fa-check"></i><b>2.5</b> R 예제(R-multiscale)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wavelettransform.html"><a href="wavelettransform.html"><i class="fa fa-check"></i><b>3</b> 웨이블릿 변환</a><ul>
<li class="chapter" data-level="3.1" data-path="wavelettransform.html"><a href="wavelettransform.html#-haar--discrete-haar-wavelet-transform"><i class="fa fa-check"></i><b>3.1</b> 이산 Haar 웨이블릿 변환(discrete Haar wavelet transform)</a></li>
<li class="chapter" data-level="3.2" data-path="wavelettransform.html"><a href="wavelettransform.html#scaling-coefficient-translation-coefficient-"><i class="fa fa-check"></i><b>3.2</b> 압축계수(scaling coefficient)와 전이계수(translation coefficient) 개념</a></li>
<li class="chapter" data-level="3.3" data-path="wavelettransform.html"><a href="wavelettransform.html#--fine-scale-approximation"><i class="fa fa-check"></i><b>3.3</b> 섬세한 척도 근사(fine-scale approximation)</a></li>
<li class="chapter" data-level="3.4" data-path="wavelettransform.html"><a href="wavelettransform.html#-----computing-coarser-scale-coefficients-from-fine-scale"><i class="fa fa-check"></i><b>3.4</b> 섬세한 척도로부터 성긴 척도 계수의 계산(computing coarser scale coefficients from fine scale)</a></li>
<li class="chapter" data-level="3.5" data-path="wavelettransform.html"><a href="wavelettransform.html#---defference-between-scale-approximations--"><i class="fa fa-check"></i><b>3.5</b> 척도 근사들 사이의 차이(defference between scale approximations)(이를 웨이블릿이라 부름)</a></li>
<li class="chapter" data-level="3.6" data-path="wavelettransform.html"><a href="wavelettransform.html#-types-of-wavelets"><i class="fa fa-check"></i><b>3.6</b> 웨이블릿의 종류들(types of wavelets)</a><ul>
<li class="chapter" data-level="3.6.1" data-path="wavelettransform.html"><a href="wavelettransform.html#haar-haar-wavelet"><i class="fa fa-check"></i><b>3.6.1</b> Haar 웨이블릿(Haar wavelet)</a></li>
<li class="chapter" data-level="3.6.2" data-path="wavelettransform.html"><a href="wavelettransform.html#shannon-shannon-wavelet"><i class="fa fa-check"></i><b>3.6.2</b> Shannon 웨이블릿(Shannon wavelet)</a></li>
<li class="chapter" data-level="3.6.3" data-path="wavelettransform.html"><a href="wavelettransform.html#meyer-meyer-wavelet"><i class="fa fa-check"></i><b>3.6.3</b> Meyer 웨이블릿(Meyer wavelet)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html"><i class="fa fa-check"></i><b>4</b> 웨이블릿 수축</a><ul>
<li class="chapter" data-level="4.1" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#---main-concept-of-wavelet-shrinkage"><i class="fa fa-check"></i><b>4.1</b> 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage)</a></li>
<li class="chapter" data-level="4.2" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#oracle"><i class="fa fa-check"></i><b>4.2</b> 오라클(oracle)</a></li>
<li class="chapter" data-level="4.3" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#-universal-thresholding"><i class="fa fa-check"></i><b>4.3</b> 만능 임계화(universal thresholding)</a></li>
<li class="chapter" data-level="4.4" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#stein---steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>4.4</b> Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE))</a></li>
<li class="chapter" data-level="4.5" data-path="waveletshrinkage.html"><a href="waveletshrinkage.html#r-r-waveletshrinkage"><i class="fa fa-check"></i><b>4.5</b> R 예제(R-waveletshrinkage)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html"><i class="fa fa-check"></i><b>5</b> 웨이블릿 수축의 고등 논제들</a><ul>
<li class="chapter" data-level="5.1" data-path="advwaveletshrinkage.html"><a href="advwaveletshrinkage.html#cross-validation"><i class="fa fa-check"></i><b>5.1</b> 교차타당성(cross-validation)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="admultiscale.html"><a href="admultiscale.html"><i class="fa fa-check"></i><b>6</b> 고급 다중척도 방법론</a><ul>
<li class="chapter" data-level="6.1" data-path="admultiscale.html"><a href="admultiscale.html#--second-generation-wavelet-transform"><i class="fa fa-check"></i><b>6.1</b> 2세대 웨이블릿 변환(second-generation wavelet transform)</a></li>
<li class="chapter" data-level="6.2" data-path="admultiscale.html"><a href="admultiscale.html#-lifting-scheme"><i class="fa fa-check"></i><b>6.2</b> 리프팅 스킴(lifting scheme)</a></li>
<li class="chapter" data-level="6.3" data-path="admultiscale.html"><a href="admultiscale.html#---lifting-in-two-dimensions"><i class="fa fa-check"></i><b>6.3</b> 2차원 자료의 리프팅 스킴(lifting in two dimensions)</a></li>
</ul></li>
<li class="part"><span><b>Spatial Statistics</b></span></li>
<li class="chapter" data-level="7" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>7</b> 공간통계학</a><ul>
<li class="chapter" data-level="7.1" data-path="spatial.html"><a href="spatial.html#-classes-of-spatial-data"><i class="fa fa-check"></i><b>7.1</b> 공간자료의 종류(classes of spatial data)</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="spatialprocess.html"><a href="spatialprocess.html"><i class="fa fa-check"></i><b>8</b> 공간과정</a><ul>
<li class="chapter" data-level="8.1" data-path="spatialprocess.html"><a href="spatialprocess.html#-stationary-in-spatial-data"><i class="fa fa-check"></i><b>8.1</b> 공간자료의 정상성(stationary in spatial data)</a></li>
<li class="chapter" data-level="8.2" data-path="spatialprocess.html"><a href="spatialprocess.html#strictly-stationary"><i class="fa fa-check"></i><b>8.2</b> 순정상성(strictly stationary)</a></li>
<li class="chapter" data-level="8.3" data-path="spatialprocess.html"><a href="spatialprocess.html#second-order-stationary-weakly-stationary"><i class="fa fa-check"></i><b>8.3</b> 약정상성(second order stationary, weakly stationary)</a></li>
<li class="chapter" data-level="8.4" data-path="spatialprocess.html"><a href="spatialprocess.html#intrinsic-stationary"><i class="fa fa-check"></i><b>8.4</b> 내재정상성(intrinsic stationary)</a></li>
<li class="chapter" data-level="8.5" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-stationarity"><i class="fa fa-check"></i><b>8.5</b> 정상성들 사이의 관계(relationship between stationarity)</a><ul>
<li class="chapter" data-level="8.5.1" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-strong-and-weak-stationary"><i class="fa fa-check"></i><b>8.5.1</b> 순정상성과 약정상성간의 관계(relationship between strong and weak stationary)</a></li>
<li class="chapter" data-level="8.5.2" data-path="spatialprocess.html"><a href="spatialprocess.html#--relationship-between-weak-and-intrinsic-stationary"><i class="fa fa-check"></i><b>8.5.2</b> 약정상성와 내재정상성간의 관계(relationship between weak and intrinsic stationary)</a></li>
<li class="chapter" data-level="8.5.3" data-path="spatialprocess.html"><a href="spatialprocess.html#----counterexample-of-intrinsic-stationary-but-not-weak-stationary"><i class="fa fa-check"></i><b>8.5.3</b> 내재정상성이나 약정상성이 안 되는 예(counterexample of intrinsic stationary but not weak stationary)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="spatialprocess.html"><a href="spatialprocess.html#-ergodic-process"><i class="fa fa-check"></i><b>8.6</b> 에르고딕 과정(ergodic process)</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="covfct.html"><a href="covfct.html"><i class="fa fa-check"></i><b>9</b> 공분산함수</a><ul>
<li class="chapter" data-level="9.1" data-path="covfct.html"><a href="covfct.html#--spectral-representation-theorem"><i class="fa fa-check"></i><b>9.1</b> 스펙트럴 표현 정리(spectral representation theorem)</a></li>
<li class="chapter" data-level="9.2" data-path="covfct.html"><a href="covfct.html#--kolmogorovs-existence-theorem"><i class="fa fa-check"></i><b>9.2</b> 콜모고로프 존재 정리(Kolmogorov’s existence theorem)</a></li>
<li class="chapter" data-level="9.3" data-path="covfct.html"><a href="covfct.html#-properties-of-covariance-functions"><i class="fa fa-check"></i><b>9.3</b> 공분산함수의 성질(properties of covariance functions)</a></li>
<li class="chapter" data-level="9.4" data-path="covfct.html"><a href="covfct.html#isotropy"><i class="fa fa-check"></i><b>9.4</b> 등방성(isotropy)</a></li>
<li class="chapter" data-level="9.5" data-path="covfct.html"><a href="covfct.html#homogeneous"><i class="fa fa-check"></i><b>9.5</b> 동질성(homogeneous)</a></li>
<li class="chapter" data-level="9.6" data-path="covfct.html"><a href="covfct.html#anisotropy"><i class="fa fa-check"></i><b>9.6</b> 이등방성(anisotropy)</a><ul>
<li class="chapter" data-level="9.6.1" data-path="covfct.html"><a href="covfct.html#-geometric-anisotropy"><i class="fa fa-check"></i><b>9.6.1</b> 기하학적 이등방성(Geometric anisotropy)</a></li>
<li class="chapter" data-level="9.6.2" data-path="covfct.html"><a href="covfct.html#-zonal-anisotropy"><i class="fa fa-check"></i><b>9.6.2</b> 띠모양 이등방성(zonal anisotropy)</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="covfct.html"><a href="covfct.html#---continuity-and-differentiabiliy-of-spatial-stochastic-process"><i class="fa fa-check"></i><b>9.7</b> 공간 확률과정의 연속성과 미분가능성(continuity and differentiabiliy of spatial stochastic process</a><ul>
<li class="chapter" data-level="9.7.1" data-path="covfct.html"><a href="covfct.html#path-continuity---path-differentiability"><i class="fa fa-check"></i><b>9.7.1</b> 경로연속(path-continuity) 또는 경로 미분가능성(path-differentiability)</a></li>
<li class="chapter" data-level="9.7.2" data-path="covfct.html"><a href="covfct.html#mean-square-continuity---mean-square-differentiability"><i class="fa fa-check"></i><b>9.7.2</b> 평균제곱연속(mean-square continuity) 또는 평균제곱 미분가능성(mean-square differentiability)</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="covfct.html"><a href="covfct.html#bartlett-bartletts-theorem"><i class="fa fa-check"></i><b>9.8</b> Bartlett의 정리(Bartlett’s theorem)</a></li>
<li class="chapter" data-level="9.9" data-path="covfct.html"><a href="covfct.html#kent---kents-sufficient-condition-for-path-continuity-2-d-ver"><i class="fa fa-check"></i><b>9.9</b> Kent의 경로연속을 위한 충분조건(Kent’s sufficient condition for path-continuity) (2-d ver)</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="covmodel.html"><a href="covmodel.html"><i class="fa fa-check"></i><b>10</b> 공분산모형</a><ul>
<li class="chapter" data-level="10.1" data-path="covmodel.html"><a href="covmodel.html#-nugget-effect"><i class="fa fa-check"></i><b>10.1</b> 덩어리 효과(nugget effect)</a></li>
<li class="chapter" data-level="10.2" data-path="covmodel.html"><a href="covmodel.html#--idealized-shape-of-variogram-isotropic-case"><i class="fa fa-check"></i><b>10.2</b> 이상적인 변동도의 모양(idealized Shape of Variogram (isotropic case))</a></li>
<li class="chapter" data-level="10.3" data-path="covmodel.html"><a href="covmodel.html#-effective-range"><i class="fa fa-check"></i><b>10.3</b> 유효 범위(effective range)</a></li>
<li class="chapter" data-level="10.4" data-path="covmodel.html"><a href="covmodel.html#----classical-parametric-isotropic-variogram-models"><i class="fa fa-check"></i><b>10.4</b> 대표적인 모수 등방성 변동도 모형들(classical parametric isotropic variogram models)</a><ul>
<li class="chapter" data-level="10.4.1" data-path="covmodel.html"><a href="covmodel.html#-----addtional-explanation-for-k_alpha"><i class="fa fa-check"></i><b>10.4.1</b> 변형된 이형 베셀에 대한 보충 설명(addtional explanation for K_alpha)</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="covmodel.html"><a href="covmodel.html#---variograms-in-other-situation"><i class="fa fa-check"></i><b>10.5</b> 기타 다른 상황에서의 변동도들(variograms in other situation)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="variogramest.html"><a href="variogramest.html"><i class="fa fa-check"></i><b>11</b> 변동도의 추정</a><ul>
<li class="chapter" data-level="11.1" data-path="variogramest.html"><a href="variogramest.html#empirical-variogram"><i class="fa fa-check"></i><b>11.1</b> 경험변동도(empirical variogram)</a></li>
<li class="chapter" data-level="11.2" data-path="variogramest.html"><a href="variogramest.html#----fitting-parametric-models-to-empirical-variogram"><i class="fa fa-check"></i><b>11.2</b> 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram)</a></li>
</ul></li>
<li class="part"><span><b>Extreme Value Statistics</b></span></li>
<li class="chapter" data-level="12" data-path="extremevaluestat.html"><a href="extremevaluestat.html"><i class="fa fa-check"></i><b>12</b> 극단값 통계학</a></li>
<li class="chapter" data-level="13" data-path="uGEVtheory.html"><a href="uGEVtheory.html"><i class="fa fa-check"></i><b>13</b> 일변량 극단값 이론</a><ul>
<li class="chapter" data-level="13.1" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--generalized-extreme-value-distribution"><i class="fa fa-check"></i><b>13.1</b> 일반화 극단값 분포(generalized extreme value distribution)</a></li>
<li class="chapter" data-level="13.2" data-path="uGEVtheory.html"><a href="uGEVtheory.html#max-stablity"><i class="fa fa-check"></i><b>13.2</b> 최대안정성(max-stablity)</a></li>
<li class="chapter" data-level="13.3" data-path="uGEVtheory.html"><a href="uGEVtheory.html#-return-level"><i class="fa fa-check"></i><b>13.3</b> 복귀 수준(return level)</a></li>
<li class="chapter" data-level="13.4" data-path="uGEVtheory.html"><a href="uGEVtheory.html#--inference-in-extreme-value-statistics"><i class="fa fa-check"></i><b>13.4</b> 극단값 분포에서의 추론(inference in extreme value statistics)</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://seoncheolpark.github.io/" target="blank">Return to Park's Github Page</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">통계공부와 관련된 글들</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="waveletshrinkage" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> 웨이블릿 수축</h1>
<p>이 장의 주된 내용과 그림들은 <span class="citation">(G. Nason <a href="#ref-Nason2010">2010</a>)</span>를 참고하였다.</p>
<p>다음과 같이 자료를 관찰하는 도메인(domain)인 physical domain (physical model)에서의 모델 <span class="math inline">\(y_{i}=g(x_{i})+e_{i}, i=1,\cdots,n\)</span>에서 관측한 길이 <span class="math inline">\(n\)</span>의 자료 <span class="math inline">\(\mathbf{y}=(y_{1},\cdots,y_{n})^{T}\)</span>이 있다고 하자. 여기서 <span class="math inline">\(x_{i}=\frac{i}{n} \text{ (designed point)}\)</span>이라고 하자. 이 공간(space)은 equally-spaced이고 <span class="math inline">\(x \in (0, 1]\)</span>이다. 우리의 목표는 알려지지 않은 함수 <span class="math inline">\(g(x), x \in [0,1]\)</span>를 추정하는 것이다.</p>
<p>일반적으로 <span class="math inline">\(e_{i} \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^{2})\)</span>으로 가정한다. <strong>독립 동일 분포 가정(independent and identically distributed, iid)</strong>이 없으면 모형이 좀 더 복잡해진다. <strong>정규분포(normal distribution, Gaussian distribution)</strong> 가정도 중요한데, 정규분포처럼 대칭(symmetric)인 분포를 가정하지 않을 경우 평균 추정이 힘들어지므로 보통 <strong>분위수(quantile)</strong> 추정을 하게 된다.</p>
<p>우리가 얻는 자료 <span class="math inline">\(\mathbf{y}\)</span>가 noise가 전혀 없는 순수한 signal이라고 하면, wavelet transform후 바로 wavelet reconstruction을 통해 원래 자료를 얻을 수 있다. <span class="math display">\[\mathbf{y} \xrightarrow{W} \boldsymbol{\delta} \xrightarrow{W^{-1}} \mathbf{y}\]</span> 그런데 자료에 <strong>잡음(noise)</strong>이 있는 경우 얘기가 좀 달라진다. <span class="math display">\[\mathbf{d}=\mathbf{Wy} =\mathbf{Wg}+\mathbf{We}=\boldsymbol{\theta}+\boldsymbol{\epsilon}\]</span> 이 경우에는 위와 같은 방법을 적용하면 잡음이 낀 신호가 그대로 나오게 된다. 우리는 적당한 방법을 통해 noise가 거의 없는 <span class="math inline">\(\hat{\mathbf{d}}\)</span>를 추정해 <span class="math inline">\(\mathbf{W}^{-1}\hat{\mathbf{d}} \rightarrow \hat{\mathbf{g}}\)</span>를 하고 싶다. 이럴 떄 쓰는 방법이 <strong>임계화(thresholding)</strong>이다.</p>
<div id="---main-concept-of-wavelet-shrinkage" class="section level2">
<h2><span class="header-section-number">4.1</span> 웨이블릿 수축의 주된 개념(main concept of wavelet shrinkage)</h2>
<p>다시 원래 얘기로 돌아가서 우리는 웨이블릿 변환을 통해 <span class="math inline">\(g\)</span>를 추정하고자 한다. <span class="math inline">\(\mathbf{W}\)</span>를 이산 웨이블릿 변환 <strong>연산자(연산자)</strong>라고 하면, 다음과 같은 웨이블릿 변환을 생각해 볼 수 있다.</p>
<p><span class="math display">\[\mathbf{y}=\mathbf{g}+\mathbf{e} \rightarrow \mathbf{Wy} =\mathbf{Wg}+\mathbf{We} \text{ or } \mathbf{d}=\boldsymbol{\theta}+\boldsymbol{\epsilon}.\]</span></p>
<p>웨이블릿 변환 연산자는 physical domain에 있는 자료를 wavelet domain (model in the wavelet domain, wavelet-transformed model or wavelet model)으로 보내주는 역할을 한다. 웨이블릿 변환은 정규직교(orthonormal)이므로 변환된 오차항 또한 <span class="math inline">\(\epsilon \sim \mathcal{N}(0,\sigma^{2}\mathbf{I})\)</span>로 정규분포를 따르는 좋은 성질을 가진다. 또 웨이블릿 변환은 오차(error(가 약하게 correlated (stationary process)된 경우 웨이블릿 변환을 하면 변환된 오차가 de-correlated(whitening, 더 약하게 correlated되는 것)되는 좋은 성질이 있다.</p>
<p><strong>웨이블릿 수축(wavelet shrinkage)</strong>을 위해 알아두어야 할 컨셉들은 다음과 같다.</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{\theta}\)</span>는 많은 함수들의 <strong>성긴 벡터(sparse vector)</strong>이다. 그리고 <span class="math inline">\(\boldsymbol{\theta}\)</span>는 다음과 같이 Parseval’s identity를 만족시킨다. 즉 데이터의 에너지와 계수들의 에너지가 같다(보존된다). <span class="math display">\[\sum g^{2}(x)=\sum \theta_{i}^{2}.\]</span></p></li>
<li><p><span class="math inline">\(\boldsymbol{\theta}\)</span>는 “concentrated”되어있다.</p></li>
<li><p><span class="math inline">\(\epsilon \sim \mathcal{N}(0,\sigma^{2}\mathbf{I})\)</span>, 즉 웨이블릿 계수 <span class="math inline">\(\mathbf{d}\)</span>에는 <span class="math inline">\(\boldsymbol{\theta}\)</span>뿐 아니라 <span class="math inline">\(\mathbf{\epsilon}\)</span>의 정보도 들어있다.</p></li>
<li><p>위의 사실에 기초하여 <span class="math inline">\(\mathbf{d}\)</span>중 값이 큰 원소의 경우에는 진짜 신호 + 잡음의 형태로 이루어져 있을 것이다.</p></li>
<li><p><span class="math inline">\(\mathbf{d}\)</span>중 값이 작은 원소의 경우에는 잡음만 있을 것이다.</p></li>
</ul>
<p>이런 상황에서는 평균이 틀리게 된다. 즉 <span class="math inline">\(\hat{\theta}=\frac{1}{n}\sum_{i=1}^{n}d_{i}\)</span>가 <span class="math inline">\(\theta\)</span>의 좋은 추정량이 될 수 없다는 것이다. 그래서 이를 해결하기 위해 도입된 아이디어가 <strong>임계화(thresholding)</strong>이다. 임계화가 등장하기 전까지 모든 추정량에는 평균 개념이 있었다. (ex. Ridge) 기존의 자료분석들은 “aggregation”에 치중했다. 모든 변수에 다 신호가 존재한다고 생각한 것이다. 이런 방식으로는 위의 문제를 해결할 수 없다. 그러나 웨이블릿 변환의 등장 이후에는 “sparsity” 개념이 등장하였고 몇 개의 신호만 선택하게 되었다. 이 개념 덕분 에 고차원(high-dimensional) 자료(<span class="math inline">\(n \ll p\)</span>)를 분석할 수 있게 되었다.</p>
<p>수축 방법에는 두 가지가 있다. <strong>하드 임계화(hard thresholding)</strong>와 <strong>소프트 임계화(soft thresholding)</strong>가 그것이다. 두 임계화를 다음과 같이 정의한다.</p>
<div class="definition">
<p>주어진 (empirical) 웨이블릿 상수 d와 <strong>threshold</strong> <span class="math inline">\(\mathbf{\lambda}\)</span>가 있을 때, 그것의 <strong>하드 임계화(hard thresholding)</strong>는 <span class="math display">\[\hat{\theta}_{H}=\eta_{H}(d,\lambda)=d\mathbb{I}\{ |d| &gt; \lambda \}\]</span> 이다. 그리고 <strong>소프트 임계화(soft thresholding)</strong>는 <span class="math display">\[\hat{\theta}_{S}=\eta_{S}(d,\lambda)=\text{sgn}(d)(|d|-\lambda)\mathbb{I}\{ |d| &gt; \lambda \}\]</span> 이다.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-15"></span>
<img src="images/waveletshrinkage_thresholding.png" alt="Hard thresholding (dotted line) and soft thresholding." width="263" />
<p class="caption">
Figure 4.1: Hard thresholding (dotted line) and soft thresholding.
</p>
</div>
<p>두 방법 다 공통적으로 <span class="math inline">\(\mathbf{d} \in (-\lambda, \lambda)^{n}\)</span>이면 0이 된다.</p>
</div>
<p>하드 임계화는 “keep or kill” 방법이라고도 불린다. 그 이유는 값이 어떤 threshold(<span class="math inline">\(\mathbf{\lambda}\)</span>)보다 작을 경우 무조건 0으로 놓기 때문이다. 이것은 회귀분석의 변수 선택(variable selection)과 동일한 아이디어이다. 변수 선택에서도 변수를 넣기 또는 빼기 두 가지 선택지만 있다는 것을 생각하기 바란다. 그리고 하드임계화에서는 축소를 하지 않는다. 소프트 임계화는 하드 임계화를 함과 동시에 신호 변환 함수가 연속이 되도록 값이 큰 signal도 같이 축소(shrinkage)하는 방법이다. 이는 변수선택에서 LASSO와 대응되는 방법이다.</p>
<p>때때로 굉장히 큰 <span class="math inline">\(\mathbf{d}\)</span>에는 오차가 작게 들어있을 것이라 생각할 수도 있다. 이를 보완하기 위해 SCAD 같은 방법들이 나중에 제안되었는데, 원래 이는 웨이블릿을 연구하는 학자들이 생각했던 개념으로 이를 통계학 언어로 옮긴 것에 불과하다.</p>
<p>여기서 등장하는 <span class="math inline">\(\lambda\)</span>는 <strong>핵평활(kernel smoothing)</strong>이나 <strong>평활 스플라인(smoothing spline)</strong>에서 나오는 <strong>띠너비(bandwidth)</strong>와 비슷한 개념이라고 생각하면 된다. <span class="math inline">\(\lambda\)</span>의 선택 또한 중요한 이슈가 된다. 이것을 어떻게 선택하느냐에 따라 performance가 굉장히 변하고 <span class="math inline">\(\hat{g}\)</span>의 질(quality)에 영향을 미친다. <span class="math display">\[y \xrightarrow{W} d \xrightarrow{\text{th}} \hat{\theta}_{Shrink} \xrightarrow{W^{-1}} \hat{g}.\]</span></p>
</div>
<div id="oracle" class="section level2">
<h2><span class="header-section-number">4.2</span> 오라클(oracle)</h2>
<p>만약 우리가 <span class="math inline">\(g\)</span>를 알고 있다면, <span class="math inline">\(\hat{g}\)</span>의 quality를 계산하는 방법 중 하나로 다음과 같은 <strong>적분제곱오차(integrated squared error, ISE, <span class="math inline">\(\hat{M}\)</span>)</strong>를 생각해 볼 수 있다.</p>
<p><span class="math display">\[\hat{M}=\frac{1}{n}\sum_{i=1}^{n}(\hat{g}(x_{i})-g(x_{i}))^{2}.\]</span> 그러나 우리는 <span class="math inline">\(g\)</span>를 모르기 때문에 실제로 ISE를 계산할 수는 없다. 대신 ‘평균(average)’ 개념을 적용한 <strong>평균적분제곱오차(mean integrated squared err, MISE)</strong> <span class="math inline">\(E(\hat{M})\)</span>을 정의한다.</p>
<p><span class="math display">\[ M \triangleq E(\hat{M})=\text{Risk of }\hat{g}.\]</span> 웨이블릿에서 <span class="math inline">\(\hat{g}\)</span>는 <span class="math inline">\(\lambda, \eta, \theta\)</span>에 좌우(depend)한다. 참고로 보통 <span class="math inline">\(g\)</span>가 정의되는 함수공간 <span class="math inline">\(g \in \mathcal{F}\)</span>은 일반적으로 <span class="math inline">\(L^{2}(\mathbb{R})\)</span>에서만 생각한다. 웨이블릿은 점프가 있는 함수도 다룰 수 있긴 하다. 결국 통계적 추정의 목표는 이 MISE를 최소화하는 <span class="math inline">\(\hat{g}\)</span>를 찾는 것이다.</p>
<p>웨이블릿에서 <span class="math inline">\(\hat{M}=\sum_{j,k}(\hat{\theta}_{jk}-\theta_{jk})^{2}\)</span>이다. 여기서 웨이블릿 변환은 정규직교이므로 ’decoupling’이라는 성질을 이용할 수 있다. 이 얘기는 위의 값을 계산할 때 <span class="math inline">\(j,k\)</span>를 무시하고 마치 하나만 있는 것처럼 계산해도 된다는 것이다. 마치 벡터(vector)를 스칼라(scalar)처럼 볼 수 있다는 것이다.</p>
<p>잠시 선형 회귀분석 모형을 복습해보자. 다음과 같은 선형 회귀분석 모형 <span class="math display">\[\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\]</span> 이 있다고 하자. 여기서 <span class="math inline">\(\mathbf{y}\)</span>는 <span class="math inline">\(n \times 1\)</span> 행렬, <span class="math inline">\(\mathbf{X}\)</span>는 <span class="math inline">\(n \times d\)</span> 행렬, <span class="math inline">\(\boldsymbol{\beta}\)</span>는 <span class="math inline">\(d \times 1\)</span> 행렬, 그리고 <span class="math inline">\(\boldsymbol{\epsilon}\)</span>은 <span class="math inline">\(n \times 1\)</span> 행렬이다. 만약 여기서 <span class="math inline">\(X\)</span>의 열(column)이 정규직교라고 해보자. 그러면 <span class="math display">\[\begin{eqnarray*}
\hat{\boldsymbol{\beta}}&amp;=&amp;(\hat{\beta}_{1},\cdots,\beta_{d})^{T}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}=\mathbf{X}^{T}\mathbf{y}\\
&amp;\Longrightarrow&amp; \hat{\beta}_{1}=\sum X_{i1}y_{i}, \hat{\beta}_{2}=\sum X_{i2}y_{i}, \cdots
\end{eqnarray*}
\]</span> 로 모든 <span class="math inline">\(\boldsymbol{\beta}\)</span>의 원소들이 separate(decoupled)되는 것을 볼 수 있다. 참고로 <span class="math inline">\((\mathbf{X}^{T}\mathbf{X})^{-1}\neq \mathbf{I}\)</span>인 경우 <span class="math inline">\(\hat{\beta}\)</span>가 다 연결되므로 이렇게 분석할 수 없다. 그리고 <span class="math display">\[\hat{\boldsymbol{\beta}}=\min \| \mathbf{y}-\mathbf{X}\boldsymbol{\beta} \|^{2} \Leftrightarrow \min \| \mathbf{X}^{T}\mathbf{y} - \mathbf{X}^{T}\boldsymbol{\beta} \|^{2}=\min \| \hat{\boldsymbol{\beta}}-\boldsymbol{\beta} \|^{2}\]</span> 가 된다. <span class="citation">(Donoho and Johnstone <a href="#ref-Donoho1994">1994</a>)</span> 논문에 갑자기 이 사실을 이용해 전개하는 내용이 있다.</p>
더 나아가 벌점화 최소자승법(penalized least square) 문제를 생각해보자. <span class="math inline">\(\mathbf{z}=\mathbf{X}^{T}\mathbf{y}\)</span>, <span class="math inline">\(\hat{\mathbf{y}}=\mathbf{Xz}=\mathbf{XX}^{T}\mathbf{y}\)</span>를 정의하면
\begin{eqnarray*}
\| \mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|^{2}+\lambda \sum_{j=1}^{d}P(| \beta_{j} |) &amp;=&amp; \| \mathbf{y}-\hat{\mathbf{y}}+\hat{\mathbf{y}}-\mathbf{X}\boldsymbol{\beta}\|^{2}+\lambda \sum_{j=1}^{d}P(|\beta_{j}|)\\
&amp;=&amp;\| \mathbf{y} -\hat{\mathbf{y}} \|^{2} + \sum_{j}(z-{j}-\beta_{j})^{2}=\lambda\sum_{j}P(|\beta_{j}|)\\
\end{eqnarray*}
<p>여기서 <span class="math inline">\(\| \mathbf{y} -\hat{\mathbf{y}} \|^{2}\)</span>는 <span class="math inline">\(\beta_{j}\)</span>와 관련 없으므로 뒤의 두 항만 최소화(minimize)하면 된다. 그런데 <span class="math inline">\(\beta_{j}\)</span>는 seperate되므로 벌점화 최소자승법의 해는 <span class="math display">\[\hat{\beta}=\min_{\beta}(z-\beta)^{2}+\lambda P(| \beta |)\]</span> 이다.</p>
<p>다시 웨이블릿 문제로 돌아가서, 웨이블릿 변환 행렬 <span class="math inline">\(\mathbf{W}\)</span> (회귀분석에서 <span class="math inline">\(\mathbf{X}\)</span>와 같은 역할을 함)이 정규직교이므로, 우리는 <span class="math inline">\(E(\hat{\theta}-\theta)^{2}\)</span>(=risk)만 보면 된다. 참고로 <span class="math inline">\(\mathbf{W}\)</span>는 정방행렬(square matrix)이라는 점에서 <span class="math inline">\(\mathbf{X}\)</span>와 다르다. Separated 성질에 의해</p>
<p><span class="math display">\[
M(\hat{\theta},\theta)=E(\hat{\theta}-\theta)^{2} =
\begin{cases}
E(d-\theta)^{2}=E\epsilon^{2} &amp; \text{if $|d| &gt; \lambda$}\\
E(\theta^{2})=\theta^{2} &amp; \text{o.w.}
\end{cases}
\]</span> 이 된다. 결론적으로 임계화(thresholding)를 위해서는 신호와 오차의 크기를 비교해 보면 되는데, 만약 신호가 오차보다 굉장히 큰 경우, <span class="math inline">\(\theta \gg \sigma\)</span>인 경우면 우리는 <span class="math inline">\(|d| &gt; \lambda\)</span>인 경우를 취하는게 유리하므로 <span class="math inline">\(\lambda\)</span>를 작게 선택하면 된다. 반대의 경우에는 <span class="math inline">\(\lambda\)</span>를 크게 취하는 것이 유리하다.</p>
<p>통계학에서 오라클이라는 개념을 처음 사용한 사람은 Dave Donoho이다. 오라클이라는 개념이 처음 등장하는 논문은 <span class="citation">(Donoho and Johnstone <a href="#ref-Donoho1994">1994</a>)</span>인데, 오라클을“With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel to the unknown function”이라고 소개하고 있다. 교수님의 요약은 다음과 같다. “The oracle is notional device that tells you which coefficients you should select.”</p>
<p>오라클에 의한 ideal risk는(hard thresholding의 경우) <span class="math inline">\(M_{ideal}=\sum_{j,k}\min(\theta_{j,k}^{2},\sigma^{2})\)</span>이다. 그렇다면 <span class="math inline">\(\hat{\theta}\)</span>를 어떻게 구하는가? 이 문제는 결국 <span class="math inline">\(\eta_{H}, \eta_{S}, \lambda\)</span>를 선택하는 문제로 귀착된다. Donoho와 Johnstone은 <span class="math inline">\(M_{ideal}\approx M\)</span>이 되게 하는 <span class="math inline">\(\hat{\theta}\)</span>를 몇 가지 제시하였다. <span class="citation">(Donoho and Johnstone <a href="#ref-Donoho1994">1994</a>)</span>에서 <span class="math inline">\(\hat{\theta}=\eta_{x}(d,\lambda)\)</span>, <span class="math inline">\(\lambda=\sigma\sqrt{2\log n}\)</span>으로 할 시 <span class="math display">\[M_{\text{universal}}\leq(2\log n +1)(\sigma^{2}+M_{\text{ideal}})\]</span> 임을 증명하였다. 다시 말하면 이 <span class="math inline">\(\hat{\theta}\)</span>가 오라클 성질(oracle property)과 굉장히 유사하며 <span class="math inline">\(M_{\text{ideal}}\)</span>에 가깝게(대략<span class="math inline">\(2\log n\)</span>배 보다 작다) 행동한다는 것이다. 위 논문에 따르면 핵평활(kernel smoothing)이나 평활 스플라인(smoothing spline)도 <span class="math inline">\(2\log n\)</span>을 만족하지 못한다(n). 가장 이상적인 fitting은 정확한 knot point들을 모두 알고 있는 piecewise polynomial이다. 그러나 ideal한 knot을 모두 안다는 것은 true을 안다는 것이므로 이는 불가능하다. Bandwidth나 knot selection을 잘 한다는 것은 true의 분산을 안다는 것과 거의 같은 얘기다.</p>
</div>
<div id="-universal-thresholding" class="section level2">
<h2><span class="header-section-number">4.3</span> 만능 임계화(universal thresholding)</h2>
<p>앞서 등장한 <span class="math inline">\(\lambda^{u}=\sigma \sqrt{2 \log n}\)</span>을 특별히 <strong>만능 임계화(universal Thresholding)</strong>라고 한다. 실제로는 <span class="math inline">\(\sigma\)</span>를 모르므로 <span class="math inline">\(\hat{\sigma}\)</span>를 사용한다.</p>
<div class="theorem">
<p><span class="math inline">\(X_{1},\cdots , X_{n}\)</span>을 <span class="math inline">\(EX_{i}=0, EX_{i}^{2}=1, EX_{i}X_{i+k}=\gamma(k)\)</span>인 stationary Gaussian process (Lag-k covariance structure를 갖는 Gaussian process)라고 하고 특별히 <span class="math inline">\(X_{(n)}=\max \{ X_{i} \}\)</span>라 하자. 만약 <span class="math inline">\(\lim_{k \rightarrow \infty} \gamma (k) =0\)</span>이면, <span class="math display">\[\frac{X_{(n)}}{\sqrt{2 \log n}} \rightarrow 1 \text{ as } n \rightarrow \infty\]</span> 이다.</p>
</div>
<p>위 정리는 n Gaussian 확률 변수들 중 가장 큰 것은(독립일 필요는 없다) 대략 <span class="math inline">\(\sqrt{2 \log n}\)</span> 사이즈라는 것이다. 이 정리에 비추어 만능 임계화를 생각하면 이 임계화는 오차(error)가 Gaussian random variable을 따르는 것이라면 모두 다 임계화하겠다는 뜻으로 해석할 수 있다. 이 방법은 이론적으로는 완벽해 보이기는 하나 너무나 많은 잡음(noise)을 줄이는 underfit한 임계화이다. 결국 SURE와 같은 실용적인 임계화 방법을 생각하게 된 것이다. 이 얘기는 추후에 다시 나올 것이다.</p>
<p>만능 임계화로 돌아가서, 우리는 <span class="math inline">\(\sigma\)</span>를 모르므로 대신 <span class="math inline">\(\hat{\lambda}^{u}=\hat{\sigma}\sqrt{2 \log n}\)</span>을 이용해야 할 것이다. 그렇다면 <span class="math inline">\(\sigma\)</span>를 어떻게 추정할 것인가? 대부분의 방법은 data를 제외한 가장 finest scale (ex.J-1)의 detail 웨이블릿 계수(<span class="math inline">\(d_{J-1}\)</span>)를 이용해 추정한다. <span class="math inline">\(y\)</span>를 이용해 <span class="math inline">\(\epsilon\)</span>의 분산을 추정하려고 할 경우 <span class="math inline">\(f\)</span>의 정보가 너무 강해서 <span class="math inline">\(\epsilon\)</span>의 분산구조를 알 수 없을 것이다. 그리고 좀 더 성긴 스케일(coarser scale)로 갈수록 잡음보다는 신호 정보가 많을 것이라는 생각을 하면, <span class="math inline">\(d_{J-1}\)</span>를 이용해 분산 구조를 추정하는 것이 당연하다.</p>
<p>가장 널리 알려진 방법은 <span class="math display">\[\hat{\sigma}=\sqrt{\frac{1}{n/2-1}\sum_{k=1}^{n/2}(d_{J-1,k}-\bar{d_{J-1}})^{2}}\]</span> 이다. 이 방법은 자료가 희소(sparse)한 경우에는 잘 맞지 않음이 알려져 있다. 그런 경우에는 대신 중앙값(median)을 이용하여 <span class="math display">\[
\hat{\sigma}=1.4826 \times \text{median}(|d_{J-1,1}-\tilde{d_{J-1}}|,\cdots,|d_{J-1,\frac{n}{2}}-\tilde{d_{J-1}}|\\
\text{ where } \tilde{d}_{J-1}=\text{median}(\mathbf{d}_{J-1})
\]</span> 이런 식으로 추정하기도 한다.</p>
<p>지금까지 했던 방법은 universal threshold rule(<span class="math inline">\(\lambda^{u}\)</span>)에 soft thresholding function <span class="math inline">\(\eta_{s}\)</span>를 적용한 <span class="math inline">\(\hat{\theta}=\eta_{s}(d,\lambda^{u})\)</span>로 이것을 <strong>VisuShrink</strong>라고 부른다. 이 방법은 앞서 말한 대로 noise-free reconstrunction이나 oversmooth (underfit)하는 문제가 생긴다. 즉 noise-free하지만 signal도 너무 많이 죽일 가능성이 있다는 것이다. 한편 <span class="math inline">\(\lambda^{u}\)</span>는 noise-free reconstruction을 하는 최소의 <span class="math inline">\(\lambda\)</span>이므로, <span class="math inline">\(0&lt; \lambda^{*} \ll \lambda^{u}\)</span>인 <span class="math inline">\(\lambda^{*}\)</span>를 생각할 수 있을 것이다. 이런 <span class="math inline">\(\lambda^{*}\)</span> 중의 하나로 <span class="citation">(Donoho and Johnstone <a href="#ref-Donoho1994">1994</a>)</span>에서는 <strong>RiskShrink</strong>라는 것을 제시했다. 이 방법은 <span class="math inline">\(\Lambda_{n}^{*}(=2 \log n +1)\)</span>에 해당하는 <span class="math inline">\(\Lambda_{n}^{*}\)</span>과 이에 대응되는 <span class="math inline">\(\lambda^{*}\)</span>을 table 형태로 계산한 것이다. 예를 들어 <span class="math inline">\(n=1024\)</span> 일 때 <span class="math inline">\(\lambda^{u}=3.72, \lambda^{*}=2.23, \Lambda_{n}^{*}=5.976\)</span>이다.</p>
<p>참고로 이 논문의 결과와 더불어 일반적으로 알려져 있는 사실은 함수가 부드럽(smooth)지 않을 때 웨이블릿이 다른 어떤 비모수 방법들보다 좋다는 것이다.</p>
</div>
<div id="stein---steins-unbiased-risk-estimator-sure" class="section level2">
<h2><span class="header-section-number">4.4</span> Stein의 불편 위험 추정량(Steins Unbiased Risk Estimator (SURE))</h2>
<p>앞서 얘기했던 VisuShirnk나 RiskShrink는 이론상으로는 완벽하나 실용성이 떨어져 실제로는 많이 쓰이지 않고 있다. 실제로 많이 쓰이는 shrinkage 방법 중 하나가 <strong>Steins Unbiased Risk Estimator (SURE)</strong>이다. Shrinkage 추정량들은 Bayesian과 밀접한 관련이 있다. Bayesian들이 주로 하는 것은 자료를 prior의 정보에 민감하게 반응하도록 수축(shrinkage)해 주는 것이다.</p>
<p>SURE가 처음 등장한 논문은 <span class="citation">(Donoho and Johnstone <a href="#ref-Donoho1995">1995</a>)</span>로, <span class="citation">(Stein <a href="#ref-Stein1981">1981</a>)</span>의 내용을 웨이블릿 도메인으로 갖고 온 것이다. 다음과 같은 data domain에서의 모델 <span class="math inline">\(y_{i}=g(x_{i})+e_{i}, i=1,\cdots,n,e_{i} \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^{2})\)</span>과 이를 웨이블릿 도메인(domain)으로 옮긴 <span class="math inline">\(\mathbf{d}=\boldsymbol{\theta}+\boldsymbol{\epsilon}, \boldsymbol{\epsilon} \sim \mathcal{N}(0,\sigma^{2}\mathbf{I})\)</span>를 생각하자. Stein의 논문에서는 이 notation을 다음과 같이 썼다. <span class="math display">\[\mathbf{x}=\boldsymbol{\mu}+\boldsymbol{\epsilon}.\]</span></p>
<div class="theorem">
<p><span class="citation">(Stein <a href="#ref-Stein1981">1981</a>)</span> 만약 <span class="math inline">\(\hat{\boldsymbol{\mu}}\mathbf{(x)}=\mathbf{x}+\mathbf{g(x)}\)</span>, <span class="math inline">\(g:\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}\)</span> is weakly differentiable 조건이면 <span class="math display">\[E \| \hat{\boldsymbol{\mu}}\mathbf{(x)}-\boldsymbol{\mu} \|^{2} =n+ E\{ \|\mathbf{g(x)}\|^{2}+2\bigtriangledown \cdot \mathbf{g(x)} \}, \bigtriangledown\cdot \mathbf{g}=\sum_{i} \frac{\partial}{\partial x_{i}}g_{i}\]</span> 이다.</p>
</div>
<p>그리고 <span class="math inline">\(\hat{\mu}_{i}(\lambda)=\eta_{s}(x_{i},\lambda) \Rightarrow \frac{\partial}{\partial x_{i}}\hat{\mu}_{i}(\lambda)=I( |x_{i}|&gt;\lambda)\)</span>과 <span class="math inline">\(\| \mathbf{g(x)} \|^{2}=\sum \hat{\mu}_{i}(\lambda, x)^{2}=\sum_{i=1}^{n}(|x_{i}|-\lambda)^{2}I(|x_{i}&gt;\lambda)\)</span> 사실을 이용해 SURE를 정의한다.</p>
<div class="theorem">
<p><span class="math inline">\(\text{SURE}(\lambda,\mathbf{x})=n-2\#\{ i: |x_{i}| \leq \lambda \} + \sum_{i=1}^{n}(|x_{i}| \wedge \lambda)^{2}\)</span>는 risk의 불편추정량이다. <span class="math display">\[E \| \eta_{s}(\mathbf{x},\lambda)-\mu \|^{2}=E\text{SURE}(\lambda, \mathbf{x}).\]</span> 실제로, <span class="math inline">\(\lambda=\text{argmin}_{0&lt;\lambda \leq lambda^{u}}\text{SURE}(\lambda,\mathbf{x})\)</span>이며, 이 방법을 <strong>SUREShrink</strong>라고 한다.</p>
</div>
</div>
<div id="r-r-waveletshrinkage" class="section level2">
<h2><span class="header-section-number">4.5</span> R 예제(R-waveletshrinkage)</h2>
<p>다음은 R 패키지 <code>wavethresh</code>를 이용한 축소 예제이다. 임계화를 위해 <code>threshold</code>라는 함수를 사용하며 <code>type</code> 및 <code>policy</code>를 선택할 수 있다.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))
<span class="kw">set.seed</span>(<span class="dv">1234</span>)
data_bump &lt;-<span class="st"> </span><span class="kw">example.1</span>()
x &lt;-<span class="st"> </span>data_bump$x; y &lt;-<span class="st"> </span>data_bump$y
<span class="kw">plot</span>(x,y, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Original&quot;</span>)
y_noise &lt;-<span class="st"> </span>y +<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(y), <span class="dt">sd=</span><span class="fl">0.1</span>)
<span class="kw">plot</span>(x,y_noise, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Noisy&quot;</span>)
y_noise_wd &lt;-<span class="st"> </span><span class="kw">wd</span>(y_noise)
y_noise_threshold &lt;-<span class="st"> </span><span class="kw">threshold</span>(y_noise_wd, <span class="dt">type=</span><span class="st">&quot;soft&quot;</span>, <span class="dt">policy=</span><span class="st">&quot;sure&quot;</span>)
y_sure &lt;-<span class="st"> </span><span class="kw">wr</span>(y_noise_threshold)
<span class="kw">plot</span>(x,y_sure, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">main=</span><span class="st">&quot;Soft Thresholding&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-17"></span>
<img src="SeoncheolPark-book_files/figure-html/unnamed-chunk-17-1.png" alt="Wavelet shrinkage example." width="672" />
<p class="caption">
Figure 4.2: Wavelet shrinkage example.
</p>
</div>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Nason2010">
<p>Nason, Guy. 2010. <em>Wavelet Methods in Statistics with R</em>. 2nd ed. Spring Street, New York: Springer Science &amp; Business Media.</p>
</div>
<div id="ref-Donoho1994">
<p>Donoho, David L., and Iain M. Johnstone. 1994. “Ideal Spatial Adaptation by Wavelet Shrinkage.” <em>Biometrika</em> 81 (3): 425–55. doi:<a href="https://doi.org/10.1093/biomet/81.3.425">10.1093/biomet/81.3.425</a>.</p>
</div>
<div id="ref-Donoho1995">
<p>Donoho, David L., and Iain M. Johnstone. 1994. “Ideal Spatial Adaptation by Wavelet Shrinkage.” <em>Biometrika</em> 81 (3): 425–55. doi:<a href="https://doi.org/10.1093/biomet/81.3.425">10.1093/biomet/81.3.425</a>.</p> 1995. “Adapting to Unknown Smoothness via Wavelet Shrinkage.” <em>Journal of the American Statistical Association</em> 90 (4): 1200–1224. doi:<a href="https://doi.org/10.1080/01621459.1995.10476626">10.1080/01621459.1995.10476626</a>.</p>
</div>
<div id="ref-Stein1981">
<p>Stein, Charles M. 1981. “Estimation of the Mean of a Multivariate Normal Distribution.” <em>The Annals of Statistics</em> 9 (6): 1135–51. doi:<a href="https://doi.org/10.1214/aos/1176345632">10.1214/aos/1176345632</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="wavelettransform.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advwaveletshrinkage.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://seoncheolpark.github.io/book/_book/13-waveletshrinkage.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
