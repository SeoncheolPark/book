# 주성분분석 {#PCA}

**주성분분석(principal component analysis, PCA)**란 **차원 축소(dimension reduction)**을 위해 많이 쓰이는 툴이다. 이 분야의 유명한 참고문헌으로는 [@Jolliffe2002]가 있다. 주성분분석은 차원 축소방법 중 변수를 직접적으로 변환하는 변수변환(feature transformation) 방법이다.

여러 다변량 데이터셋이 갖는 문제들 중 하나는 변수가 너무 많다는 것이다. 너무 많은 변수들이 가질 수 있는 문제로는 차원의 저주(curse of dimensionality)가 알려져 있다.

## 차원의 저주(curse of dimensionality)

이러한 문제로부터 데이터셋에 존재하는 원래의 변동(variation)을 가능한 한 많이 설명하면서 다변량 데이터의 차원을 줄이는 것을 주된 목적으로 하는 다변량 기법인 주성분분석이 만들어졌다. 이러한 목적은 원래 변수(original variable)들의 선형결합(linear combination) 형태로 새로운 변수 집합인 **주성분(principal component)**로 변환함으로써 얻을 수 있다. 주성분들은 서로 상관되지 않으며 순서화되어 처음 몇 개의 주성분이 원래 변수들의 변동(variation)을 대부분 설명하도록 한다. 주성분분석의 결과는 많은 원래 변수들의 대용(surrogate)으로서 사용될 수 있는 작은 개수의 새로운 변수를 만드는 것이다.

## 주성분분석의 기본 목적(basic object of PCA)

다음과 같이 $\mathbf{x}_{i}\in\mathbb{R}^{d}, i=1,\ldots, n$이라는 training pattern이 있다고 하자. 주성분은 $q <d$의 직교정규 벡터의 집합이며 첫번째 주성분은 데이터를 한 개의 축으로 사상(projection)시켰을 때 그 분산이 가장 커지는 축이다.

$\mathbf{y}$를 subspace로의 projection이라고 하자. $\mathbf{W}$를 column에 principal component를 포함하는 $d\times q$ 행렬이라고 하자. 그러면
$$\mathbf{y}=\mathbf{W}^{T}\mathbf{x}$$
가 된다. 즉 $\mathbf{y}$는 $\mathbf{x}$의 dimension-reduced 표현이 되는 것이다. $\hat{\mathbf{x}}$를 $\mathbf{y}$가 주어졌을 때 $\mathbf{x}$의 reconstruction이라고 하자.
$$\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}.$$
그리면 PCA의 목표는 다음과 같은
$$E_{rec}=\frac{1}{n}\sum_{i=1}^{n}\| \mathbf{x}_{i}-\hat{\mathbf{x}}_{i}\|^{2}$$
$E_{rec}$을 minimize하도록 subspace를 set하는 것이다.

## 주성분 분석의 결과 정리(PCA summary)

박창이 교수님 외 R을 이용한 데이터마이닝 책을 참고하였다. 주성분 분석의 간단한 결과 요약은 다음과 같다.

1. 주성분은 원 변수들의 공분산행렬 또는 상관계수행렬의 고유벡터로부터 구해진다.

2. 임의의 두 주성분간의 공분산은 항상 0이므로 주성분들 간에는 상관성이 존재하지 않는다.

3. 원 자료의 변수들의 분산의 합은 주성분변수들의 분산의 합과 항상 일치한다.

4. 주성분의 각 적재계수(loading coefficient)는 특정 변수가 주성분변수에 기여하는 정도를 나타낸다.

## 주성분 회귀분석(principal component regression)

[@Jolliffe2002]의 내용을 참고한다. 다음과 같은 standard 회귀모형이 있다고 하자. 즉 모형은 다음과 같다.

\begin{equation}
\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}
(\#eq:PCreg01)
\end{equation}

이 때 $\mathbf{y}$는 dependent variable의 평균에서 잰 $n$개 관찰값들의 벡터이고, $\mathbf{X}$는 $n\times p$ predictor matrix, $\boldsymbol{\beta}$는 $p$개 회귀분석 계수들의 벡터, $\boldsymbol{\epsilon}$는 오차항들의 벡터이다. 이 때 $\boldsymbol{\epsilon}$의 원소들은 각각 독립이고, 분산은 $\sigma^{2}$이다.

참고로 주성분 회귀분석에서는 자료들을 센터링(centering)하는 것이 좋다.

```{r, echo=F, fig.cap='주성분분석에서 센터링을 해야 하는 이유.', fig.align='center'}
knitr::include_graphics("images/pca_pcreg.png")
```

## 다변량 자료에서의 PCA (PCA for multivariate data)

[@Ramsay2005]의 8장을 참고하였다. 다변량 자료에서의 기초 컨셉은 다음과 같은 설명변수들의 선형 결합을 취하는 것이다.
$$f_{i}=\sum_{j=1}^{p}\beta_{j}x_{ij}, i=1,\ldots, N.$$
여기서 $\beta_{j}$는 $j$번째 변수에 대한 $x_{ij}$ 관련 가중 계수(weighting coefficient)이다.

## 함수자료에서의 PCA (PCA for functional data)

