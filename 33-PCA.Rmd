# 주성분분석 {#PCA}

**주성분분석(principal component analysis, PCA)**란 **차원 축소(dimension reduction)**을 위해 많이 쓰이는 툴이다. 이 분야의 유명한 참고문헌으로는 [@Jolliffe2002]가 있다. 주성분분석은 차원 축소방법 중 변수를 직접적으로 변환하는 변수변환(feature transformation) 방법이다. 기상학에서는 empirical orthogonal functions라고 부른다.

여러 다변량 데이터셋이 갖는 문제들 중 하나는 변수가 너무 많다는 것이다. 너무 많은 변수들이 가질 수 있는 문제로는 차원의 저주(curse of dimensionality)가 알려져 있다.

## 차원의 저주(curse of dimensionality)

이러한 문제로부터 데이터셋에 존재하는 원래의 변동(variation)을 가능한 한 많이 설명하면서 다변량 데이터의 차원을 줄이는 것을 주된 목적으로 하는 다변량 기법인 주성분분석이 만들어졌다. 이러한 목적은 원래 변수(original variable)들의 선형결합(linear combination) 형태로 새로운 변수 집합인 **주성분(principal component)**로 변환함으로써 얻을 수 있다. 주성분들은 서로 상관되지 않으며 순서화되어 처음 몇 개의 주성분이 원래 변수들의 변동(variation)을 대부분 설명하도록 한다. 주성분분석의 결과는 많은 원래 변수들의 대용(surrogate)으로서 사용될 수 있는 작은 개수의 새로운 변수를 만드는 것이다.

## 주성분분석의 기본 목적(basic object of PCA)

다음과 같이 $\mathbf{x}_{i}\in\mathbb{R}^{d}, i=1,\ldots, n$이라는 training pattern이 있다고 하자. 주성분은 $q <d$의 직교정규 벡터의 집합이며 첫번째 주성분은 데이터를 한 개의 축으로 사상(projection)시켰을 때 그 분산이 가장 커지는 축이다.

$\mathbf{y}$를 subspace로의 projection이라고 하자. $\mathbf{W}$를 column에 principal component를 포함하는 $d\times q$ 행렬이라고 하자. 그러면
$$\mathbf{y}=\mathbf{W}^{T}\mathbf{x}$$
가 된다. 즉 $\mathbf{y}$는 $\mathbf{x}$의 dimension-reduced 표현이 되는 것이다. $\hat{\mathbf{x}}$를 $\mathbf{y}$가 주어졌을 때 $\mathbf{x}$의 reconstruction이라고 하자.
$$\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}.$$
그리면 PCA의 목표는 다음과 같은
$$E_{rec}=\frac{1}{n}\sum_{i=1}^{n}\| \mathbf{x}_{i}-\hat{\mathbf{x}}_{i}\|^{2}$$
$E_{rec}$을 minimize하도록 subspace를 set하는 것이다.

## 모집단 주성분들(population principal components)

[@Izenman2009]를 참고하였다.

다음과 같은 무작위 $r$-벡터
$$\mathbf{X}=(X_{1},\ldots, X_{t})^{T}$$
가 평균이 $\boldsymbol{\mu}_{X}$이고 $(r\times r)$ 공분산행렬 $\boldsymbol{\Sigma}_{XX}$를 갖는다고 가정하자. PCA는 $r$개의 correlated된 투입 변수들 $X_{1},\ldots, X_{r}$로부터 $t (\leq r)$개의 uncorrelated된 linear projection들 $\xi_{1},\ldots ,\xi_{t}$을 얻고자 하는 것이다. 이 때, $\xi_{j}$는
\begin{equation}
\xi_{j}=\mathbf{b}_{j}^{T}\mathbf{X}=b_{j1}X_{1}+\ldots + b_{jr}X_{r},\qquad{j=1,\ldots , t}
(\#eq:PCAlinearprojection)
\end{equation}
이다. 그리고 $\xi$들을 정보를 최소화하는 방향으로 찾고자 한다. 이 식을 $\mathbf{X}$의 **처음 $t$ principal components**라고 알려져 있다. PCA에서 정보는 original input varible들의 total variation으로 설명할 수 있다.
$$\sum_{j=1}^{r}\text{var}(X_{j})=\text{tr}(\boldsymbol{\Sigma}_{XX}).$$
스펙트럼 분해 정리(1장 참조)에 의해 우리는 다음과 같이 쓸 수 있다.
$$\boldsymbol{\Sigma}_{XX}=\mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{T}, \qquad{\mathbf{U}^{T}\mathbf{U}=\mathbf{I}_{r}.}$$
이 때 대각행렬 $\boldsymbol{\Lambda}$는 $\boldsymbol{\Sigma}$의 고유값들 $\{\lambda_{j}\}$을 대각원소들로 갖는다. 그리고 $\mathbf{U}$의 열들이 $\boldsymbol{\Sigma}_{XX}$의 고유치(eigenvector)가 된다. 그래서 총 variation은 $\text{tr}(\boldsymbol{\Sigma}_{XX})=\text{tr}(\boldsymbol{\Lambda})=\sum_{j=1}^{r}\lambda_{j}$가 된다.

$j$번째 계수 벡터 $\mathbf{b}_{j}=(b_{1j},\ldots, b_{rj})^{T}$들은 다음과 같이 고른다.

- $\mathbf{X}$의 처음 $t$ 선형 사영 $\xi_{j}, j=1,\ldots, t$들은 그들의 분산 $\{ \text{var}\{\xi_{j} \} \}$들을 이용해 $\text{var}\{\xi_{1} \} \geq \text{var}\{\xi_{2} \} \ldots \geq \text{var}\{\xi_{t} \} $로 순위를 매겨 결정한다.

- $k<j$일 경우 $\xi_{j}$는 $\xi_{k}$와 uncorrelated이다.

PCA의 집합들을 유도하는 방법은 두 가지가 있다.

1. **Least-squares optimality criterion**을 활용한다.

2. Variance-maximizing technique을 활용한다.

### 최소제곱법으로 구하는 PCA

$\mathbf{B}=(\mathbf{b}_{1},\ldots, \mathbf{b}_{t})^{T}$를 가중치의 $(t\times r)$ $(t\leq r)$ 행렬이라고 하자. 선형 사영식 \@ref(eq:PCAlinearprojection)은 다음과 같이 $t$-벡터
$$\boldsymbol{\xi}=\mathbf{BX}$$
로 쓸 수 있다. 여기서 $\boldsymbol{\xi}=(\xi_{1},\ldots , \xi_{t})^{T}$이다. 우리는 $\mathbf{X}\approx\boldsymbol{\mu}+\mathbf{A}\boldsymbol{\xi}$를 least-square 관점에서 만족시키는 $r$-벡터 $\boldsymbol{\mu}$와 $(r\times t)$ 행렬 $\mathbf{A}$를 찾고자 한다. 우리는 다음과 같은 최소자승 오차 조건
\begin{equation}
E\{ (\mathbf{X}-\boldsymbol{\mu}-\mathbf{A}\boldsymbol{\xi})^{T}(\mathbf{X}-\boldsymbol{\mu}-\mathbf{A}\boldsymbol{\xi}) \}
(\#eq:PCAcriterion)
\end{equation}
을 선형 사영 $\boldsymbol{\xi}$가 $\mathbf{X}$를 얼마나 잘 재구성하는지에 대한 측도로 사용한다.

앞선 식 \@ref(eq:PCAcriterion)을 $\boldsymbol{\xi}$를 $\mathbf{BX}$로 바꿔 씀으로써 좀더 명백한 방법으로 표현할 수 있다. 그러면 평가기준은 $(r\times t)$ 행렬 $\mathbf{A}$와 $(t\times r)$ 행렬 $\mathbf{B}$ (둘 다 full rank $t$를 갖는다), $r$-벡터 $\boldsymbol{\mu}$로 표현할 수 있다.
\begin{equation}
E\{ (\mathbf{X}-\boldsymbol{\mu}-\mathbf{ABX})^{T}(\mathbf{X}-\boldsymbol{\mu}-\mathbf{ABX}) \}
(\#eq:PCAcriterion2)
\end{equation}

## 주성분 분석의 결과 정리(PCA summary)

박창이 교수님 외 R을 이용한 데이터마이닝 책을 참고하였다. 주성분 분석의 간단한 결과 요약은 다음과 같다.

1. 주성분은 원 변수들의 공분산행렬 또는 상관계수행렬의 고유벡터로부터 구해진다.

2. 임의의 두 주성분간의 공분산은 항상 0이므로 주성분들 간에는 상관성이 존재하지 않는다.

3. 원 자료의 변수들의 분산의 합은 주성분변수들의 분산의 합과 항상 일치한다.

4. 주성분의 각 적재계수(loading coefficient)는 특정 변수가 주성분변수에 기여하는 정도를 나타낸다.

## 주성분 회귀분석(principal component regression)

[@Jolliffe2002]의 내용을 참고한다. 다음과 같은 standard 회귀모형이 있다고 하자. 즉 모형은 다음과 같다.

\begin{equation}
\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}
(\#eq:PCreg01)
\end{equation}

이 때 $\mathbf{y}$는 dependent variable의 평균에서 잰 $n$개 관찰값들의 벡터이고, $\mathbf{X}$는 $n\times p$ predictor matrix, $\boldsymbol{\beta}$는 $p$개 회귀분석 계수들의 벡터, $\boldsymbol{\epsilon}$는 오차항들의 벡터이다. 이 때 $\boldsymbol{\epsilon}$의 원소들은 각각 독립이고, 분산은 $\sigma^{2}$이다.

참고로 주성분 회귀분석에서는 자료들을 센터링(centering)하는 것이 좋다.

```{r, echo=F, fig.cap='주성분분석에서 센터링을 해야 하는 이유.', fig.align='center'}
knitr::include_graphics("images/pca_pcreg.png")
```

## 다변량 자료에서의 PCA (PCA for multivariate data)

[@Ramsay2005]의 8장을 참고하였다. 다변량 자료에서의 기초 컨셉은 다음과 같은 설명변수들의 선형 결합을 취하는 것이다.
$$f_{i}=\sum_{j=1}^{p}\beta_{j}x_{ij}, i=1,\ldots, N.$$
여기서 $\beta_{j}$는 $j$번째 변수에 대한 $x_{ij}$ 관련 가중 계수(weighting coefficient)이다.

## 함수자료에서의 PCA (PCA for functional data)

