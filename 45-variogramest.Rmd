# 변동도의 추정 {#variogramest}

이 문서에서는 variogram을 어떻게 추정할 것인지에 대해 다룰 것이다. 크게 두 가지가 있다.

- Empirical model (nonparametric)

- Parametric fit

그리고 [@Gelfand2010]의 33쪽부터, [@Cressie1993]의 69쪽부터 참고했다.

## 경험변동도(empirical variogram)

이것은 변동도를 비모수 추정하는 것이다. 다시 한 번 변동도의 정의를 살펴보면
$$2\gamma(\mathbf{h})=\text{Var}(Z(\mathbf{s}+\mathbf{h})-Z(\mathbf{s}))$$
으로 lag $\mathbf(h)$에만 의존하는 함수이다. 그런데 내재정상성(instinsic stationary)에서는 평균이 0이므로 
$$2\gamma(\mathbf{h})=E[Z(\mathbf{s}+\mathbf{h})-Z(\mathbf{s}))^{2}]$$
이 된다.

다음은 추정량을 구하기 위한 방법들이다.

1. 적률 추정(Metohd of moment (MoM) estimation (Matheron, 1962))

$E(Z(\mathbf{s}))=\mu$라는 상수 평균(constant mean) 가정하에 적률추정량(MoM estimator)은
$$2\hat{\gamma}(\mathbf{h})=\frac{1}{|N(\mathbf{h})|}\sum_{(s_{i},s_{j})\in N(\mathbf{h})}(Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j}))^{2}, \mathbf{h}\in \mathbb{R}^{d}$$
이다. 여기서 $N(\mathbf{h})$는 거리가 $\mathbf{h}$가 되는 $(\mathbf{s}_{i},\mathbf{s}_{j})$들의 집합이다. 즉
$$N(\mathbf{h})=\{ (\mathbf{s}_{i},\mathbf{s}_{j}), \mathbf{s}_{i}-\mathbf{s}_{j}=\mathbf{h} \}$$
이다. $N(\mathbf{h}) \neq N(\mathbf{-h})$임에 주의하자. 이 추정량의 문제는 **정규 격자(regular grid)** 자료에만 잘 적용된다는 점이다. Irregular한 자료에서는 $\mathbf{h}$에 대응되는 $N(\mathbf{h})$가 공집합(empty set)일 수도 있다. 그런 상황을 해결하기 위해 $\mathbf{h}$의 적당한 근방(neighborhood) $T(\mathbf{h})$을 생각하여 $N(\mathbf{h})$를 정의하기도 한다.
$$N(\mathbf{h})=\{ (\mathbf{s}_{i},\mathbf{s}_{j}), \mathbf{s}_{i}-\mathbf{s}_{j}=T(\mathbf{h}) \} .$$

그렇다면 이 근방의 size는 어떻게 정해야 할 것인가라는 질문이 생길 수도 있다. 이것은 **띠너비 선택(bandwidth selection)** 문제와 유사하다. Practically하게 [@Journel2003]는 $| \cup \{N(\mathbf{h}): \mathbf{h} \in T(\mathbf{h}) \} |$에 들어가는 distinct pair들이 적어도 30개 이상이 되도록 잡는 것이 좋다고 하였다. 

그러나 이 경우도 역시 데이터의 사이즈가 적을 경우 문제가 된다. 또한 $\mathbf{h}$의 방향도 고려할 경우 자료가 더 부족해지고, $\mathbf{h}$에 따라 pair 갯수 또한 차이가 난다. 그리고 자료로 인해 관측할 수 있는 $\mathbf{h}$의 minimum과 maximum length가 존재한다. 역시 practically하게 $\mathbf{h}$는 observation location들의 maximum length의 절반 정도를 고르도록 권장하고 있다.

마지막으로 이 추정량의 성질에 대해 알아보자. 우선 unbiased하다(특히 grid 자료인 경우). 그러나 outlier에 로버스트하지는 않다. $Z(\mathbf{s})$가 Gaussian distribution이면
$$(Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j}))^{2} \sim 2 \gamma(\mathbf{h})\chi_{1}^{2}$$
이다. 그런데 카이제곱 분포는 매우 skewed된 분포인데 이 분포를 sample mean을 이용해 추정했으므로 finite sample 이용시 variation이 클 수 있다.

2. 로버스트 추정량(robust estimator (Cressie and Howkins, 1980))

이 문제를 해결하기 위해 Cressie와 Howkins는 robust한 통계량을 제시하였다.
$$2 \bar{\gamma}(\mathbf{h})=\frac{1}{0.457+\frac{0.494}{|N(\mathbf{h})|}}\{\frac{1}{|N(\mathbf{h})|}\sum_{(\mathbf{s}_{i},\mathbf{s}_{j} \in N(\mathbf{h}))} |Z(\mathbf{s}_{i}-Z(\mathbf{s}_{j})|^{\frac{1}{2}}\}^{4}.$$
앞의 $\frac{1}{0.457+\frac{0.494}{|N(\mathbf{h})|}}$는 bias correction term이다.

이 추정량의 아이디어는 다음과 같다. 어떤 확률변수 $X \sim \chi_{1}^{2}$때 $X^{\frac{1}{4}}$는 거의 symmetric임을 보일 수 있다고 한다. 즉 $|Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})|^{2}$보다는 $|Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})|^{\frac{1}{2}}$이 더 symmetric하게 행동할 수 있을 것이다. 따라서 이것을 이용하고자 하는 것이다. $\mathbf{X}_{n}$을  $X_{n}\equiv\frac{1}{|N(\mathbf{h})|}\sum_{(\mathbf{s}_{i},\mathbf{s}_{j} \in N(\mathbf{h}))} |Z(\mathbf{s}_{i}-Z(\mathbf{s}_{j})|^{\frac{1}{2}}$ 이라고 하자.

다음은 $X \sim \chi_{1}^{2}$시 몇 가지 계산 결과이다.
$$E(X^{\frac{1}{4}})=0.82216, \text{Var}(X^{\frac{1}{4}})=0.12192, E(X_{n})=0.82216 \equiv \nu$$
$$\text{Var}(X_{n})=\frac{0.12192}{|N(\mathbf{h}|)} \text{(cross-covariance 무시할 경우)} .$$
그 다음 $f(x)=x^{4}$에 대해 $\nu$ 근방에서 테일러 전개를 해보자. 그러면
$$f(X_{n})\circeq f(\nu) +f'(\nu)(X_{n}-\nu)+\frac{1}{2}f''(\nu)(X_{n}-\nu)^{2} .$$
여기서 $X_{n}$만 random이다. 기댓값을 취하면
$$
\begin{aligned}
E(X_{n})^{4}&\circeq f(\nu) + f'(\nu)E(X_{n}-\nu) +\frac{1}{2}f''(\nu)E(X_{n}-\nu)^{2}\\
&\circeq 0.457 + 0 +\frac{0.494}{|N(\mathbf{h})|} \text{(second order까지 bias correction)}\\
\end{aligned}
$$
따라서 robust estimator 앞에 bias correction을 위한 숫자항이 붙는 것이다.

3. 또 다른 로버스트 추정량(another robust estimator)

특별한 이름은 없으며 앞 estimator에서 약간 변형한 형태이다.
$$
\begin{aligned}
2\tilde{\gamma}(\mathbf{h})&=\frac{1}{0.457}\text{Median}\{ |Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})|^{2} , (\mathbf{s}_{i},\mathbf{s}_{j})\in N(\mathbf{h}) \}\\
&=\frac{1}{0.457}\{\text{Median} \{ |Z(\mathbf{s}_{i})-Z(\mathbf{s}_{j})|^{\frac{1}{2}} \}^{4} \}
\end{aligned}
$$
Median을 쓸 경우 제곱근을 한 다음 네제곱을 하거나 그냥 제곱을 하거나 차이가 없다고 한다.

## 경험변동도를 이용한 모수적 모형 추정(fitting parametric models to empirical variogram)

경험변동도(empirical variogram) 자료들을 가지고 왜 또 모수적(parametric)인 적합(fitting)을 하려고 할까? 지구통계학(geostatistics)에서는 종속구조(dependence structure) $\boldsymbol{\sigma}$를 이용한 **예측(prediction)**에 관심이 있다. 그런데 예측을 하려면 $\boldsymbol{\sigma}^{-1}$이 필요하다. 그런데 경험변동도로 하면 $\boldsymbol{\sigma}$가 비음정치(non-negative definite)가 아니거나 수치적 특이성(numerical singularity)이 생긴다. 일반적으로 다음과 같은 모수 모델
$$\hat{\boldsymbol{\gamma}}(h;\hat{\boldsymbol{\theta}})$$
는 양정치함수(positive definite function)임을 보장한다(물론 numerical singular한 경우도 있을 수는 있다). 이러한 이유로 공간통계학에서는 모수를 이용한 모델링을 선호하는 것이다.

다음과 같이 $\hat{\gamma}(\mathbf{h}_{1}), \cdots , \hat{\gamma}(h_{m})$이 available하다고 하자(이들을 새로운 자료로 생각해도 좋다). 여기서 $m$은 고정시킨다. 우리의 목표는 $\boldsymbol{\gamma}(h;\boldsymbol{\theta})$가 true model일 때 $\hat{\boldsymbol{\gamma}}(h;\hat{\boldsymbol{\theta}})$를 만들고자 한다. 추정 방법은 크게 세 가지가 있다.

### LS method

$$
\begin{aligned}
\hat{\boldsymbol{\theta}}_{LS}&=\text{argmin}_{\boldsymbol{\theta}}\sum_{j=1}^{m}\{ \hat{\gamma}(h_{j})-\gamma(h_{j};\boldsymbol{\theta})\}^{2}\\
&=\text{argmin}_{\boldsymbol{\theta}}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))^{T}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))\\
\end{aligned}
$$
이 방법은 $\hat{\gamma}(\mathbf{h}_{i})$들의 종속성(dependency)을 무시한다.

### GLS method

최소자승법의 약점을 보완하기 위한 방법이다.
$$\hat{\boldsymbol{\theta}}_{GLS}=\text{argmin}_{\boldsymbol{\theta}}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))^{T}V^{-1}(\boldsymbol{\theta})(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))$$
여기서 $V^{-1}(\boldsymbol{\theta})=\text{Var}(\hat{\boldsymbol{\gamma}})$이다. 일반적인 GLS는 $(\hat{y}-X\beta)^{T}V^{-1}(\theta)(\hat{y}-X\beta)$꼴처럼 $\beta$와 $\theta$가 다르지만, 이 경우는 두 개가 $\theta$로 같다. 

### WLS method

$$\hat{\boldsymbol{\theta}}_{GLS}=\text{argmin}_{\boldsymbol{\theta}}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))^{T}W(\boldsymbol{\theta})(\boldsymbol{\theta})(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))$$
여기서 $W(\boldsymbol{\theta})=\text{diag}(V(\boldsymbol{\theta}))$이다. 이것은 highly nonlinear한 object function이라 $\boldsymbol{\theta}$가 많을수록 적합이 어려워진다.

GLS처럼 $\beta$와 $\theta$가 같으므로 two-stage iteration을 통해 적합한다. Iteration procedure는 다음과 같다.
$$\hat{\boldsymbol{\theta}}^{(k+1)}=\text{argmin}_{\boldsymbol{\theta}}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))^{T}V^{-1}(\hat{\boldsymbol{\theta}}^{(k)})(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))$$
OLS로 $\boldsymbol{\gamma}(\boldsymbol{\theta})$ 부분을 먼저 고정시키고 iteration을 돌린다는 것 같다.

### 근사 WLS (approximated WLS)

[@Cressie1985]는 WLS의 계산의 어려움을 피하기 위해 다음과 같은 근사 WLS 방법을 쓰기도 한다.
$$\hat{\boldsymbol{\theta}}=\text{argmin}_{\boldsymbol{\theta}}\sum_{j}\frac{|N(\mathbf{h}_{j})}{\gamma^{2}(\mathbf{h}_{j};\boldsymbol{\theta})}\{ \hat{\gamma}(\mathbf{h}_{j})-\gamma(\mathbf{h}_{j};\boldsymbol{\theta}) \}^{2}$$
이 근사가 가능한 이유는 $\text{Var}(\hat{\gamma(\mathbf{h}_{j})}) \cong \frac{8 \gamma^{2}(\mathbf{h}_{j}l\boldsymbol{\theta})}{|N(\mathbf{h}_{j})|}$이기 때문이다(8은 상수라 무시해도 수렴함). 그러나 여전히 두 군데에 $\boldsymbol{\theta}$가 있어 two-stage iteration을 해야 한다.

이렇게 계산한 AWLS 추정량이 (어떤 조건 하에) asymptotic normal이 됨을 보일 수 있다고 한다. 그런데 이런 경우에도 $\hat{\gamma}(\mathbf{h}_{j}) \stackrel{n \rightarrow}{\rightarrow} \gamma(\mathbf{h};\boldsymbol{\theta})$를 따로 보여야 한다고 한다.

몇 가지 기본 가정들은 다음과 같다.

- $\mathbf{\gamma}(\mathbf{h}_{1}), \cdots , \mathbf{\gamma}(\mathbf{h}_{m})$ 이 $\mathbf{h}_{1}, \cdots , \mathbf{h}_{m}$에서 계산되어있다. 여기서 $m$은 고정되어 있다고 가정한다.

- $|N(\mathbf{h}_{j})|=O(n)$. 좀 더 자세하게는 $N(\mathbf{h}_{j})=n\cdot \phi_{j,n}$이고 $\lim_{n \rightarrow \infty}\phi_{j,n}=\phi_{j} < \infty$이다(그렇지 않으면 $O(n)$의 order가 올라갈 것이다). 말로 설명하자면 데이터가 커질 때마다 밀도가 일정해야 하므로 region도 커져야 한다는 것이다.

참고로 [@Cressie1985]에서는 다음과 같이 적혀 있다. "$|N(\mathbf{h}_{j})| \rightarrow \infty$ for each $j=1, \cdots, k$ as $N \rightarrow \infty$ as $|D| \rightarrow \infty$ such that $N/|D|$, the sampling rate per unit area is constant." 그리고 data가 evenly spaced 되어야 이 방법이 잘 맞는다고 한다.

- $\text{Cov}(\hat{\gamma}(\mathbf{h}_{j}), \hat{\gamma}(\mathbf{h}_{k}))=O(\frac{1}{n})$. 여기서 $\text{Cov}(\hat{\gamma}(\mathbf{h}_{j}), \hat{\gamma}(\mathbf{h}_{k})) \sim \frac{U_{jk}(\boldsymbol{\theta})}{n}$을 만족하는 $m \times m$  행렬 $U(\boldsymbol{\theta})$가 존재한다. Decay 속도가 충분히 빠른 variogram model들 (ex. exponential)이 이 조건을 만족한다고 한다.

이 세 가지 조건을 만족할 경우
$$\sqrt{n}(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))  \stackrel{\mathcal{D}}{\rightarrow} \mathcal{N}(\mathbf{0}, U(\boldsymbol{\theta}))$$
$m$이 커지면 $(\hat{\boldsymbol{\gamma}}-\boldsymbol{\gamma}(\boldsymbol{\theta}))$의 dimension 또한 커져 복잡한 문제가 되므로 $m$을 고정하는 것이다.
$$\sqrt{n}(\hat{\boldsymbol{\theta}}-\boldsymbol{\theta})  \stackrel{\mathcal{D}}{\rightarrow} \mathcal{N}(\mathbf{0}, H^{-1}R^{T}URH^{-1})$$
여기서 $R$은 $m \times p$ (p: number of parameter) 행렬인데
$$R_{ij}=\frac{2\phi_{i}}{\gamma^{2}(\mathbf{h}_{i};\boldsymbol{\theta})}\cdot \frac{2\gamma}{2\boldsymbol{\theta}_{j}}(\mathbf{h}_{j};\boldsymbol{\theta})$$
가 성립한다.

한편 $H$는 목적함수 $S_{n}(\boldsymbol{\theta}*)$의 헤시안의 확률수렴 값이다.
$$H:\frac{\nabla^{2}S_{n}(\boldsymbol{\theta}*)}{n} \stackrel{P}{\rightarrow} H(\boldsymbol{\theta})$$
이다. 

참고로 목적함수의 asymptotic은 보통 다음과 같이 한다. Consistency는 따로 보이고 이것을 만족하면 그 다음 테일러 전개로
$$S_{n}'(\boldsymbol{\theta})=S_{n}'(\hat{\boldsymbol{\theta}})+S_{n}''(\boldsymbol{\theta}*)(\hat{\boldsymbol{\theta}}-\boldsymbol{\theta})$$
를 만든다 (mean value theorem에 의해 등호 성립). 이때 $S_{n}'(\hat{\boldsymbol{\theta}})$은 minimization 문제이므로 0이 된다. $S_{n}''(\boldsymbol{\theta}*)$는 $H$에 해당된다. 따라서 $S_{n}'(\boldsymbol{\theta})$의 분포만 알아내면 되는 것이다. 이러한 방법을 "Sandwich method"라고 한다. 자세한 내용은  [(Fuentes, 2007)](http://www.stat.ncsu.edu/people/fuentes/courses/st790m/) 강좌의 4장을 보면 된다.

## R 예제(R-variogramest)

이제 실제 R 예제에 대해 살펴보자.

```{r, message=F, echo=F}
library(geoR)
```

```{r, fig.align='center', comment=">", fig.cap = 'Various variogram estimation methods.'}
data(s100)
s100.v1 <- variog(s100, option="cloud")
s100.v2 <- variog(s100, max.dist=1, estimator.type="classical")
s100.v3 <- variog(s100, max.dist=1, estimator.type="modulus")

par(mfrow=c(2,2))
plot(s100$coords[,1], s100$coords[,2],
     pch="x", xlab="", ylab="", main="locations")
plot(s100.v1, main="Variogram cloud")
plot(s100.v2, main="MoM estimator")
plot(s100.v3, main="Robust estimator")

true <- 1-matern(seq(0,1,0.1),0.3,0.5)
ols <- variofit(s100.v2, ini=c(0.9,0.2), cov.model="mat",
                fix.kappa=F, kap=1.5, nug=0.2, weights="equal")
wls <- variofit(s100.v2, ini=c(0.9,0.2), cov.model="mat",
                fix.kappa=F, kap=1.5, nug=0.2, weights="cressie")
```

```{r, fig.align='center', comment=">", fig.cap = 'Comparison of variogram estimation methods.'}
par(mfrow=c(1,1))
plot(s100.v2, main="",col="blue", lwd=2)
lines(seq(0,1,0.1), true, col="red")
lines(ols, lwd=1.5)
lines(wls, lty=2, lwd=1.5)
legend("bottomright", c("true", "ols", "wls"),
       col=c("red", "black", "black"), lty=c(1,1))
```
