# 벌점화 및 축소방법 {#penalshrink}

때때로 설명변수들이 너무 많아져 특이 관찰값보다도 많아지는 경우가 존재한다. 이런 경우에는 전체 모형을 적합할 경우 prediction interval이 커지고 최소제곱 추정량이 유일하지 않을 수도 있는 등 문제점들이 생기게 된다. 이런 문제점들을 해결하는 방법들을 **벌점화(penalization)**라고 한다.

최소제곱 추정량은 $(X^{T}X)^{-1}$에 depend하므로 만약 $(X^{T}X)^{-1}$이 비정칙(singular)이거나 거의 비정칙일 때 $\beta_{LS}$를 계산하는 데 어려움을 겪을 수 있다. 이런 경우에 $X$에 약간의 변화만 있어도 $(X^{T}X)^{-1}$은 크게 달라지게 된다. 이 때 최소제곱 추정량 $\hat{beta}_{LS}$는 training data에 적합할 때에는 큰 문제가 되지 않으나 test data에 적합할 때에는 문제가 생길 수도 있다.

## 능형회귀(ridge regression)

앞서 말한 문제점의 한 가지 해결방법으로 추정량의 불편성을 약간 깨는 방법이 있다. [@Hoerl1970]은 $X^{T}X$에 작은 상수값 $\lambda$를 더해 역을 취해 추정량의 불안정성을 해결할 수 있다고 생각하고 다음과 같은 추정량을 제안하였다.
$$\hat{\beta}_{ridge}=(X^{T}X+\lambda I_{p})^{-1}X^{T}Y.$$
이를 **능형회귀(ridge regression)** 추정량이라고 부른다. 참고로 수학에서는 이 방법을 [Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)이라고 부른다.

$\hat{\beta}_{ridge}$는 다음 벌점화제곱합
$$\hat{\beta}^{rodge}=\text{arg}\min_{\beta}\{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}+\lambda\sum_{j=1}^{p}\beta_{j}^{2}\}$$
을 최소화하여 얻을 수 있다. 또 다른 표현으로는
$$\hat{\beta}^{rodge}=\text{arg}\min_{\beta}\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}\text{ for some } t, \sum_{j=1}^{p}\beta_{j}^{2}<t^{2}$$
로도 쓸 수 있다.

벌점화 항을 살펴보면, $\lambda$는 미리 선택된 상수(constant)에 벡터 $\beta$의 제곱합이 곱해진 형태이다. 이는 $\beta_{j}$가 큰 값을 가지면 페널티를 주겠다는 뜻이다. $\beta_{j}$가 0에 가까울수록 벌점화 항은 작을 것이다.

### 능형회귀의 기하학적 해석(geometrical interpretation of ridge regression)

```{r, echo=F, fig.cap='Geometric interpretation of ridge regression.', fig.align='center'}
knitr::include_graphics("images/basic_ridge.png")
```

$p=2$인 경우를 생각해보자. 등고선이 있는 붉은색 타원이 residual sum of squares (RSS)에 해당하며 가장 안쪽에 있는 원이 작은 RSS를 가지고 그 중심에 $\hat{\beta}_{OLS}$가 있다. $p=2$일 때 능형회귀의 제약조건은 원에 대응된다. 능형회귀의 해는 타원과 원이 만나는 지점이 될 것이다.

여기서 penalty term과 RSS 사이에는 trade-off가 있다고 한다. 

### 능형회귀추정량의 성질들(properties of ridge estimator)

### SVD를 이용한 능형회귀 해의 계산(computing the ridge solutions via the SVD)

능형회귀의 해

$$\hat{\beta}_{ridge}=(X^{T}X+\lambda I_{p})^{-1}X^{T}Y$$

를 다시 상기해보자. $\hat{\beta}_{ridge}$를 구할 때 피하고 싶은 역행렬의 계산이 있다. 수치적으로 불안정할 뿐더러 계산량이 대략 $\mathcal{O}(p^{3})$ 정도 된다. 역행렬의 계산을 피할 수 있는 방법으로 SVD를 이용하는 방법이 있다. 즉

$$X=UDV^{T}$$

임을 이용해 해를 계산하는 것이다.

## 라쏘(LASSO)

능형회귀는 축소 추정치를 주지만 변수선택은 하지 않는다. 따라서 고차원 자료의 경우 최종 모형에 대한 해석이 그리 용이하지 않다.
