# 주성분분석 {#PCA}

**주성분분석(principal component analysis, PCA)**란 **차원 축소(dimension reduction)**을 위해 많이 쓰이는 툴이다. 이 분야의 유명한 참고문헌으로는 [@Jolliffe2002]가 있다. 주성분분석은 차원 축소방법 중 변수를 직접적으로 변환하는 변수변환(feature transformation) 방법이다. 기상학에서는 empirical orthogonal functions라고 부른다.

여러 다변량 데이터셋이 갖는 문제들 중 하나는 변수가 너무 많다는 것이다. 너무 많은 변수들이 가질 수 있는 문제로는 차원의 저주(curse of dimensionality)가 알려져 있다.

## 차원의 저주(curse of dimensionality)

이러한 문제로부터 데이터셋에 존재하는 원래의 변동(variation)을 가능한 한 많이 설명하면서 다변량 데이터의 차원을 줄이는 것을 주된 목적으로 하는 다변량 기법인 주성분분석이 만들어졌다. 이러한 목적은 원래 변수(original variable)들의 선형결합(linear combination) 형태로 새로운 변수 집합인 **주성분(principal component)**로 변환함으로써 얻을 수 있다. 주성분들은 서로 상관되지 않으며 순서화되어 처음 몇 개의 주성분이 원래 변수들의 변동(variation)을 대부분 설명하도록 한다. 주성분분석의 결과는 많은 원래 변수들의 대용(surrogate)으로서 사용될 수 있는 작은 개수의 새로운 변수를 만드는 것이다.

## 주성분분석의 기본 목적(basic object of PCA)

다음과 같이 $\mathbf{x}_{i}\in\mathbb{R}^{d}, i=1,\ldots, n$이라는 training pattern이 있다고 하자. 주성분은 $q <d$의 직교정규 벡터의 집합이며 첫번째 주성분은 데이터를 한 개의 축으로 사상(projection)시켰을 때 그 분산이 가장 커지는 축이다.

$\mathbf{y}$를 subspace로의 projection이라고 하자. $\mathbf{W}$를 column에 principal component를 포함하는 $d\times q$ 행렬이라고 하자. 그러면
$$\mathbf{y}=\mathbf{W}^{T}\mathbf{x}$$
가 된다. 즉 $\mathbf{y}$는 $\mathbf{x}$의 dimension-reduced 표현이 되는 것이다. $\hat{\mathbf{x}}$를 $\mathbf{y}$가 주어졌을 때 $\mathbf{x}$의 reconstruction이라고 하자.
$$\hat{\mathbf{x}}=\mathbf{W}^{T}\mathbf{y}.$$
그리면 PCA의 목표는 다음과 같은
$$E_{rec}=\frac{1}{n}\sum_{i=1}^{n}\| \mathbf{x}_{i}-\hat{\mathbf{x}}_{i}\|^{2}$$
$E_{rec}$을 minimize하도록 subspace를 set하는 것이다.

## 모집단 주성분들(population principal components)

[@Izenman2009]를 참고하였다.

다음과 같은 무작위 $r$-벡터
$$\mathbf{X}=(X_{1},\ldots, X_{t})^{T}$$
가 평균이 $\boldsymbol{\mu}_{X}$이고 $(r\times r)$ 공분산행렬 $\boldsymbol{\Sigma}_{XX}$를 갖는다고 가정하자. PCA는 $r$개의 correlated된 투입 변수들 $X_{1},\ldots, X_{r}$로부터 $t (\leq r)$개의 uncorrelated된 linear projection들 $\xi_{1},\ldots ,\xi_{t}$을 얻고자 하는 것이다. 이 때, $\xi_{j}$는
\begin{equation}
\xi_{j}=\mathbf{b}_{j}^{T}\mathbf{X}=b_{j1}X_{1}+\ldots + b_{jr}X_{r},\qquad{j=1,\ldots , t}
(\#eq:PCAlinearprojection)
\end{equation}
이다. 그리고 $\xi$들을 정보를 최소화하는 방향으로 찾고자 한다. 이 식을 $\mathbf{X}$의 **처음 $t$ principal components**라고 알려져 있다. PCA에서 정보는 original input varible들의 total variation으로 설명할 수 있다.
$$\sum_{j=1}^{r}\text{var}(X_{j})=\text{tr}(\boldsymbol{\Sigma}_{XX}).$$
스펙트럼 분해 정리(1장 참조)에 의해 우리는 다음과 같이 쓸 수 있다.
$$\boldsymbol{\Sigma}_{XX}=\mathbf{U}\boldsymbol{\Lambda}\mathbf{U}^{T}, \qquad{\mathbf{U}^{T}\mathbf{U}=\mathbf{I}_{r}.}$$
이 때 대각행렬 $\boldsymbol{\Lambda}$는 $\boldsymbol{\Sigma}$의 고유값들 $\{\lambda_{j}\}$을 대각원소들로 갖는다. 그리고 $\mathbf{U}$의 열들이 $\boldsymbol{\Sigma}_{XX}$의 고유치(eigenvector)가 된다. 그래서 총 variation은 $\text{tr}(\boldsymbol{\Sigma}_{XX})=\text{tr}(\boldsymbol{\Lambda})=\sum_{j=1}^{r}\lambda_{j}$가 된다.

$j$번째 계수 벡터 $\mathbf{b}_{j}=(b_{1j},\ldots, b_{rj})^{T}$들은 다음과 같이 고른다.

- $\mathbf{X}$의 처음 $t$ 선형 사영 $\xi_{j}, j=1,\ldots, t$들은 그들의 분산 $\{ \text{var}\{\xi_{j} \} \}$들을 이용해 $\text{var}\{\xi_{1} \} \geq \text{var}\{\xi_{2} \} \ldots \geq \text{var}\{\xi_{t} \} $로 순위를 매겨 결정한다.

- $k<j$일 경우 $\xi_{j}$는 $\xi_{k}$와 uncorrelated이다.

PCA의 집합들을 유도하는 방법은 두 가지가 있다.

1. **Least-squares optimality criterion**을 활용한다.

2. Variance-maximizing technique을 활용한다.

### 최소제곱법으로 구하는 PCA

$\mathbf{B}=(\mathbf{b}_{1},\ldots, \mathbf{b}_{t})^{T}$를 가중치의 $(t\times r)$ $(t\leq r)$ 행렬이라고 하자. 선형 사영식 \@ref(eq:PCAlinearprojection)은 다음과 같이 $t$-벡터
$$\boldsymbol{\xi}=\mathbf{BX}$$
로 쓸 수 있다. 여기서 $\boldsymbol{\xi}=(\xi_{1},\ldots , \xi_{t})^{T}$이다. 우리는 $\mathbf{X}\approx\boldsymbol{\mu}+\mathbf{A}\boldsymbol{\xi}$를 least-square 관점에서 만족시키는 $r$-벡터 $\boldsymbol{\mu}$와 $(r\times t)$ 행렬 $\mathbf{A}$를 찾고자 한다. 우리는 다음과 같은 최소자승 오차 조건
\begin{equation}
E\{ (\mathbf{X}-\boldsymbol{\mu}-\mathbf{A}\boldsymbol{\xi})^{T}(\mathbf{X}-\boldsymbol{\mu}-\mathbf{A}\boldsymbol{\xi}) \}
(\#eq:PCAcriterion)
\end{equation}
을 선형 사영 $\boldsymbol{\xi}$가 $\mathbf{X}$를 얼마나 잘 재구성하는지에 대한 측도로 사용한다.

앞선 식 \@ref(eq:PCAcriterion)을 $\boldsymbol{\xi}$를 $\mathbf{BX}$로 바꿔 씀으로써 좀더 명백한 방법으로 표현할 수 있다. 그러면 평가기준은 $(r\times t)$ 행렬 $\mathbf{A}$와 $(t\times r)$ 행렬 $\mathbf{B}$ (둘 다 full rank $t$를 갖는다), $r$-벡터 $\boldsymbol{\mu}$로 표현할 수 있다.
\begin{equation}
E\{ (\mathbf{X}-\boldsymbol{\mu}-\mathbf{ABX})^{T}(\mathbf{X}-\boldsymbol{\mu}-\mathbf{ABX}) \}
(\#eq:PCAcriterion2)
\end{equation}

만약 $t=1$일 경우 식 \@ref(eq:PCAcriterion2)는 최소자승 문제로 쓸 수 있다.
$$\min_{\boldsymbol{\mu},\mathbf{A},\mathbf{B}}E\sum_{j=1}^{r}(X_{j}-\mu_{j}-a_{j1}\mathbf{b}_{1}^{T}\mathbf{X})^{2},$$
여기서 $\boldsymbol{\mu}=(\mu_{1},\ldots, \mu_{r})^{T}$, $\mathbf{A}=\mathbf{a}_{1}=(a_{11},\ldots, a_{r}1)^{T}$, $\mathbf{B}=\mathbf{b}_{1}^{T}$이다.

(최소화하는 방법 소개)

식 \@ref(eq:PCAcriterion2)는 **reduced-rank rregression solution**으로 최소화할 수 있고, 결과는 다음과 같다.
$$\mathbf{A}^{(t)}=(\mathbf{v}_{1},\ldots, \mathbf{v}_{t})=\mathbf{B}^{(t)T},$$
$$\boldsymbol{\mu}^{(t)}=(\mathbf{I}_{r}-\mathbf{A}^{(t)}\mathbf{B}^{(t)})\boldsymbol{\mu}_{X},$$
여기서 $\mathbf{v}_{j}=\mathbf{v}_{j}(\boldsymbol{\Sigma}_{XX})$는 $j$번째 큰 고유값에 대응되는 고유벡터들이다. 그러면 원래 자료의 가장 최고의 rank-$t$ 근사는 다음과 같다.
$$\hat{\mathbf{X}}^{(t)}=\boldsymbol{\mu}^{(t)}+\mathbf{C}^{(t)}\mathbf{X}=\boldsymbol{\mu}_{X}+\mathbf{C}^{(t)}(\mathbf{X}-\boldsymbol{\mu}),$$
여기서
$$\mathbf{C}^{(t)}=\mathbf{A}^{(t)}\mathbf{B}^{(t)}=\sum_{j=1}^{t}\mathbf{v}_{j}\mathbf{v}_{j}^{T}$$
는 rank $t$ principal component case의 reduced-rank regression coefficient matrix이다. 식 \@ref(eq:PCAcriterion2)의 최소값은 $\sum_{j=t+1}^{r}\lambda_{j}$로 $\boldsymbol{\Sigma}_{XX}$의 가장 작은 $r-t$ 고유값들의 합이 된다.

이러한 결과를 다음과 같이 생각하는 것이 도움이 될 수도 있다. $\mathbf{V}=(\mathbf{v}_{1},\ldots, \mathbf{v}_{r})$은 $(r\times r)$ 행렬로 column들이 $\boldsymbol{\Sigma}_{XX}$의 ordered eigenvector들의 complete set을 만든다고 가정해보자. $\mathbf{X}$의 가장 정밀한 rank-$t$ 최소자승 reconstruction은 두개의 linear maps $L'\circ L$의 합성으로 얻을 수 있다. 첫 번째 맵 $L:\mathbb{R}^{r} \rightarrow \mathbb{R}^{t}$는 $\mathbf{V}$의 첫 $t$ 개의 열들을 $\mathbf{X}$의 $t$ linear projection을 형성하는데 사용한다. 두 번째 맵 $L':\mathbb{R}^{t}\rightarrow \mathbb{R}^{r}$은 같은 $\mathbf{V}$의 첫 $t$ 개의 열들을 $\mathbf{X}$의 선형 재구성을 만드는 데 사용한다.

$\mathbf{X}$의 처음 $t$ 주성분들은 (이것들을 또한 Karhunen-Lo$\grave{e}$ve 변환으로 부르기도 함) $\xi_{1},\ldots, \xi_{t}$의 선형 사영들로 주어지며,
$$\xi_{j}=\mathbf{v}_{k}^{T}\mathbf{X}, \qquad{j=1,\ldots, t}$$
이다. $\xi_{i}$와 $\xi_{j}$ 사이의 공분산은
$$\text{cov}(\xi_{i},\xi_{j})=\text{cov}(\mathbf{v}_{i}^{T}\mathbf{X}, \mathbf{v}_{j}^{T}\mathbf{X})=\mathbf{v}_{i}^{T}\boldsymbol{\Sigma}_{XX}\mathbf{v}_{j}=\lambda_{j}\mathbf{v}_{i}^{T}\mathbf{v}_{j}=\delta_{ij}\lambda_{j}$$
이고 $\delta_{ij}$는 크로네커 델타이다.

## 주성분 분석의 결과 정리(PCA summary)

박창이 교수님 외 R을 이용한 데이터마이닝 책을 참고하였다. 주성분 분석의 간단한 결과 요약은 다음과 같다.

1. 주성분은 원 변수들의 공분산행렬 또는 상관계수행렬의 고유벡터로부터 구해진다.

2. 임의의 두 주성분간의 공분산은 항상 0이므로 주성분들 간에는 상관성이 존재하지 않는다.

3. 원 자료의 변수들의 분산의 합은 주성분변수들의 분산의 합과 항상 일치한다.

4. 주성분의 각 적재계수(loading coefficient)는 특정 변수가 주성분변수에 기여하는 정도를 나타낸다.

## 주성분 회귀분석(principal component regression)

[@Jolliffe2002]의 내용을 참고한다. 다음과 같은 standard 회귀모형이 있다고 하자. 즉 모형은 다음과 같다.

\begin{equation}
\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}
(\#eq:PCreg01)
\end{equation}

이 때 $\mathbf{y}$는 dependent variable의 평균에서 잰 $n$개 관찰값들의 벡터이고, $\mathbf{X}$는 $n\times p$ predictor matrix, $\boldsymbol{\beta}$는 $p$개 회귀분석 계수들의 벡터, $\boldsymbol{\epsilon}$는 오차항들의 벡터이다. 이 때 $\boldsymbol{\epsilon}$의 원소들은 각각 독립이고, 분산은 $\sigma^{2}$이다.

참고로 주성분 회귀분석에서는 자료들을 센터링(centering)하는 것이 좋다.

```{r, echo=F, fig.cap='주성분분석에서 센터링을 해야 하는 이유.', fig.align='center'}
knitr::include_graphics("images/pca_pcreg.png")
```

## 다변량 자료에서의 PCA (PCA for multivariate data)

[@Ramsay2005]의 8장을 참고하였다. 다변량 자료에서의 기초 컨셉은 다음과 같은 설명변수들의 선형 결합을 취하는 것이다.
$$f_{i}=\sum_{j=1}^{p}\beta_{j}x_{ij}, i=1,\ldots, N.$$
여기서 $\beta_{j}$는 $j$번째 변수에 대한 $x_{ij}$ 관련 가중 계수(weighting coefficient)이다.

## 함수자료에서의 PCA (PCA for functional data)

[@Izenman2009]에 있는 간단한 설명이다. 어떤 상황들에서는 우리는 함수나 또는 curve로 구성된 자료를 분석하려고 한다. 이러한 함수 자료는 종종 시간-의존적이지만 시간이 특별한 역할을 하지 않는다고 놓는다. 실제로, 다르고 독립적으로 얻어진 함수자료들은 다른 시간 point에서 기록될 것이므로 자료 자체가 equally spaced되지 않았을 가능성이 높다. 이런 경우에는 이산 점들에서 얻어진 자료들을 개별적인 함수 관찰값로 보는 것이 좋다.

함수 PCA에서는 각각의 관찰된 함수들이 **같은 individual들의 반복된 관찰값**이라고 본다. 그리고 우리는 그들을 가지고 **함수의 main feature를 묘사**하고자 한다. 이것을 할 수 있는 한 가지 방법은 functional PCA를 이용하는 것이다. 여기서는 각각의 값 대신 커브를 생각하기 때문에 벡터로 표시한 관찰값들 $\mathbf{X}_{1},\ldots , \mathbf{X}_{n}$이 일변량 함수들 $X_{1}(t),\ldots, X_{n}(t)$로 대체된다. 여기서 $t$는 일반적으로 시간을 의미하나 닫힌 구간 $[0,T]$에서 연속인 어떤 값들로도 바뀔 수 있다.

함수적 PCA에서는 각 표본 커브들이 부드러운 평균함수 $E\{X(t)\}=\mu(t)$ 그리고 공분산 함수
$$\text{cov}\{X(s),X(t)\}=\sigma(s,t)$$
를 갖는 **일변량 확률과정 $X(t)$의 독립적인 실현**이라고 간주한다. 이것은 $X(t)$의 잘 알려진 Karhunen-Loeve expansion이다. 공분산 함수의 스펙트럼 분해를 통해 우리는 $\sigma$를 고유값들 $\{\lambda_{j}\}$와 그것에 대응되는 고유함수(eigenfunction)들 $\{V_{j}(t)\}$를 가지고 L2 관점에서 직교인 확장으로 표현할 수 있다. 즉
$$\sigma(s,t)=\sum_{j=1}^{\infty}\lambda_{j}V_{j}(s)V_{j}(t)$$
로 표현할 수 있다. 여기서 고유값들은 처음 몇 개들만을 제외하고는 빠르게 0으로 간다고 가정한다. 공분산 함수는 양정치이므로, 우리는 고유값들이 양수이며 $\lambda_{1}\geq \lambda_{2} \ldots \geq 0$으로 정렬할 수 있다. 여기서의 목표는 $\sigma(s,t)$에서 functional variation을 나타내는 처음 몇 개의 부분을 찾는 것으로, 고유치는 각각의 component들이 total variance를 얼마나 설명해주는지를 말해준다.

무작위 커브 $X(t)$는 다음과 같이 표현할 수 있다.
$$X(t)=\mu(t) + \sum_{j=1}^{\infty}\xi_{j}V_{j}(t).$$
여기서 계수 $\xi_{j}$는 다음과 같은 스칼라 확률변수이다.(**j번째 functional PC score**이다.)

\begin{equation}
\xi_{j} =\int [X(t)-\mu(t)]V_{j}(t)dt.
(\#eq:Karhunen)
\end{equation}

이 때 $E\{xi_{j}\}=0$, $\text{var}\{\xi_{j}\}=\lambda_{j}$, $\sum_{j}\lambda_{j}<\infty$ 그리고 $\text{cov}\{\xi_{j},\xi_{k}\}=0,j\neq k$이다. 고유함수들 $\{V_{j}(t) \}$ (**PC 함수라고도 부른다**)은 다음을 만족한다.
$$\int_{[0,T]} [V_{j}(t)]^{2}dt=1, \qquad{\int_{[0,T]} V_{j}(t)V_{k}(t)dt=0, j\neq k. }$$
여기서 적분은 $[0,T]$에서 정의되며 periodic일 수 있다. 식 \@ref(eq:Karhunen)은 $X(t)$에 대해 잘 알려진 **Karhunen-Loeve expansion**이다. 그래서 $X(t)-\mu(t)$는 각각의 곡선들이 uncorrelated random amplitude를 갖는 직교 곡선들의 유한한 합으로 생각할 수 있다. 

과학적으로는 함수 자료임이 알려졌다고 하더라도 실제로는 표본을 통해 유한한 양의 지식만을 알고 있다. 따라서 평균 곡선 $\mu(t)$와 공분산 함수 $\sigma$를 추정하는 것은 $n$개의 표본 곡선들의 collection들 $X_{1}(t),\ldots, X_{n}(t)$로부터 기반한다. 이 때 $i$번째 곡선 $X_{i}(t)$는 $X_{i}(t)=\mu(t)+\sum_{j}\xi_{ij}V_{j}(t)$이다. $i$번째 곡선의 $k$번째 점은 $X_{ik}=X_{i}(t_{k})$이다.

이러한 functional PCA를 적합하는 방법으로는 각각의 표본 곡선들을 spline이나 local-linear smoother로 부드럽게 만든 후 적합하게 되며 regularization을 사용하기도 한다.

[@Kokoszka2017]에 따르면, 함수자료 PCA의 목적은 관찰된 자료 $X(t_{m})$을 추정 또는 예측된 주성분 점수 $\{\hat{\xi}_{j} \}$ 또는 부드러운 경로로 대체하는 데에 그 목적이 있다. 또한 FPCA가 달성할 수 있는 것으로 다음의 네 가지를 적었다.

1. irregular data are now on a “common scale” when working with scores,

2. the scores are typically of a lower dimension (though not necessarily for very sparse data),

3. the scores can be used in further multivariate statistical analyses,

4. the scores and FPC’s can be used to reconstruct individual trajectories.

### 함수자료 PCA의 적합(estimation of functional PCA)

[@Ramsay2005] 책 8장에 FPCA에 대해 자세히 소개되어 있다. L2관점에서 fPCA를 하는 것은 eigneproblem을 푸는 것과 같다고 한다. FPCA를 하는 것에는 다음과 같은 방법들이 있다.

1. **Discretizing the functions**: 가장 간단한 방법은 관찰된 함수 $x_{i}$을 이산화하는 것이다. equally spcaed grid point $t_{1},\ldots , t_{m}$을 생각하고 covariance matrix  $\mathbf{V}^{*}$를 $\mathbf{V}(t_{i},t_{j})$로 나타낸 다음 다변량 PCA를 $\mathbf{V}^{*}$에 적합, smoothes PC를 얻기 위해 interpolation을 추가로 한다. 이를 이용하는 방법은 함수 `prcomp`로 할 수 있다.

2. **Basis expansion of the functions**: $y_{i}(t)=\sum_{j=1}^{K}c_{ij}\phi_{k}(t)$라고 호자. $\Phi$를 orthonormal basis의 집합이라고 하자. 그리고 $\mathbf{C}$를 $(n\times K)$ coefficient matrix라고 하자. 그러면 $\mathbf{Y}=\mathbf{C}\Phi$이고 표본 공분산 행렬은 $\mathbf{V}=\frac{1}{n}\Phi^{T}\mathbf{C}^{T}\mathbf{C}\Phi$로 쓸 수 있다. 이를 이용하는 방법은 패키지 `FDA`에서 구현되어 있다.

3. **Smoothing the covariance kernel**: 이 방법은 `PACE` 패키지에 구현되어있다.

4. **Regularized principal components**
