# (PART) Basic Concepts {-}

# 기본적인 수학 개념들 {#math}
이 장에서는 앞으로 다룰 내용을 이해하기 위해 필요한 기본적인 수학 개념을 정리하였다.

## 집합론(set theory)

### 카디널리티(cardinality)

**카디널리티(cardinality)**는 집합의 원소의 갯수를 세기 위해 도입되었다. 유한집합에서는 원소의 갯수를 세는 것이 어렵지 않지만, 무한집합의 경우는 갯수를 세는 것이 문제가 될 수 있다. 집합 $A$가 주어졌을 때, 그것의 카디널리티를 $|A|$로 쓰도록 하자. 자연수 집합의 카디널리티는 특별히 $|\mathbb{N}|=\aleph_{0}$로 쓰며 **알레프-널(aleph-null)**로 부른다.

```{definition, name="카디널리티가 같다"}
두 집합 $A$, $B$ 사이에 전단사(bijection, 일대일 대응) 관계가 성립할 때, 두 집합의 카디널리티가 같다고 정의하고, $|A|=|B|$로 표기한다.

```

## 실함수의 수열들(sequences of real functions)

확률변수(random variable)의 수열이 함수의 수열처럼 여겨질 수 있다는 사실에서 함수의 수열의 성질을 이해하는 것은 중요하다. 일반적인 수열의 수렴에 관한 성질들과 비교했을 때 함수의 수열들의 수렴에 관한 성질에서는 여러 개의 정의로 다뤄질 수 있다는 점에서 차이가 있다. 가장 널리 알려진 함수의 수렴은 **점별수렴(pointwise convergence)**과 **균등수렴(uniform convergence)**이 있다.

```{definition, name="점별수렴"}
$\{ f_{n}(x)\}_{n=1}^{\infty}$를 실함수의 수열이라고 하자. 그러면 모든 $x\in\mathbb{R}$에 대해
$$\lim_{n\rightarrow \infty}f_{n}(x)=f(x)$$
를 만족하는 실함수 $f$가 존재할 때 수열 $\{ f_{n}(x)\}_{n=1}^{\infty}$이 $f$에 **점별수렴(pointwise convergence)**한다고 한다. 여기서는 $n\rightarrow \infty$일 때
$$f_{n} \stackrel{pw}{\rightarrow} f$$
로 표현하기로 한다.

```

점별수렴보다 더 강한 조건으로 $x \in \mathbb{R}$에 상관 없이 모든 지점에서 동시에 $n\rightarrow\infty$일 때 $f_{n}(x) \rightarrow f(x)$이길 요구할 수도 있다. 이 때 사용되는 정의가 균등수렴이다.

```{definition, name="균등수렴"}
$\{ f_{n}(x)\}_{n=1}^{\infty}$를 실함수의 수열이라고 하자. 그러면 모든 $\epsilon > 0$에 대해
$$|f_{n}(x)-f(x)| <\epsilon \forall n \geq n_{\epsilon} \text{ and } x\in \mathbb{R}$$
을 만족시키는 정수 $n_{\epsilon}$이 존재할 때 수열 $\{ f_{n}(x)\}_{n=1}^{\infty}$이 $f$에 **균등수렴(uniform convergence)**한다고 한다. 여기서는 $n\rightarrow \infty$일 때
$$f_{n} \stackrel{u}{\rightarrow} f$$
로 표현하기로 한다.

```

(두 수열의 차이점 설명)

## 연산자들과 노름(operators and norms)

### 직합(direct sum)

```{definition, name="직합"}
크기 $m \times n$인 행렬 $\mathbf{A}$와 $p\times q$인 행렬 $\mathbf{B}$가 있을 때 이들의 **직합(direct sum)**은
$$\mathbf{A}\oplus\mathbf{B}=
\begin{bmatrix}
\mathbf{A} & 0\\
0 & \mathbf{B}\\
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & \cdots & a_{1n} & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn} & 0 & \cdots & 0 \\
0 & \cdots & 0 & b_{11} & \cdots & b_{1q} \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & b_{p1} & \cdots & b_{pq} \\
\end{bmatrix}
$$

```

```{example, name="직합의 예"}
$$
\begin{bmatrix}
1 & 3 & 2\\
2 & 3 & 1\\
\end{bmatrix}
\oplus
\begin{bmatrix}
1 & 6\\
0 & 1\\
\end{bmatrix}
=
\begin{bmatrix}
1 & 3 & 2 & 0 & 0\\
2 & 3 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 6\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

```

### 크로네커 곱(Kronecker product)

```{definition, name="크로네커 곱"}
크기 $m \times n$인 행렬 $\mathbf{A}$와 $p\times q$인 행렬 $\mathbf{B}$가 있을 때 이들의 **크로네커 곱(Kronecker product)**은
$$\mathbf{A}\otimes\mathbf{B}
=
\begin{bmatrix}
a_{11}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\
\vdots & \ddots & \vdots \\
a_{m1}\mathbf{B} & \cdots & a_{mn}\mathbf{B} \\
\end{bmatrix}
$$

```

```{example, name="크로네커 곱의 예"}
다음은 크로네커 곱의 한 예이다.

$$
\begin{bmatrix}
1 & 2\\
3 & 4\\
\end{bmatrix}
\otimes
\begin{bmatrix}
0 & 5\\
6 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
1\cdot 0 & 1\cdot 5 & 2\cdot 0 & 2\cdot 5\\
1\cdot 6 & 1\cdot 7 & 2\cdot 6 & 2\cdot 7\\
3\cdot 0 & 3\cdot 5 & 4\cdot 0 & 4\cdot 5\\
3\cdot 6 & 3\cdot 7 & 4\cdot 6 & 4\cdot 7\\
\end{bmatrix}
=
\begin{bmatrix}
0 & 5 & 0 & 10\\
6 & 7 & 12 & 14\\
0 & 5 & 0 & 20\\
18 & 21 & 24 & 28\\
\end{bmatrix}
$$

```

### 텐서곱(tensor product)

### 데카르트 곱(Cartesian product)

두 개의 집합 $A$, $B$가 있을 때, 이들의 **데카르트 곱(Cartesian product)** $A\times B$는
$$A\times B = \{ (a,b)| a\in A \text{ and } b \in B\}$$
로 정의된다.

### 노름(norm)

#### 벡터 노름(vector norm)

```{definition, name="노름과 노름공간"}
벡터공간 $X$에서 다음 세 조건들이 만족되면 함수 $\|\cdot\|$을 **노름(norm)**이라 하고 또한 벡터공간 $X$를 **노름공간(normed space)**라 한다.

1. 임의의 $\mathbf{x}\in X$, where $\|\mathbf{x}\| \geq 0$이며 $\|\mathbf{x}\|=0$이기 위한 필요충분조건은 $\mathbf{x}=\mathbf{0}$이다.

2. 임의의 $\mathbf{x}, \mathbf{y}\in X$에 대해
$$\| \mathbf{x}+\mathbf{y}\|\leq \|\mathbf{x}\| + \|\mathbf{y}\|$$
가 성립한다.

3. 임의의 스칼라 $\alpha$와 임의의 $\mathbf{x}\in X$에 대해
$$\| \alpha \mathbf{x}\|=|\alpha| \|\mathbf{x}\|$$
가 성립한다.

```

#### 행렬 노름(matrix norm)

### 거리(metric)

```{definition, name="거리"}
집합 $X$의 두 원소들 $x,y$에 대해 값 $d(x,y)$를 할당하는 함수 $d(\cdot, \cdot)$가 다음 세 조건들을

1. 임의의 $x,y\in X$에 대해서 $d(x,y)\geq 0$이다. 여기서 $d(x,y)=0$이기 위한 필요충분조건은 $x=y$이다.

2. 임의의 $x,y\in X$에 대해서 $d(x,y)=d(y,x)$이다.

3. 임의의 $x,y,z\in X$에 대해서 $d(x,z)\leq d(x,y)+d(y,z)$가 성립한다.

```

만약 $d(\mathbf{x},\mathbf{y})=\| \mathbf{x}-\mathbf{y}\|$라 하면 노름공간이 거리공간이 된다. 그러나 모든 거리공간이 노름공간으로부터 유도되는 것은 이니다.

## 행렬의 분해(matrix decomposition)

### 고유값 분해(eigenvalue decomposition)

고유값 분해는 행렬 $A$가 $n\times n$ 정방행렬일 때만 적용 가능하다.

### 스펙트럼 분해(spectral decomposition)

$p\times p$ 대칭행렬 $A$에 대한 **스펙트럼 분해(spectral decomposition)**는 다음과 같다. $p\times p$ 대칭행렬 $A$는 직교행렬 $P$에 의해 **대각화(diagonalization)**된다고 한다.

$$A=P\Lambda P^{T}=\sum_{i=1}^{p}\lambda_{i}e_{i}e_{i}^{T}.$$

이때 $PP^{T}=P^{T}P=I$를 만족하는 직교행렬 $P$는 $P=[e_{1},\ldots , e_{p}]$로 이루어지며, $\Lambda$는 $A$의 고유값(eigenvalue)들로만 이루어진 대각행렬(diagonal matrix)

$$
\Lambda=
\begin{bmatrix}
\lambda_{1} & \cdots & 0\\
\vdots & \ddots & \vdots\\
0 & \cdots & \lambda_{p}\\
\end{bmatrix}
$$

이다. 대각행렬 $\Lambda$는 $P^{T}AP=\Lambda$이다.

### 특이값분해(SVD)

**특이값분해(singular value decomposition, SVD)**는 $m\times n$ 직사각형 행렬 $A$에 대해 스펙트럼 분해를 일반화한 것이다.
$A$의 특이값 분해는 다음과 같다.

$$A=U\Sigma V^{T}.$$

이 때

- $U$: $A$의 left singular vector로 이루어진 $m\times m$ 직교행렬(orthogonal matrix)

- $\Sigma$: 주 대각성분이 $\sqrt{\lambda_{i}}$로 이루어진 $m\times n$ 직사각 대각행렬(diagonal matrix)

- $V$: $A$의 right singular vector로 이루어진 $n\times n$ 직교행렬(orthogonal matrix)

행렬 $A$의 계수(rank)가 $k$라고 할 때, 

- $U=[u_{1},\ldots , u_{k}, \ldots u_{m}]$는 $AA^{T}$를 고유값분해(eigenvalue decomposition)로 직교대각화하여 얻은 $m\times m$ 직교행렬(orthogonal matrix)이며, 특히 $[u_{1},\ldots, u_{k}]$를 **좌특이벡터(left signular vector)**라고 한다.

- $V=[v_{1},\ldots ,v_{k},\ldots , v_{n}]$는 $A^{T}A$를 고유값분해로 직교대각화하여 얻은 $n\times n$ 직교행렬이며, 특히 $[v_{1},v_{2},\ldots ,v_{k}]$를 **우특이벡터(right signular vector)**라고 한다.

- $\Sigma$는 $A^{T}A$의 0이 아닌 고유값이 $\lambda_{1},\lambda_{2},\ldots , \lambda_{k}$일 때 $\sqrt{\lambda_{1}},\ldots, \sqrt{\lambda_{k}}$를 대각성분으로 가지고 나머지 성분을 0으로 갖는 $m\times n$ 직사각 **대각행렬(diagonal matrix)**이다.

$$
\Sigma=
\begin{bmatrix}
\sqrt{\lambda_{1}} & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & \sqrt{\lambda_{2}} & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \cdots & 0\\
0 & 0 & \cdots & \sqrt{\lambda_{k}} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\end{bmatrix}.
$$

즉 $A$를 다시 쓰면

$$
A=
\begin{bmatrix}
u_{1} & \cdots & u_{k} & \cdots & u_{m}\\
\end{bmatrix}
\begin{bmatrix}
\sqrt{\lambda_{1}} & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & \sqrt{\lambda_{2}} & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \cdots & 0\\
0 & 0 & \cdots & \sqrt{\lambda_{k}} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\end{bmatrix}
\begin{bmatrix}
V_{1}^{T}\\
\vdots \\
V_{k}^{T}\\
\vdots\\
V_{n}^{T}\\
\end{bmatrix}
$$

이다. 위 식에서 **특이값(singular value)**는 $\sigma_{i}^{2}=\lambda_{i}$로부터 $\sigma_{i}=\sqrt{\lambda_{i}}$가 된다. 참고로 $U, V$가 직교행렬이면 $UU^{T}=I$, $VV^{T}=I$가 성립한다.

### 특이값분해와 고유값분해의 관계

$m\times n$ 행렬 $A$의 특이값분해의 $U$는 $AA^{T}$의 고유벡터이고, $V$는 $A^{T}A$의 고유벡터이며, $A$의 0이 아닌 특이값들의 제곱 $\Sigma\Sigma^{T}, \Sigma^{T}\Sigma$는 $AA^{T}$, $A^{T}A$의 고유값과 같음을 알 수 있다. 참고로 $\sigma_{i}=\sqrt{\lambda_{i}}$ 이므로 $\Sigma\Sigma^{T}$ 또는 $\Sigma^{T}\Sigma=\lambda_{i}$이다.

\begin{eqnarray*}
U&=&AA^{T}\\
&=&(U\Sigma V^{T})(U\Sigma V^{T})^{T}\\
&=&(U\Sigma V^{T})(V\Sigma^{T} U^{T})\\
&=&U(\Sigma\Sigma^{T})U^{T}\\
\end{eqnarray*}

\begin{eqnarray*}
V&=&A^{T}A\\
&=&(U\Sigma V^{T})^{T}(U\Sigma V^{T})\\
&=&(V\Sigma^{T} U^{T})(U\Sigma V^{T})\\
&=&V(\Sigma^{T}\Sigma)V^{T}\\
\end{eqnarray*}

### 특이값 분해의 기하학적 의미

[위키피디아](https://en.wikipedia.org/wiki/Singular_value_decomposition)를 참고하자.

## 기저(basis)

### Riesz basis

In the Hilbert space $L_{2}[0,1]$, an unconditional basis is called a **Riesz basis** if it is "almost normalized". This means that there exist real, positive, non-zero consts $m$ and $M$ so that
$$0 < m \leq \| \phi_{i}\|\leq M < \infty.$$
A Riesz basis is characterized by two Riesz constants $A$ and $B$, so that for all $f=\sum_{i}s_{i}\phi_{i}\in L_{2}[0,1]$,
$$A^{2}\| f \|^{2}\leq \sum_{i\in\mathbb{Z}}s_{i}^{2}\leq B^{2}\|f\|^{2}.$$

(Jensen의 Noise reduction and wavelet thresholding으로부터)

There exists $\phi_{0}(x)\in\mathcal{V}_{1}$ such that $\{ \phi_{0}(x-k) | k\in\mathcal{Z} \}$ forms a Riesz basis of $\mathcal{V}_{1}$, i.e, there exists $0< A \leq B <\infty$ such that
$$A \| c_{k}\|^{2} \leq \| \sum_{k}c_{k}\phi_{0}(x-k)\|^{2} \leq B \| c_{k} \|^{2}$$
for all $\{c_{k}\}\in l^{2}$, where $A$ and $B$ do not depend on the $c_{k}$

(동익이형 박사논문 47쪽)

### Radial basis function

Radial funtion이란 거리에만 의존하는 함수를 의미한다. 어떤 함수에 대한 근사 모델을 radial function의 선형조합으로 표현할 수 있다.

- Gaussian

$$\phi(r)=e^{-(\epsilon r)^{2}}$$

- Multiquadric

$$\phi(r)=\sqrt{1+(\epsilon r)^{2}}$$

- Inverse quadratic

$$\phi(r)=\frac{1}{1+(\epsilon r)^{2}}$$

- Inverse multiquadric

$$\phi(r)=\frac{1}{\sqrt{1+(\epsilon r)^{2}}}$$


## 공간(space)

### L2 공간(L2 space)

$$\| f\|_{p}=(\int_{S}|f|^{p}d\mu)^{1/p}<\infty$$

### 바나흐공간(Banach space)

### 힐버트공간(Hilbert space)

### Sobolev 공간(Sobolev space)

### Besov 공간(Besov space)

### Reproducing kernel Hilbert space

## 거리(distance)

군집분석 방법에서는 관측값들의 거리를 이용해 군집을 나눌 때 사용된다.

- 유클리드 거리(Euclidean distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{2})^{1/2}$

- 민콥스키 거리(Minkowski distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{m})^{1/m}$

- 맨하탄 거리(Manhattan distance): $d(x,y)=\sum_{i=1}^{p}|x_{i}-y_{i}|$

- 표준화 거리(standardized distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{2}/s_{i}^{2})^{1/2}$, 여기서 $s_{i}$는 $i$번째 변수에 대한 표준편차

- 마할라노비스 거리(Mahalanobis distance): $d(x,y)=(x-y)^{T}\boldsymbol{\Sigma}^{-1}(x-y)$, 여기서 $\Sigma$는 공분산행렬

- 체비셰프 거리(Chebychev distance): $d(x,y)=\max_{i=1,\ldots ,p}|x_{i}-y_{i}|$

다음 거리들은 유클리드 거리와 더불어 공간통계에서 많이 쓰이는 것들이다.

- chordal distance (현 거리? 잘 모르겠음)

- geodesic distance

