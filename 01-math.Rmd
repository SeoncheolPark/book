# (PART) Basic Concepts {-}

# 기본적인 수학 개념들 {#math}
이 장에서는 앞으로 다룰 내용을 이해하기 위해 필요한 기본적인 수학 개념을 정리하였다.

## 집합론(set theory)

### 카디널리티(cardinality)

**카디널리티(cardinality)**는 집합의 원소의 갯수를 세기 위해 도입되었다. 유한집합에서는 원소의 갯수를 세는 것이 어렵지 않지만, 무한집합의 경우는 갯수를 세는 것이 문제가 될 수 있다. 집합 $A$가 주어졌을 때, 그것의 카디널리티를 $|A|$로 쓰도록 하자. 자연수 집합의 카디널리티는 특별히 $|\mathbb{N}|=\aleph_{0}$로 쓰며 **알레프-널(aleph-null)**로 부른다.

```{definition, name="카디널리티가 같다"}
두 집합 $A$, $B$ 사에 전단사(bijection, 일대일 대응) 관계가 성립할 때, 두 집합의 카디널리티가 같다고 정의하고, $|A|=|B|$로 표기한다.

```

## 극한(limit)

이 부분은 [@Polansky2011]의 1장을 참고하였다.

```{definition, name="수열의 수렴"}
$\{ x_{n}\}_{n=1}^{\infty}$를 실수들의 수열이라고 하고 $x\in\mathbb{R}$을 실수라고 하자. 그러면 모든 $\epsilon >0$에 대해 $n_{\epsilon}\in\mathbb{N}$이 존재해 모든 $n\geq n_{\epsilon}$에 대해 $|x_{n}-x|<\epsilon$일 때 $x_{n}$은 $n\rightarrow\infty$함에 따라 $x$에 **수렴(converge)**한다고 말하며, 다음과 같이 쓴다.
$$\lim_{n\rightarrow\infty}x_{n}=x.$$

```

```{definition, name="상계와 하계"}
$\{ x_{n}\}_{n=1}^{\infty}$를 실수들의 수열이라고 하자. 만약  $x_{n}\leq u , \forall n\in\mathbb{N}$을 만족하는 실수 $u\in\mathbb{R}$이 존재할 경우 $u$를 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **상계(upper bound)**라고 한다. 마찬가지로 $x_{n}\geq l, \forall n\in\mathbb{N}$을 만족하는 실수 $l\in\mathbb{R}$을 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **하계(lower bound)**라고 한다.

```

```{definition, name="최소상계와 최대하계"}
$\{ x_{n}\}_{n=1}^{\infty}$를 실수들의 수열이라고 하자. 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **최소상계(supremum)** $u_{l}$은 상계들 중 가장 작은 상계, 즉 모든 다른 상계들 $u$에 대해 $u_{l} \leq u$인 상계이며,
$$u_{l}=\sup_{n\in\mathbb{N}}x_{n}$$
으로 쓴다. 마찬가지로 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **최대하계(infimum)** $l_{u}$는 하계들 중 가장 큰 하계, 즉 모든 다른 하계들 $l$에 대해 $l_{u}\geq l$인 하계이며,
$$l_{u}=\inf_{n\in\mathbb{N}}x_{n}$$
으로 쓴다.

```

```{r, echo=F, fig.cap='최소상계.', fig.align='center'}
knitr::include_graphics("images/math_sup.png")
```

우리는 최소상계와 최대하계의 극한 또한 생각해 볼 수 있다.

```{definition, name="상극한과 하극한 그리고 극한"}
$\{ x_{n}\}_{n=1}^{\infty}$를 실수들의 수열이라고 하자. 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **상극한(limit supremum)**은
$$\lim_{n\rightarrow\infty}\sup x_{n}=\inf_{n\in\mathbb{N}}\sup_{k\geq n}x_{k}$$
이다. 마찬가지로 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **하극한(limit infimum)**은
$$\lim_{n\rightarrow\infty}\inf x_{n}=\sup_{n\in\mathbb{N}}\inf_{k\geq n}x_{k}$$
로 정의한다. 만약
$$\lim_{n\rightarrow\infty}\sup x_{n}=\lim_{n\rightarrow\infty}\inf x_{n}=c\in\mathbb{R}$$
일 경우 $c$를 수열 $\{ x_{n}\}_{n=1}^{\infty}$의 **극한(limit)**이라고 한다.

```

```{r, echo=F, fig.cap='상극한과 하극한.', fig.align='center'}
knitr::include_graphics("images/math_limsupliminf.png")
```

## 연산자들과 노름(operators and norms)

### 직합(direct sum)

```{definition, name="직합"}
크기 $m \times n$인 행렬 $\mathbf{A}$와 $p\times q$인 행렬 $\mathbf{B}$가 있을 때 이들의 **직합(direct sum)**은
$$\mathbf{A}\oplus\mathbf{B}=
\begin{bmatrix}
\mathbf{A} & 0\\
0 & \mathbf{B}\\
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & \cdots & a_{1n} & 0 & \cdots & 0 \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn} & 0 & \cdots & 0 \\
0 & \cdots & 0 & b_{11} & \cdots & b_{1q} \\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
0 & \cdots & 0 & b_{p1} & \cdots & b_{pq} \\
\end{bmatrix}
$$

```

```{example, name="직합의 예"}
$$
\begin{bmatrix}
1 & 3 & 2\\
2 & 3 & 1\\
\end{bmatrix}
\oplus
\begin{bmatrix}
1 & 6\\
0 & 1\\
\end{bmatrix}
=
\begin{bmatrix}
1 & 3 & 2 & 0 & 0\\
2 & 3 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 6\\
0 & 0 & 0 & 0 & 1\\
\end{bmatrix}
$$

```

### 크로네커 곱(Kronecker product)

```{definition, name="크로네커 곱"}
크기 $m \times n$인 행렬 $\mathbf{A}$와 $p\times q$인 행렬 $\mathbf{B}$가 있을 때 이들의 **크로네커 곱(Kronecker product)**은
$$\mathbf{A}\otimes\mathbf{B}
=
\begin{bmatrix}
a_{11}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\
\vdots & \ddots & \vdots \\
a_{m1}\mathbf{B} & \cdots & a_{mn}\mathbf{B} \\
\end{bmatrix}
$$

```

```{example, name="크로네커 곱의 예"}
다음은 크로네커 곱의 한 예이다.

$$
\begin{bmatrix}
1 & 2\\
3 & 4\\
\end{bmatrix}
\otimes
\begin{bmatrix}
0 & 5\\
6 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
1\cdot 0 & 1\cdot 5 & 2\cdot 0 & 2\cdot 5\\
1\cdot 6 & 1\cdot 7 & 2\cdot 6 & 2\cdot 7\\
3\cdot 0 & 3\cdot 5 & 4\cdot 0 & 4\cdot 5\\
3\cdot 6 & 3\cdot 7 & 4\cdot 6 & 4\cdot 7\\
\end{bmatrix}
=
\begin{bmatrix}
0 & 5 & 0 & 10\\
6 & 7 & 12 & 14\\
0 & 5 & 0 & 20\\
18 & 21 & 24 & 28\\
\end{bmatrix}
$$

```

### 크로네커 거듭곱(Kroneckerian power)

We call $p^{k}$-vector $\mathbf{a}^{\otimes k}$, the $k$-th power of the $p$-vector $\mathbf{a}$, if $\mathbf{a}^{\otimes 0}=1$ and
$$\mathbf{a}^{\otimes k} = \mathbf{a} \otimes \cdots \otimes\mathbf{a} (\text{ k times}).$$
In general, for any matrix $\mathbf{A}$, the Kroneckerian power is given by
$$\mathbf{A}^{\otimes k} = \mathbf{A} \otimes \cdots \otimes\mathbf{A} (\text{ k times}).$$
Furthermore, $\mathbf{A}^{\otimes k}\mathbf{B}^{\otimes k}=(\mathbf{A}\mathbf{B})^{\otimes k}$. In particular, it is noted that
$$\mathbf{a}^{\otimes k}\otimes\mathbf{a}^{\otimes j}= \mathbf{a}^{\otimes (k+ j)}, \qquad{k,j\in\mathbb{N}.}$$
The following statement makes it possible to identify where in a long vector of the Kroneckerian power a certain element of the product is situated.

### 텐서곱(tensor product)

이 부분은 [@Kokoszka2017]의 10.5절을 참고하였다. 우리가 covariance operator를 다룰 때 텐서곱과 텐서 공간을 생각하는 것이 편리할 때가 있다고 한다. 특히 무한차원이나 일반적인 힐버트 공간에서는 텐서곱의 활용이 절대적이다.

다음과 같은 행렬 $\mathbf{A}\in\mathbb{R}^{N\times M}$을 생각해보자. 이 행렬은 세 가지 방법으로 바라볼 수 있다. 

1. 전통적인 행렬

2. $\mathbb{R}^{N}\rightarrow\mathbb{R}^{M}$으로 가는 선형 변환: 벡터의 왼쪽이나 오른쪽에 행렬을 곱하는 형태 (유클리드 공간에서의 모든 선형 변환을 떠올려보자)

3. $\mathbb{R}^{N}\times \mathbb{R}^{M}\rightarrow\mathbb{R}$인 겹선형(bilinear) functional: 이차 형식에서 많이 다루는 형태이다. 다음과 같이 두 유클리드 공간 $\mathcal{H}_{1}=\mathbb{R}^{N},\mathcal{H}_{2}=\mathbb{R}^{M}$가 있다고 하자. 두 개의 벡터 $x_{1}\in\mathbb{R}^{N}$, $x_{2}\in\mathbb{R}^{M}$에 **텐서곱(tensor product)**를 적용하면 다음과 같은 행렬을 유도한다.
$$x_{1}\otimes x_{2}:=x_{1}x_{2}^{T}.$$
또한 공간 $\mathbb{R}^{N}\otimes \mathbb{R}^{M}$은 위와 같은 원소들의 모든 유한한 선형 결합의 공간으로 정의할 수 있다. 다시 말하면 만약 $\mathbf{A}\in\mathbb{R}^{N}\otimes \mathbb{R}^{M}\rightarrow\mathbb{R}$이면 $J$개의 원소들 $x_{1j}\in\mathbb{R}^{N}$, $x_{2j}\in\mathbb{R}^{M}$이 존재해
$$\mathbf{A}=\sum_{j=1}^{J}x_{1j}\otimes x_{2j}=\sum_{j=1}^{J}x_{1j}x_{2j}^{T}$$
를 만족한다. 어떤 $N\times M$ 행렬도 이런 표현 방법으로 표현할 수 있으므로 $\mathbb{R}^{N}\otimes \mathbb{R}^{M}=\mathbb{R}^{N\times M}$으로 표현할 수 있다. 이제 이 세번째 관점을 다른 공간에 대해서도 특정화해보자. 어떤 $N\times M$ 행렬 $\mathbf{A}$는 다음과 같은
$$\mathbf{A}(y_{1},y_{2}):=y_{1}^{T}\mathbf{A}y_{2}$$
인 겹선형 mapping을 이끌어낸다. 따라서 공간 $\mathbb{R}^{N}\otimes \mathbb{R}^{M}$은 모든 겹선형 map $\mathbb{R}^{N}\times \mathbb{R}^{M}\rightarrow \mathbb{R}$들의 집합으로 identified될 수 있다. 오브젝트 $x_{1}\otimes x_{2}$는 또한 다음과 같은 겹선형 mapping
$$(x_{1}\otimes x_{2})(y_{1},y_{2})=y_{1}^{T}x_{1}x_{2}^{T}y_{2}=(y_{1}^{T}x_{1})(y_{2}^{T}x_{2})$$
로 identified될 수 있다.

이러한 관점에서 텐서를 바라보자. 텐서는 스칼라들의 체가 연관된 공간에서의 cartesian 곱으로부터 오는 다중선형사상(multilinear map)이다. 만약 $\{e_{1j} \}$와  $\{e_{2j} \}$가 $\mathbb{R}^{N}$과 $\mathbb{R}^{M}$의 정규 기저들이라고 하자. 그러면 $\{e_{1j}\otimes e_{2j} \}$는 $\mathbb{R}^{N}\times \mathbb{R}^{M}$의 기저가 된다. 이런 방식으로 우리는 이 기저를 전통적인 행렬에도 적용할 수 있다.
$$\mathbf{A}(e_{1j}^{T}, e_{2k})=e_{1j}^{Y}\mathbf{A}e_{2k}=\mathbf{A}(j,k).$$

```{definition, name="힐버트 공간에서의 텝서곱"}
$x_{1}\in\mathcal{H}_{1}$과 $x_{2}\in\mathcal{H}_{2}$를 두 실 힐버트 공간에서의 원소들이라고 하자. 그러면 텐서곱 $x_{1}\otimes x_{2}: \mathcal{H}_{1}\times\mathcal{H}_{2} \rightarrow\mathbb{R}$은 겹선형 사상으로 어떤 $(y_{1},y_{2})\times \mathcal{H}_{1}\times \mathcal{H}_{2}$에 대해
$$(x_{1}\otimes x_{2})(y_{1},y_{2})=\langle x_{1},y_{1}\rangle_{\mathcal{H}_{1}}\langle x_{2},y_{2}\rangle_{\mathcal{H}_{2}}$$
로 정의된다.

```

```{example, name="행렬의 텐서곱 예시"}
$$
\begin{bmatrix}
1 & 2\\
3 & 4
\end{bmatrix}
\otimes
\begin{bmatrix}
1 & 1 & 1\\
1 & 1 & 1\\
1 & 1 & 2
\end{bmatrix}
=
\begin{bmatrix}
1 & 1 & 1 & 2 & 2 & 2\\
1 & 1 & 1 & 2 & 2 & 2\\
1 & 1 & 2 & 2 & 2 & 4\\
3 & 3 & 3 & 4 & 4 & 4\\
3 & 3 & 3 & 4 & 4 & 4\\
3 & 3 & 6 & 4 & 4 & 8\\
\end{bmatrix},
$$
$$
\begin{bmatrix}
1 & 1 & 1\\
1 & 1 & 1\\
1 & 1 & 2
\end{bmatrix}
\otimes
\begin{bmatrix}
1 & 2\\
3 & 4
\end{bmatrix}
=
\begin{bmatrix}
1 & 2 & 1 & 2 & 1 & 2\\
3 & 4 & 3 & 4 & 3 & 4\\
1 & 2 & 1 & 2 & 1 & 2\\
3 & 4 & 3 & 4 & 3 & 4\\
1 & 2 & 1 & 2 & 2 & 4\\
3 & 4 & 3 & 4 & 6 & 8\\
\end{bmatrix}.
$$

```



### 데카르트 곱(Cartesian product)

두 개의 집합 $A$, $B$가 있을 때, 이들의 **데카르트 곱(Cartesian product)** $A\times B$는
$$A\times B = \{ (a,b)| a\in A \text{ and } b \in B\}$$
로 정의된다.

### 노름(norm)

#### 벡터 노름(vector norm)

```{definition, name="노름과 노름공간"}
벡터공간 $X$에서 다음 세 조건들이 만족되면 함수 $\|\cdot\|$을 **노름(norm)**이라 하고 또한 벡터공간 $X$를 **노름공간(normed space)**라 한다.

1. 임의의 $\mathbf{x}\in X$, where $\|\mathbf{x}\| \geq 0$이며 $\|\mathbf{x}\|=0$이기 위한 필요충분조건은 $\mathbf{x}=\mathbf{0}$이다.

2. 임의의 $\mathbf{x}, \mathbf{y}\in X$에 대해
$$\| \mathbf{x}+\mathbf{y}\|\leq \|\mathbf{x}\| + \|\mathbf{y}\|$$
가 성립한다.

3. 임의의 스칼라 $\alpha$와 임의의 $\mathbf{x}\in X$에 대해
$$\| \alpha \mathbf{x}\|=|\alpha| \|\mathbf{x}\|$$
가 성립한다.

```

#### 행렬 노름(matrix norm)

## 행렬의 분해(matrix decomposition)

### 고유값 분해(eigenvalue decomposition)

고유값 분해는 행렬 $A$가 $n\times n$ 정방행렬일 때만 적용 가능하다.

### 스펙트럼 분해(spectral decomposition) {#spectraldecomposition}

$p\times p$ 대칭행렬 $A$에 대한 **스펙트럼 분해(spectral decomposition)**는 다음과 같다. $p\times p$ 대칭행렬 $A$는 직교행렬 $P$에 의해 **대각화(diagonalization)**된다고 한다.

$$A=P\Lambda P^{T}=\sum_{i=1}^{p}\lambda_{i}e_{i}e_{i}^{T}.$$

이때 $PP^{T}=P^{T}P=I$를 만족하는 직교행렬 $P$는 $P=[e_{1},\ldots , e_{p}]$로 이루어지며, $\Lambda$는 $A$의 고유값(eigenvalue)들로만 이루어진 대각행렬(diagonal matrix)

$$
\Lambda=
\begin{bmatrix}
\lambda_{1} & \cdots & 0\\
\vdots & \ddots & \vdots\\
0 & \cdots & \lambda_{p}\\
\end{bmatrix}
$$

이다. 대각행렬 $\Lambda$는 $P^{T}AP=\Lambda$이다.

### 특이값분해(SVD)

**특이값분해(singular value decomposition, SVD)**는 $m\times n$ 직사각형 행렬 $A$에 대해 스펙트럼 분해를 일반화한 것이다.
$A$의 특이값 분해는 다음과 같다.

$$A=U\Sigma V^{T}.$$

이 때

- $U$: $A$의 left singular vector로 이루어진 $m\times m$ 직교행렬(orthogonal matrix)

- $\Sigma$: 주 대각성분이 $\sqrt{\lambda_{i}}$로 이루어진 $m\times n$ 직사각 대각행렬(diagonal matrix)

- $V$: $A$의 right singular vector로 이루어진 $n\times n$ 직교행렬(orthogonal matrix)

행렬 $A$의 계수(rank)가 $k$라고 할 때, 

- $U=[u_{1},\ldots , u_{k}, \ldots u_{m}]$는 $AA^{T}$를 고유값분해(eigenvalue decomposition)로 직교대각화하여 얻은 $m\times m$ 직교행렬(orthogonal matrix)이며, 특히 $[u_{1},\ldots, u_{k}]$를 **좌특이벡터(left signular vector)**라고 한다.

- $V=[v_{1},\ldots ,v_{k},\ldots , v_{n}]$는 $A^{T}A$를 고유값분해로 직교대각화하여 얻은 $n\times n$ 직교행렬이며, 특히 $[v_{1},v_{2},\ldots ,v_{k}]$를 **우특이벡터(right signular vector)**라고 한다.

- $\Sigma$는 $A^{T}A$의 0이 아닌 고유값이 $\lambda_{1},\lambda_{2},\ldots , \lambda_{k}$일 때 $\sqrt{\lambda_{1}},\ldots, \sqrt{\lambda_{k}}$를 대각성분으로 가지고 나머지 성분을 0으로 갖는 $m\times n$ 직사각 **대각행렬(diagonal matrix)**이다.

$$
\Sigma=
\begin{bmatrix}
\sqrt{\lambda_{1}} & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & \sqrt{\lambda_{2}} & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \cdots & 0\\
0 & 0 & \cdots & \sqrt{\lambda_{k}} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\end{bmatrix}.
$$

즉 $A$를 다시 쓰면

$$
A=
\begin{bmatrix}
u_{1} & \cdots & u_{k} & \cdots & u_{m}\\
\end{bmatrix}
\begin{bmatrix}
\sqrt{\lambda_{1}} & 0 & \cdots & 0 & 0 & \cdots & 0\\
0 & \sqrt{\lambda_{2}} & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \cdots & 0\\
0 & 0 & \cdots & \sqrt{\lambda_{k}} & 0 & \cdots & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 0\\
\end{bmatrix}
\begin{bmatrix}
V_{1}^{T}\\
\vdots \\
V_{k}^{T}\\
\vdots\\
V_{n}^{T}\\
\end{bmatrix}
$$

이다. 위 식에서 **특이값(singular value)**는 $\sigma_{i}^{2}=\lambda_{i}$로부터 $\sigma_{i}=\sqrt{\lambda_{i}}$가 된다. 참고로 $U, V$가 직교행렬이면 $UU^{T}=I$, $VV^{T}=I$가 성립한다.

### 특이값분해와 고유값분해의 관계(the relationship between spectral decomposition and eigenvalue decomposition)

$m\times n$ 행렬 $A$의 특이값분해의 $U$는 $AA^{T}$의 고유벡터이고, $V$는 $A^{T}A$의 고유벡터이며, $A$의 0이 아닌 특이값들의 제곱 $\Sigma\Sigma^{T}, \Sigma^{T}\Sigma$는 $AA^{T}$, $A^{T}A$의 고유값과 같음을 알 수 있다. 참고로 $\sigma_{i}=\sqrt{\lambda_{i}}$ 이므로 $\Sigma\Sigma^{T}$ 또는 $\Sigma^{T}\Sigma=\lambda_{i}$이다.

\begin{eqnarray*}
U&=&AA^{T}\\
&=&(U\Sigma V^{T})(U\Sigma V^{T})^{T}\\
&=&(U\Sigma V^{T})(V\Sigma^{T} U^{T})\\
&=&U(\Sigma\Sigma^{T})U^{T}\\
\end{eqnarray*}

\begin{eqnarray*}
V&=&A^{T}A\\
&=&(U\Sigma V^{T})^{T}(U\Sigma V^{T})\\
&=&(V\Sigma^{T} U^{T})(U\Sigma V^{T})\\
&=&V(\Sigma^{T}\Sigma)V^{T}\\
\end{eqnarray*}

즉 $u_{1},\ldots , u_{k}, \ldots u_{m}$는 range$(A)$의 직교정규벡터, $v_{1},\ldots ,v_{k},\ldots , v_{n}$는 $\mathcal{N}(A)^{\perp}$의 직교정규벡터이다.

### 특이값 분해에 대한 추가 설명(additional explanation about spectral decomposition)

여기서는 [Boyd 교수의 강의노트](https://web.stanford.edu/class/archive/ee/ee263/ee263.1082/notes/ee263coursereader.pdf)를 참고하였다. 잠시 편의를 위해 $U$ 는 $m\times n$ 행렬, $\Sigma$는 $n \times n$ 행렬이라고 하자. 그러면 특이값 분해는

$$A=U\Sigma V^{T}=\sum_{i=1}^{n}\sigma_{i}u_{i}v_{i}^{T}$$


```{r, echo=F, fig.cap='SVD 그림.', fig.align='center'}
knitr::include_graphics("images/math_svd001.png")
```

가 된다. 선형 사상(mapping) $y=Ax$는 다음과 같이 분해할 수 있다.

- $x$를 input direction들 $v_{1}, \ldots , v_{n}$을 따라 계수들을 계산한다.

- $\sigma_{i}$는 척도계수

- 이것들을 다시 output directions $u_{1}, \ldots, u_{n}$을 따라 재구성한다.

대칭행렬 $A$에 대한 고유값 분해와 달라지는 점은 input direction들과 output direction들이 다르다는 것이다.

- $v_{1}$는 input direction으로 가장 민감하다.(most sensitive, highest gain)

- $u_{1}$은 output direction으로 가장 민감하다.

- $Av_{1}=\sigma_{1}u_{1}$이다.

```{example, name="SVD의 기하학적 의미 예"}
$A=\mathbb{R}^{2\times 2}$이며 $\Sigma=\text{diag}(1,0.5)$인 경우를 생각해보자. 이 경우 $x$를 $v_{1}$, $v_{2}$를 따라 풀면 $v_{1}^{T}x=0.5, v_{2}^{T}x=0.6$ 즉 $x=0.5v_{1} + 0.6v_{2}$이며, $Ax=(v_{1}^{T}x)\sigma_{1}u_{1} + (v_{2}^{T}x)\sigma_{2}u_{2}= (0.5)(1)u_{1} + (0.6)(0.5)u_{2}$ 이다.

```

```{r, echo=F, fig.cap='SVD 예제 그림.', fig.align='center'}
knitr::include_graphics("images/math_svd002.png")
```

### 특이값 분해의 기하학적 의미(geometrical meaning of spectral decomposition)

[위키피디아](https://en.wikipedia.org/wiki/Singular_value_decomposition)를 참고하자.

## 기저(basis)

### 리츠 기저(Riesz basis)

In the Hilbert space $L_{2}[0,1]$, an unconditional basis is called a **Riesz basis** if it is "almost normalized". This means that there exist real, positive, non-zero consts $m$ and $M$ so that
$$0 < m \leq \| \phi_{i}\|\leq M < \infty.$$
A Riesz basis is characterized by two Riesz constants $A$ and $B$, so that for all $f=\sum_{i}s_{i}\phi_{i}\in L_{2}[0,1]$,
$$A^{2}\| f \|^{2}\leq \sum_{i\in\mathbb{Z}}s_{i}^{2}\leq B^{2}\|f\|^{2}.$$

(Jensen의 Noise reduction and wavelet thresholding으로부터)

There exists $\phi_{0}(x)\in\mathcal{V}_{1}$ such that $\{ \phi_{0}(x-k) | k\in\mathcal{Z} \}$ forms a Riesz basis of $\mathcal{V}_{1}$, i.e, there exists $0< A \leq B <\infty$ such that
$$A \| c_{k}\|^{2} \leq \| \sum_{k}c_{k}\phi_{0}(x-k)\|^{2} \leq B \| c_{k} \|^{2}$$
for all $\{c_{k}\}\in l^{2}$, where $A$ and $B$ do not depend on the $c_{k}$

(동익이형 박사논문 47쪽)

### Radial basis function

Radial funtion이란 거리에만 의존하는 함수를 의미한다. 어떤 함수에 대한 근사 모델을 radial function의 선형조합으로 표현할 수 있다.

- Gaussian

$$\phi(r)=e^{-(\epsilon r)^{2}}$$

- Multiquadric

$$\phi(r)=\sqrt{1+(\epsilon r)^{2}}$$

- Inverse quadratic

$$\phi(r)=\frac{1}{1+(\epsilon r)^{2}}$$

- Inverse multiquadric

$$\phi(r)=\frac{1}{\sqrt{1+(\epsilon r)^{2}}}$$

## 공간(space)

이 부분은 전체적으로 [@Shima2016]의 정의와 내용들을 따라간다.

### 군과 장(groups and fields)

수학에서 **군(group)**은 연산과 함께 정의되는 원소들의 집합으로 원소들이 closure, associativity, identity and invertability를 만족해야 한다. 대충 말해서 **장(field)**은 합, 차, 곱, 몫의 개념을 갖고 있는 대수적 구조이다.

### 벡터 공간(vector spaces)

벡터 공간은 스칼라들의 장(field)에서 정의된다. 이런 스칼라들은 실수가 될 수도 있고, 복소수가 될 수도 있다. 벡터 공간은 덧셈과 곱셈을 적용할 수 있는 수학적 실재(mathematical entity)들의 집합이다. 예를 들면 숫자 공간 $\mathbb{R}$, $\mathbb{C}$ 등은 벡터 공간이 된다. 숫자 뿐만 아니라 함수, 다항식, 선형 연산자, 벡터 등등이 수학적 실재가 될 수 있다.

```{r, echo=F, fig.cap='벡터 공간은 다양한 종류의 수학적 실재(mathematical entity)로부터 만들어진다.', fig.align='center'}
knitr::include_graphics("images/math_vectorspace.png")
```


**벡터 공간(vector space)** $V$는 다음 공리들을 만족하는 원소들 $\mathbf{x}$ (벡터라 부른다)의 collection이다.


1. $V$는 덧셈 하에서 교환 가능한 가환군(commutative group, 교환 법칩이 성립하는 군)이다.
    i. $\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}\in V$ for any $\mathbf{x},\mathbf{y}\in V$ (closure)
    ii. $\mathbf{x} + (\mathbf{y}+\mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}\in V$ for any $\mathbf{x},\mathbf{y}, \mathbf{z}\in V$ (associativity)
    iii. 다음과 같은 $\mathbf{0}$벡터(항등원)가 존재해 모든 $\mathbf{x}\in V$에 대해 $\mathbf{x}+\mathbf{0}=\mathbf{x}$를 만족한다. (identity)
    iv. 다음과 같은 덧셈에 대한 역원 $-\mathbf{x}$이 존재해 $\mathbf{x}+(-\mathbf{x})=\mathbf{0}$이 된다. (invertibility)
    
2. $V$는 어떤 장 $\mathbb{F}$에 대해 다음의 공리를 만족하며, 이런 원소들을 **스칼라(scalar)**라고 부른다.
    i. $V$는 스칼라 곱에 대해 닫혀 있다.
    $$\alpha \mathbf{x} \in V \text{ for arbitrary } \mathbf{x} \in V, \alpha \in \mathbb{F}.$$
    ii. 스칼라 곱은 $V$와 $\mathbb{F}$의 원소들에 대해 **분배가능(distributive)**하다.
    $$\alpha(\mathbf{x}+\mathbf{y})=\alpha\mathbf{x}+ \alpha\mathbf{y}\in V, (\alpha+\beta)\mathbf{x}=\alpha\mathbf{x}+\beta\mathbf{y}\in V.$$
    iii. 스칼라 곱은 결합법칙이 성립한다. $\alpha(\beta\mathbf{x})=\beta(\alpha \mathbf{x}).$
    iv. Zero scalar $0\in\mathbb{F}$에 대한 곱은 항등원 $0\mathbf{x}=\mathbf{0}\in V$을 생산한다.
    v. Unit scalar $1 \in \mathbb{F}$는 $1\mathbf{x}=\mathbf{x}$ 성질을 갖는다.



여기서부터는 벡터 공간의 주요 성질들에 대해 다룬다.

### 내적(inner products)

```{definition, name="내적"}
내적은 $\mathbf{x}$, $\mathbf{y}$의 순서쌍을 스칼라(좀 더 일반적으로는 복소수)에 mapping하는 것이고 $\langle\mathbf{x},\mathbf{y}\rangle$로 쓴다. 이 mapping은 다음 규칙들을 만족해야 한다.

1. $\langle\mathbf{x},\mathbf{y}\rangle = \langle\mathbf{y},\mathbf{x}\rangle^{*}$. (여기서 별표는 complex conjugate를 의미한다.)

2. $\langle\alpha\mathbf{x}+\beta\mathbf{y}, \mathbf{z}\rangle = \alpha^{*}\langle\mathbf{x},\mathbf{z}\rangle+\beta^{*}\langle\mathbf{y},\mathbf{z}\rangle$ (여기서 $\alpha$, $\beta$는 어떤 복소수이다.)

3. 모든 $\mathbf{x}$에 대해 $\langle\mathbf{x},\mathbf{x}\rangle \geq 0$이다.

4. $\langle\mathbf{x},\mathbf{x}\rangle = 0$은 $\mathbf{x}=\mathbf{0}$과 필요충분조건이다.

```

특별히 내적을 갖고 있는 벡터 공간을 **내적 공간(inner product space)**이라고 부른다.

내적 공간에서는 우리는 항상 **내적 노름(inner product norm)**을 다음과 같이 정의할 수 있다.
$$\|x\|=\sqrt{\langle x,x\rangle}.$$

```{proposition, name="내적 공간의 노름과 관련된 성질들"}
내적 공간은 그 공간 상에서 노름 $\| \cdot \|$을 정의하며 다음의 설질들을 갖는다.

- $\|ax\|=|a|\|x\|$
  
- $|\langle x,y\rangle | \leq \|x\| \|y\|$ (Cauchy-Schwarz inequality)
  
- $\|x+y\| \leq \|x\| +\|y\|$ (triangle inequality)

- $d(x,y) = \|x-y\|$ is a metric (distance), i.e., $d(x,y)=d(y,x)>0$ if $x\neq y$, $d(x,x)=0$ and $d(x,y) \leq d(x,z) + d(z,y)$.

```

### 직교성(orthogonality)

다음은 [@Shima2016]책에 있는 직교성의 정의다.

```{definition, name="직교"}
내적 공간에 있는 두 원소들 $\mathbf{x}, \mathbf{y}$는 $\langle x\mathbf{x},\mathbf{y}\rangle = 0$일 때 **직교(orthogonal)**한다고 말한다.

```

#### 완비 벡터 공간(complete vector spaces)

벡터 공간이 유한차원일 때, 공간의 완비성(completeness)은 같은 공간의 다른 larger orthonormal set에 포한되어 있지 않는 orthonormal set을 찾음으로써 증명할 수 있다. 예를 들면 3차원 공간에서는 linear combination이 그 공간의 모든 벡터를 표현할 수 있는 three orthonormal vector의 set을 찾기만 하면 되는 것이다. 그러나 우리가 무한 차원 공간을 고려할 때, 무한한 숫자의 orthonormal vector를 고려하는 것은 쉽지 않다. 사실 무한한 숫자의 vector의 linear combination은 때대로 같은 공간에 포함된 벡터를 표현하는 데 충분치 않을 수도 있다. 이러한 모호함을 해결하기 위해 무한 차원 공간에서 완비성을 생각하는 것이다.

```{definition, name="벡터의 Cauchy sequence"}
벡터의 수열 $\{ \mathbf{x}_{1},\mathbf{x}_{2},\ldots \}$가 모든 $\epsilon >0$에 대해 적단한 근사 숫자 $N$이 존재해 모든 $m,n > N$에 대해 $\| \mathbf{x}_{m} -\mathbf{x}_{n} \| < \epsilon$을 만족한다면 이를 벡터의 **코시 수열(Cauchy sequence)**이라고 한다. 쉽게 얘기하자면 $\mathbf{x}_{m}$과 $\mathbf{x}_{n}$이 $m,n \rightarrow \infty$ 함체 따라 가까어지는 수열을 코시 수열이라 부르는 것이다.

```

```{definition, name="코시 수열의 수렴"}
벡터의 무한 수열 $\{\mathbf{x}_{1},\mathbf{x}_{2}, \ldots \}$가 있을 때, 만약 $\mathbf{x}$가 존재해 $\| \mathbf{x}_{n} -\mathbf{x}\|\rightarrow 0$을 만족한다면 이 수열이 수렴(convergent)한다고 한다.

```

그러나 그 역은 성립하지 않는다. 코시 수열이라고 해서 다 수렴하는 건 아니다. 완비성을 정의해 이를 해결할 수 있는 것이다.

```{definition, name="벡터 공간의 완비성"}
만약 어떤 벡터 공간의 모든 코시 수열이 수렴한다면, 우리는 그 공간을 **완비(complete)**라고 부른다.

```

다음은 [@Kokoszka2017]의 10장에 나오는 예이다.

```{example, name="유클리드 공간에서의 완비성"}
유클리드 거리 하에서 개구간 $(0,1)$은 complete metric space가 아니지만, 닫힌 구간 $[0,1]$은 complete metric space가 된다. 유클리드 공간 $\mathbb{R}^{d}$는 complete하다.

```

### 위상공간(topological space)

### 분해 가능 공간(separable space)

Countable dense subset을 포함하는 위상공간(topological space)을 **분해 가능 공간(separable space)**이라고 한다.

### 노름 공간(normed space)

```{definition, name="노름 공간"}
원소들에 일종의 '길이' 또는 '크기'가 부여된 벡터 공간 $V$을 **노름 공간(normed space)**라고 한다. 노름은 벡터 공간 $V$ 위에서의 실수 함수로 $\| \|$로 표시하며

1. 모든 $\lambda\in\mathbb{F}$와 $x\in V$에 대해 $\|\lambda \mathbf{x}\| = |\lambda|\|\mathbf{x}\|$
  
2. $\|\mathbf{x} +\mathbf{y}\| \leq \|\mathbf{x}\| +\|\mathbf{y}\|#
  
3. $\|\mathbf{x}\|=0$이라는 것은 $\mathbf{x}=0$과 동치다.

```

노름 공간에 있는 모든 원소들은 항상 유한한 노름을 갖아야 한다. 만약 무한한 노름을 갖는다면 그것은 공간이 될 수 없다. 노름 $\|\cdot\|$은 항상 계량(metric)을 유도한다. 계량은 $\mathcal{F}$에서의 거리 개념이다. 즉 $d(f,g)=\| f-g\|_{\mathcal{F}}$이다. 이것은 $\mathcal{F}$가 어떤 위상 구조를 갖고있다는 뜻도 되며 우리는 $\mathcal{F}$의 원소들의 수열들이 정의된 거리에서 수렴하는지, 또는 연속 등에 대해서 생각할 수 있게 한다.

### 바나흐 공간(Banach space)

```{definition, name="힐버트 공간"}
Normed space가 complete일 경우 이를 **바나흐 공간(Banach space)**라고 부른다.

```

유한차원에서 노름공간은 항상 완비(complete)이다. 그러나 무한차원에서 노름공간은 완비일 수도 있고, 아닐 수도 있다.

```{definition, name="원-힐버트 공간"}
Inner product spaces들 중 완비가 아닌 공간들을 **원-힐버트 공간(pre-Hilbert space)**라고 부른다.

```

```{example, name="원-힐버트 공간의 예"}
유클리드 거리 하에서 개구간 $(0,1)$은 complete metric space가 아니지만, 닫힌 구간 $[0,1]$은 complete metric space가 된다. 유클리드 공간 $\mathbb{R}^{d}$는 complete하다.

```

### 힐버트 공간(Hilbert space)

많은 함수자료분석은 힐버트 공간 이론에 많이 의존한다. 모든 힐버트 공간은 함수가 무한차원 공간의 점으로서 간주될 수 있도록 하는 수단을 제공해 준다. 이런 기하학적인 관점의 쓰임새는 우리가 기초 선형대수에서 정의했던 길이, 직교정규, 그리고 원소들의 선형 결합 개념들을 좀 더 추상적인 수학적 공간에 일반화시킬 수 있다는 것이다. 이런 기하학적 관점에 가장 핵심이 되는 개념은 공간의 **기저(basis)**다. 힐버트 공간과 함께, 무한차원 공간에서 기저의 기초적 성질들을 복습해 보기로 한다.

```{definition, name="힐버트 공간"}
A complete normed space endowed with inner product is called a **Hilbert space**

```

다음 그림은 [@Shima2016]에 있는 그림으로 각 공간들 간의 사이의 관계를 이해하는 데 도움이 된다.

```{r, echo=F, fig.cap='Metric vector space로부터 힐버트 공간까지의 관계.', fig.align='center'}
knitr::include_graphics("images/math_Hilbertspacemap.png")
```

벡터 공간(유한이어도 되고 무한이어도 된다)이 주어졌을 때, 벡터의 직교정규 집합은 만약 이것이 **완비(complete)**일 때에만 기저로서 작동한다. 만약 한 번 기저를 얻는다면, 그 공간의 벡터는 직교정규 벡터들의 선형 결합으로 분해할 수 있다. 그러나 무한히 많은 직교정규 기저로부터의 무한차원 공간에서 기저를 만들고자 할 때 한 가지 주의해야 할 점은 벡터들의 집합이 완비성을 만족하는지 체크해야 한다는 것이다. 이 주의점은 어떤 공간의 벡터들의 무한 합이 수렴하더라도, 그 극한이 같은 벡터 공간 안에 포함되지 않을 수 있다는 사실로부터 야기된다. 즉 이 극한은원래 공간으로부터 빗나가게 할 수 있게 한다. 이런 경우에는 벡터들의 집합이 무한차원 공간의 기저로써 더이상 작동하지 않는 것을 의미한다.

이러한 점을 좀 더 자세히 살펴보기 위해, 힐버트 공간 $V$에서의 직교정규 벡터들의 무한집합 $\{ e_{i}\}$에 대해 생각해보자. 그 다음에 우리는 한 개의 벡터 $\mathbf{x}\in V$를 골라 $e_{i}$의 내적을 계산한다. 그러면 $\mathbf{x}$ 는 다음과 같이 표현된다.
$$c_{i}\langle e_{i},\mathbf{x} \rangle \in \mathbb{C}.$$
무한 수열 $\{c_{i}\}$와 $\{e_{i}\}$의 처음 $n$항을 사용해 우리는 다음과 같이 유한합
$$\sum_{i=1}^{n}c_{i}e_{i} \equiv \mathbf{x}_{n}$$
을 정의할 수 있다.

힐버트 공간의 대표적인 예로, $L^{p}$ 공간이 있다.

$$\| f\|_{p}=(\int_{S}|f|^{p}d\mu)^{1/p}<\infty$$

박창이 교수님 외 책에서, 통계학에서 힐버트 공간을 고려하는 이유를 설명하였는데, 그 이유는 유한차원에서 성립하는 선형대수를 적용할 수 있기 때문이라고 한다. 특히 힐버트 공간은 완비(complete)된 공간으로써 알고리즘의 수렴성을 보장하며, 내적이 존재하므로써 직교성이나 정사영(projection)을 구할 수도 있다.

### Holder 공간(Holder space)

이 공간의 정의는 [@Wasserman2006]를 참고하였다. 어떤 정수 $m$과 $\delta \in (0,1)$에 대해 $s=m+\delta$을 정의하자. **Holder 공간(Holder space)**(정확하게는 H\"{o}lder space)은 bounded m차 derivative를 갖는, 즉 모든 $u,t$에 대해 $|f^{m}(u)-f^{m}(t)|\leq |u-t|^{\delta}$인 bounded function들의 집합니다.

### Sobolev 공간(Sobolev space)

해석학에서 소볼레프 공간(Sobolev space)이란 충분히 매끄럽고, 무한대에서 충분히 빨리 0으로 수렴하는 함수들로 구성된 함수공간으로 르베그 공간의 일반화이다. 이 공간의 정의는 [@Wasserman2006]를 참고하였다. 이 책에서 Sobolev 공간의 정의는 smooth function들의 집합이라는 것이다. $D^{j}f$를 $f$의 $j$번째 weak derivative라고 하자. 먼저 weak differentiability에 대해 알아보자. $f$가 모든 bounded interval에서 적분 가능하다고 하자. 만약 다음꽈 같은 함수 $f'$가 존재해 모든 bounded interval에서 적분 가능하다면, 즉 모든 $x\leq y$에 대해
$$\int_{x}^{y}f'(s)ds=f(y)-f(x)$$
이라면 이를 **weakly differentiable**이라 부른다.

```{definition, name="Sobolev 공간"}
**m차 Sobolev space**는 다음과 같다.
$$W(m)=\{ f \in L^{2}(0,1): D^{m}f \in L^{2}(0,1)\}$$

반지를 $c$를 갖는 $m$차 Sobolev space는 다음과 같다.
$$W(m,c)=\{ f \in W(m), \| D^{m}f \|^{2} \leq c^{2}\}$$

**주기를 갖는 Sobolev class (periodic Sobolev class)**는 다음과 같다.
$$\tilde{W}(m,c)=\{ f \in W(m,c): D^{j}f(0)=D^{j}f(1), j=0,\ldots, m-1 \}.$$

```

### Besov 공간(Besov space)

이 공간의 정의 또한 [@Wasserman2006]의 9장을 참고하였다. Wavelet threshold estimator는 Besov space에서 good optimality property를 갖는다. 다음과 같이 $\Delta_{h}^{(r)}f(x)$를 정의하자.
$$\Delta_{h}^{(r)}f(x)=\sum_{k=0}^{r}\binom{r}{k}(-1)^{k}f(x+kh).$$
그러면 $\Delta_{h}^{(0)}f(x)=f(x)$이고
$$\Delta_{h}^{(r)}f(x)=\Delta_{h}^{(r-1)}f(x+h)-\Delta_{h}^{(r-1)}f(x).$$
이다. 이제 $w_{r,p}(f;t)$를 다음과 같이 정의하자.
$$w_{r,p}(f;t)=\sup_{|h|\leq t}\|\Delta_{h}^{(r)}f\|_{p}.$$
이 때 $\|g\|_{p}=\{ \int |g(x)|^{p}dx \}^{1/p}$이다. $(p,g,\zeta)$가 주어졌을 때, $r$이 존재해 $r-1\leq \zeta \leq r$을 만족한다면, **Besov seminorm** $|f|_{p,q}^{\zeta}$는
$$|f|_{p,q}^{\zeta}=[\int_{0}^{\infty}(h^{-\zeta}w_{r,p}(f;h))^{q}\frac{dh}{h}]^{1/q}$$
로 정의된다. $q=\infty$일 때는 다음과 같이 정의한다.
$$|f|_{p,\infty}^{\zeta}=\sup_{0<h<1}\frac{w_{r,p}(f;h)}{h^{\zeta}}$$

```{definition, name="Besov 공간"}
**Besov 공간(Besov space)** $B_{p,q}^{\xi}(c)$는 $[0,1]$에서 $\mathbb{R}$로 가는 함수 $f$들 중 $\int |f|^{p}<\infty$이고 $|f|_{p,q}^{\zeta}\leq c$인 $f$들의 집합니다.

```

Sobolev space $W(m)$은 Besov ball $B_{2,2}^{m}$에 대응된다. Generalized Sobolev space $W_{p}(m)$은 $m$차 미분에서 $L^{p}$ 노름을 사용하는 데 이것은 거의 Besov space와 비슷하다. 사실 $B_{p,1}^{m} \subset W_{p}(m) \subset B_{p,\infty}^{m}$이다. H\"{o}lder space는 $B_{\infty,\infty}^{m+\delta}$와 같다. $T$를 bounded variation을 갖는 함수들을 포함하는 집합이라고 할 때, $B_{1,1}^{1}\subset T \subset B_{1,\infty}^{1}$을 만족한다. 즉 Besov space는 넓은 범위희 함수 공간들을 포함한다. 이에 수렴하는 수열(convergent sequence)와 코시 수열(Cauchy sequence)에 대해 생각해 볼 수 있다.

### Skorohod 공간(Skorohod space)

$D[a,b]$을 right continuous하고 left limit을 갖는 함수들 $z:[a,b]\rightarrow \mathbb{R}$의 집합을 **Skorohod 공간(Skorohod space)**이라고 한다. [@VanderVaart2000] 257쪽에 나와있다.

### 웨이블릿 expansion의 계수들로 이해하는 Besov 공간(Besov spaces in terms of the coefficients of the wavelet expansion)

### 재생 커널 힐버트 공간(reproducing kernel Hilbert space)

재생 커널 힐버트 공간(reproducing kernel Hilbert space)이 주어지면 이에 대응되는 유일한 재생 커널이 존재한다. 역으로, 양정치 커널 $K$가 주어지면 $K$를 재생 커널로 갖는 유일한 재생 커널 힐버트 공간 $\mathcal{H}$가 존재한다.

### C 공간과 D 공간(the spaces C and D)

[@Beran2013]의 appendix 참고

## 거리와 계량(distance and metric)

### 거리(distance)

```{definition, name="거리"}
집합 $X$의 두 원소들 $x,y$에 대해 값 $d(x,y)$를 할당하는 함수 $d(\cdot, \cdot)$가 다음 세 조건들을

1. 임의의 $x,y\in X$에 대해서 $d(x,y)\geq 0$이다. 여기서 $d(x,y)=0$이기 위한 필요충분조건은 $x=y$이다.

2. 임의의 $x,y\in X$에 대해서 $d(x,y)=d(y,x)$이다.

3. 임의의 $x,y,z\in X$에 대해서 $d(x,z)\leq d(x,y)+d(y,z)$가 성립한다.

```

만약 $d(\mathbf{x},\mathbf{y})=\| \mathbf{x}-\mathbf{y}\|$라 하면 노름공간이 거리공간이 된다. 그러나 모든 거리공간이 노름공간으로부터 유도되는 것은 이니다.

군집분석 방법에서는 관측값들의 거리를 이용해 군집을 나눌 때 사용된다.

- 유클리드 거리(Euclidean distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{2})^{1/2}$

- 민콥스키 거리(Minkowski distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{m})^{1/m}$

- 맨하탄 거리(Manhattan distance): $d(x,y)=\sum_{i=1}^{p}|x_{i}-y_{i}|$

- 표준화 거리(standardized distance): $d(x,y)=(\sum_{i=1}^{p}(x_{i}-y_{i})^{2}/s_{i}^{2})^{1/2}$, 여기서 $s_{i}$는 $i$번째 변수에 대한 표준편차

- 마할라노비스 거리(Mahalanobis distance): $d(x,y)=(x-y)^{T}\boldsymbol{\Sigma}^{-1}(x-y)$, 여기서 $\Sigma$는 공분산행렬

- 체비셰프 거리(Chebychev distance): $d(x,y)=\max_{i=1,\ldots ,p}|x_{i}-y_{i}|$

다음 거리들은 유클리드 거리와 더불어 공간통계에서 많이 쓰이는 것들이다.

- chordal distance (현 거리? 잘 모르겠음)

- geodesic distance

다른 거리들도 있다.

- Hamming distance: [@Massart2007] 책에 나오는 것으로 $\mathcal{X}^{n}$을 $n$개의 확률변수들이 값을 취하는 $n$차원 공간이라고 할 때 다음과 같다.
$$d(y,y')=\sum_{i=1}^{n}\mathbb{I}_{y_{i}\neq y'_{i}}, \qquad{\forall y,y'\in\mathcal{X}^{n}}$$

### 계량(metric)

계량(metric)은 두 집합 사이의 모든 원소 짝의 거리를 지정해주는 함수이다. 


## 실함수의 수열들(sequences of real functions)

이 부분의 내용은 [@Polansky2011]의 내용을 따른다.

확률변수(random variable)의 수열이 함수의 수열처럼 여겨질 수 있다는 사실에서 함수의 수열의 성질을 이해하는 것은 중요하다. 일반적인 수열의 수렴에 관한 성질들과 비교했을 때 함수의 수열들의 수렴에 관한 성질에서는 여러 개의 정의로 다뤄질 수 있다는 점에서 차이가 있다. 가장 널리 알려진 함수의 수렴은 **점별수렴(pointwise convergence)**과 **균등수렴(uniform convergence)**이 있다.

```{definition, name="점별수렴"}
$\{ f_{n}(x)\}_{n=1}^{\infty}$를 실함수의 수열이라고 하자. 그러면 모든 $x\in\mathbb{R}$에 대해
$$\lim_{n\rightarrow \infty}f_{n}(x)=f(x)$$
를 만족하는 실함수 $f$가 존재할 때 수열 $\{ f_{n}(x)\}_{n=1}^{\infty}$이 $f$에 **점별수렴(pointwise convergence)**한다고 한다. 여기서는 $n\rightarrow \infty$일 때
$$f_{n} \stackrel{pw}{\rightarrow} f$$
로 표현하기로 한다.

```

점별수렴에서 수렴 문제는 $x$를 고정된 실수로 놓았을 때 실수열 $\{f_{n}(x)\}_{n=1}^{\infty}$의 수렴 성질을 보는 것으로 줄일 수 있다.

다음은 [@Shima2016]에 나오는 점별수렴하는 함수열의 예제다.

```{definition, name="점별수렴의 예"}
다음과 같은 닫힌 구간 $[0,1]$에서 정의된 함수열 $\{f_{n}\}$
$$f_{n}(x)=x^{n}$$
이 있다고 하자. 이 수열은 다음 함수로 점별수렴한다.
$$
f(x)=\lim_{n\rightarrow\infty}f_{n}(x)=
\begin{cases}
0 & 0\leq x < 1\\
1 & x=1.
\end{cases}
$$

```

```{r, echo=F, fig.cap='점별수렴하는 함수의 예.', fig.align='center'}
knitr::include_graphics("images/math_ptconvergence.png")
```

또한 [@Shima2016]에서 언급한 내용 중 중요한 사실은 점별수렴의 경우에는 앞의 예제에서처럼 원래 함수열들이 모두 연속이라 할지라도 그것의 극한 함수는 연속이 아닐 수도 있다는 점이다. 위 예제에서 $f(x)$는 $x=1$에서 불연속이다(그림). 이 말은 또한 점별수렴에서는 극한의 순서를 바꾸었을 경우 그 결과가 아래 식처럼 서로 다를 수 있음을 의미한다.
$$\lim_{x\rightarrow 1}\lim_{n\rightarrow\infty}f_{n}(x)\neq \lim_{n\rightarrow\infty}\lim_{x\rightarrow 1}f_{n}(x).$$

이런 현상은 함수열 $f_{n}(x)$의 미분가능성과 적분가능성에서도 비슷하게 일어난다. 즉, 점별수렴 하에서는 함수열들이 모두 적분가능하거나 미분가넝하더라고 그것의 극한은 그렇지 않을 수도 있음을 의미한다. 이러한 예제들로 다음 예제들이 있다.

```{definition, name="점별수렴과 적분의 순서"}
다음과 같은 함수
$$f_{n}(x)=nx(1-x^{2})^{n}, \qquad{x\in [0,1]}$$
이 있다고 하자. 여기서는 극한과정 $n\rightarrow\infty$와 적분의 순서를 바꾸면 다른 결과를 줄 것임을 보여줄 것이다. 주어진 함수는 각각의 $n$에 대해 적분 가능하며 다음과 같다.
$$
\begin{align*}
\int_{0}^{1}f_{n}(x)dx &= n \int_{0}^{1}x(1-x^{2})^{n}dx = \Big[\frac{-n}{2(n+1)}(1-x^{2})^{n+1}\Big]_{0}^{1}\\
&= \frac{n}{2(n+1)}\rightarrow \frac{1}{2}.
\end{align*}
$$
  
반대로, 극한은 다음과 같다.
$$f(x)=\lim_{n\rightarrow\infty}f_{n}(x)=0, \qquad{\forall x\in [0,1].}$$

따라서 당연히 $\int_{0}^{1}f(x)dx=0$이다. 그래서 다음과 같은 결론을 내릴 수 있다.
$$\lim_{n\rightarrow\infty}\int_{0}^{1}f_{n}(x)dx\neq \int_{0}^{1}\lim_{n\rightarrow\infty}f_{n}(x)dx.$$

결론은, 점별수렴 과정에서는 극한 과정과 적분의 순서를 바꾸는 것은 안되다는 것이다.
  
```

```{definition, name="점별수렴과 미분의 순서"}
다음과 같은 함수
$$
f_{n}(x)=
\begin{cases}
-1 & x < -\frac{1}{n},\\
\sin(\frac{n\pi x}{2}) & -\frac{1}{n} < x \frac{1}{n},\\
1 & x>\frac{1}{n}.
\end{cases}
$$
우리는 이 함수의 $x=0$에서의 극한 $f(x)=\lim_{n\rightarrow\infty}f_{n}(x)$의 연속성을 점검해 볼 것이다. $f_{n}(x)$는 모든 $x\in \mathbb{R}$에서 미분 가능하므로 당연히 모든 $n$에서 $x=0$일 때 연속이다. 그러나 그것의 극한은
$$
f(x)=
\begin{cases}
-1 & x<0,\\
0 & x=0, \\
1 & x >0
\end{cases}
$$
이며 $x=0$에서 불연속이다. 따라서 점별수렴 과정에서는 극한 과정과 미분의 순서를 바꾸는 것은 안된다.
  
```

점별수렴보다 더 강한 조건으로 $x \in \mathbb{R}$에 상관 없이 모든 지점에서 동시에 $n\rightarrow\infty$일 때 $f_{n}(x) \rightarrow f(x)$이길 요구할 수도 있다. 이 때 사용되는 정의가 균등수렴이다.

```{definition, name="균등수렴"}
$\{ f_{n}(x)\}_{n=1}^{\infty}$를 실함수의 수열이라고 하자. 그러면 모든 $\epsilon > 0$에 대해
$$|f_{n}(x)-f(x)| <\epsilon \forall n \geq n_{\epsilon} \text{ and } x\in \mathbb{R}$$
을 만족시키는 정수 $n_{\epsilon}$이 존재할 때 수열 $\{ f_{n}(x)\}_{n=1}^{\infty}$이 $f$에 **균등수렴(uniform convergence)**한다고 한다. 여기서는 $n\rightarrow \infty$일 때
$$f_{n} \stackrel{u}{\rightarrow} f$$
로 표현하기로 한다.

```

```{r, echo=F, fig.cap='균등수렴하는 함수의 예. 균등수렴에서는 어떤 밴드를 잡았을 때, n을 적당히 조절하여 함수열이 그 밴드 안에 들어가도록 할 수 있다.', fig.align='center'}
knitr::include_graphics("images/math_uniformconvergence.png")
```

균등수렴과 관련된 정리로 어떤 연속함수들의 함수열이 균등수렴이면 그 수열의 극한 또한 반드시 연속이어야 한다는 정리가 있다. 이 정리는 [@Polansky2011]에 나와있다.

```{theorem, name="함수열의 균등수렴과 함수열의 극한의 연속"}
$\{f_{n}(x)\}_{n=1}^{\infty}$가 $\mathbb{R}$의 부분집합 $R$에서 정의된 함수들의 수열이며 함수 $f$로 균등수렴한다고 하자. 만약 각각의 $f_{n}$들이 어떤 점 $x\in R$에서 연속이면, $f$ 또한 점 $x$에서 연속이어야 한다.

```

이 정리의 증명은 Apostol (1974)의 9.4절에서 발견할 수 있다고 한다. 여기서 알아두어야 할 것은 균등수렴은 충분조건이며 필요조건이 아니라는 것이다. 즉

### 수렴하는 수열(convergent sequence)

```{definition, name="(함수해석학에서) 수렴하는 수열"}
노름 벡터 공간 $(\mathcal{F}, \| \cdot\|_{\mathcal{F}})$의 원소들로 이루어진 수열 $\{ f_{n}\}_{n=1}^{\infty}$가 모든 $\epsilon >0$에 대해 $N=N(\epsilon)\in \mathbb{N}$이 존재해 모든 $n \geq \mathbb{N}$에 대해 $\| f_{n}-f\|_{\mathcal{F}} <\epsilon$을 만족할 때, 우리는 수열이 $f\in \mathcal{F}$에 **수렴(converge)** 한다고 한다.

```

### 코시 수열(Cauchy sequence)

```{definition, name="(함수해석학에서) 수렴하는 수열"}
노름 벡터 공간 $(\mathcal{F}, \| \cdot\|_{\mathcal{F}})$의 원소들로 이루어진 수열 $\{ f_{n}\}_{n=1}^{\infty}$가 모든 $\epsilon >0$에 대해 $N=N(\epsilon)\in \mathbb{N}$이 존재해 모든 $m,n \geq \mathbb{N}$에 대해 $\| f_{m}-f_{n}\|_{\mathcal{F}} <\epsilon$을 만족할 때, 수열 $\{ f_{n}\}_{n=1}^{\infty}$를 **코시 수열(Cauchy sequence)**이라고 한다.

```

```{example, name="유리수의 장"}
절대값 $\| \cdot\|$을 노름으로 갖는 유리수 $\mathbb{Q}$의 장(field)은 노름 벡터 공간이다. 여기서 수열 $1, 1.4, 1.41, \ldots$는 $\mathbb{Q}$에서의 코시 수열이나 이것의 극한 $\sqrt{2} \notin \mathbb{Q}$이므로 수렴하는 수열은 아니다.

```

```{example, name="C[0,1]"}
$C[0,1]$을 구간 $[0,1]$에서 bounded continuous인 함수들의 집합이라고 하자. 다음과 같은 노름 $\| f\| =(\int_{0}^{1}|f(x)|^{2}dx)^{1/2}$을 생각할 때 함수들의 수열 $\{ f_{n}\}$은 $[0, \frac{1}{2}-\frac{1}{2n} ]$에서 $0$이고 $[\frac{1}{2}+\frac{1}{2n}, 1]$에서 $1$이기 때문에 코시 수열이나 연속 극한(continuous limit)은 존재하지 않는다.

```

```{r, echo=F, fig.cap='L2 norm에서 연속 극한을 갖지 않는 연속함수들의 코시 수열의 예.', fig.align='center'}
knitr::include_graphics("images/math_counterCauchy.png")
```

## 연속(continuity)

### 립쉬츠 연속(Lipschitz continous)

[@Abbott2015]의 문제 4.4.9를 참고하였다. 어떤 함수 $f:A\rightarrow \mathbb{R}$이 **립쉬츠 연속(Lipschitz continous)**이라는 것은 모든 $x\neq y$에 대해 유계 $M >0$이 존재해
$$|\frac{f(x)-f(y)}{x-y}|\leq M$$
을 만족하는 것이다. 기하학적으로 말하면 어떤 함수 $f$가 립쉬츠라는 것은 $f$의 임의의 두 점을 잡아 기울기를 계산해도 그것을 cover할 수 있는 uniform bound가 존재한다는 것이다. 만약 $f$가 미분 가능할 경우 $x$가 $y$랑 매우 가까워지게 되면 좌변은 미분의 절대값임을 알 수 있다. 그러나 립쉬츠 연속이 $f$가 미분가능함을 말하는 것은 아니다. 단지 $f$가 너무 가파르지 않은 함수라는 것만 알 수 있을 뿐이다.

- 함수 $f:A\rightarrow \mathbb{R}$가 립쉬츠이면 그 함수는 또한 $A$에서 uniformly continuous하다.


