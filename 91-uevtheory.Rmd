# (PART) Extreme Value Statistics {-}

# 일변량 극단값 이론 {#uGEVtheory}

통계학자 존 튜키(John Tukey)는 이렇게 말했다. "나는 모든 지구물리학자들이 실제 오차나 변동성의 분포들이 가우스(Gauss)나 라플라스(Laplace)가 만든 매끄러운 종 모양의 분포보다 훨씬 더 극단값 분포에 가까운 모양을 갖고 있음을 알고 있다고 확신한다."

이 장은 일변량 극단값 분포 이론과 모델링에 관한 내용을 담고 있다. 특히 부분 최댓값
$$M_{n}=\max\{X_{1},\ldots,X_{n}\}$$
의 통계적 움직임에 초점을 맞출 것이다. 여기서 $X_{1},\ldots,X_{n}$은 $F$라는 분포함수를 갖는 독립 확률변수의 수열이다. 일반적으로 $X_{i}$는 일정한 시간 스케일(예: 시간별, 일별, 월별 등)을 갖고 측정된 어떤 (확률)과정의 값들을 나타낸다. 정리하면 $M_{n}$은 $n$ 시간 단위의 관측값 중 최대값을 나타낸다.

```{definition, name="극단값의 예"}
$n$이 일년동한 관찰한 관측값 수라고 할 경우, $M_{n}$은 월 최대값에 대응된다.

```

$X_{i}$들이 독립이라고 가정하였으므로 $M_{n}$의 분포는
$$
\begin{align*}
P\{M_{n} \leq z \} &= P\{X_{1}\leq z, \ldots, X_{n} \leq z\}\\
&=P\{X_{1}\leq z\}\times \cdots \times P\{X_{n}z\}\\
&=\{F(z)\}^{n}.
\end{align*}
$$
가 된다.

그러나 이 식은 실제 도움이 되지 않는다. 왜냐하면 $F$의 진짜 분포가 어떤지를 모르기 때문이다. 여기서 우리는 관찰 데이터를 통해 $F$를 어떤 통계적 방법을 이용해 추정하고 그것으로 $F$를 대체할 가능성이 있는지 고민해볼 수 있다. 그렇지만 $F$의 변화가 아주 작더라도 $F^{n}$은 매우 많이 변하게 된다.

또 다른 접근방법으로는 $F$가 알려져 있지 않다는 것에 동의하고 극단값 자료만 가지고 $F^{n}$에 적합한 근사 모형이 있는지 찾아보는 방법이 있다. 이것은 표본 평균을 **중심극한정리(central limit theorem)**를 가지고 정규분포로 근사하는 것과 같은 방법이다. 즉 우리는 극단값 자료 분석에서도 중심극한정리와 같은 것이 있는지 살펴볼 필요가 있다.

$n \rightarrow \infty$일 때 $F^{n}$의 움직임에 대해 살펴보자. 논리 전개를 위해 다음과 같은 논리가 추가로 더 필요하다. $z_{+}$를 $F(z)=1$로 만드는 가장 작은 값이라고 생각하자. 그러면 모든 $z < z_{+}$에 대해 $n \rightarrow \infty$일 때 $F^{n}(z) \rightarrow 0$이다. 결국 $M_{n}$은 $z_{+}$에서 **점질량(point mass)**을 갖는 **퇴화분포(degenearte distribution)**를 따르게 된다. 퇴화분포는 다루기 어렵기 때문에 다음과 같이 $M_{n}$에 대해 **재정규화(re-normalization)**를 한다.
$$M_{n}^{*}=\frac{M_{n}-b_{n}}{a_{n}}.$$
여기서 $\{a_{n} >0\}$과 $\{b_{n}\}$은 적당한 상수열이다. $\{a_{n}\}$과 $\{b_{n}\}$은 $n$이 커짐에 따라 $M_{n}^{*}$의 위치(location)와 척도(scale)을 안정화시킬 수 있도록 잘 잡아준다.

```{theorem, name="일반화 극단값 분포 정리"}
만약 앞서 말한 상수열 $\{a_{n} >0\}$과 $\{b_{n}\}$이 존재해
$$P\{(M_{n}-b_{n})/a_{n}\leq z\} \rightarrow G(z), \qquad{n \rightarrow \infty}$$
가 성립한다고 가정하자. 여기서 $G$는 퇴화분포가 아닌 어떤 분포함수이다. 이 때, $G$다음 세 개의 족(family) 중 하나를 따른다.

$$\textbf{(Gumbel): }G(z)=\exp\{\exp[-(\frac{z-b}{a})\}, \qquad{-\infty < z < \infty}.$$
$$\textbf{(Frechet): }G(z)=
\begin{cases}
0, & \text{z $\leq$ b}\\
\exp\{-(\frac{z-b}{a})^{-\alpha}\}, & \text{z > b}
\end{cases}$$
$$\textbf{(Weibull): }G(z)=
\begin{cases}
\exp\{-[-(\frac{z-b}{a})^{-\alpha}]\}, & \text{z < b}\\
1, & \text{z $\geq$ b}
\end{cases}$$
이 때 $a>0$, $b$는 모수들이고 $\alpha >0$이다.

```

앞선 정리의 증명 스케치는 [@Coles2001] 50쪽에 잘 나와있다. 또한 증명을 위해 [@Leadbetter1983]을 참고하면 좋다. 증명을 위해서 다음과 같이 **최대안정성(max-stablity)**을 정의한다.

```{definition, name="최대안정성"}
모든 $n=2,3,\ldots,$에서 상수들 $\alpha_{n}>0$, $\beta_{n}$이 존재해 
$$G^{n}(\alpha_{n}z+\beta_{n})=G(z)$$
일 경우 분포 G를 **최대안정(max-stable)**하다고 부른다. 즉 $G$에 거듭제곱을 하는 것은 단지 위치모수와 척도모수의 변화만 야기한다는 것이다.

```

최대안정성과 일반화 극단값 분포 사이에는 다음과 같은 정리가 있다.

```{corollary}
어떤 분포가 최대안정한 것은 분포가 일반화 극단값 분포임과 동치이다. [@Coles2001]

```

```{proof}
[@Coles2001]에 따르면 GEV가 정말로 최대안정임을 보이는 것은 simple algebra로 가능하다고 한다. 그러나 역을 위해서는 함수해석 지식을 요구로 한다.

```

최대안정성의 패러다임은 다음과 같다. 유한한 $n$개의 블록 최댓값을 모델하고 그것의 최대안정 (극한) 분포를 사용하고, 자료의 temporal scale을 넘는 곳까지 외삽할 수 있게 한다.

```{r, message=FALSE, echo=F}
library(evd) #rgev를 위해 필요
```

```{r , fig.align='center', comment=">", echo=F, fig.cap="Plot of three GEV distributions."}
x <- seq(from=-10, to =10, by=0.05)
par(mfrow=c(1,3))
plot(x, dgumbel(x, scale=2), type='l', col="green", lwd=2, ylab='Density', xlab='x', main="Gumbel")
plot(x, dgev(x, scale=2, shape=1), type='l', col="blue", lwd=2, ylab='Density', xlab='x', main="Frechet")
plot(x, dgev(x, scale=2, shape=-1), type='l', col="red", lwd=2, ylab='Density', xlab='x', main="Weibull")
```

위 정리는 재정규화 시킨 표본 최대값들 $M_{n}^{*}=\frac{M_{n}-b_{n}}{a_{n}}.$이 세 가지 분포족 중 하나로 분포수렴할 것임을 알려주고 있다. $M_{n}^{*}$의 극한 분포는 놀랍게도 $F$에 상관없이 항상 저 세가지 분포족 중 하나로 수렴한다. 세 가지 분포의 이름은 앞으로 자주 등장할 것이므로 기억해두자. 참고로 이 적당한 $a_{n}$, $b_{n}$은 존재하지 않을 수도 있다. (예: 포아송 분포)


```{example, name="굼벨분포"}
$X_{1}, X_{2}, \ldots$가 정규지수분포$(=\text{Exp}(1))$에서 추출된 확률변수의 수열이라고 하자. 참고로 $F(x)=1-\exp^{x}$ ($x>0$)이다. 이때 $a_{n}=1$, $b_{n}=\log n$이라고 하면 $n\rightarrow \infty$일 때 고정된 $z\in\mathbb{R}$에 대해

\begin{eqnarray}
P\{(M_{n}-b_{n})/a_{n} \leq z \} &=& F^{n}(z+\log n) \nonumber\\
&=&[1-e^{-(z+\log n)}]^{n} \nonumber\\
&=&[1-n^{-1}e^{-z}]^{n} \nonumber\\
&\rightarrow& \exp(-e^{-z})
\end{eqnarray}

이다. 즉 $a_{n}$과 $b_{n}$을 위와 같이 선택하였을 때 $M_{n}$의 극한분포는 굼벨분포가 된다.

```

```{example, name="굼벨분포로의 수렴속도"}
$X_{1}, X_{2}, \ldots$가 표준정규분포에서 추출된 확률변수의 수열이라고 하자. 이때 
$$a_{n}=(2\log n)^{-0.5}, b_{n}=(2\log n)^{0.5}-0.5(2\log n)^{-0.5}(\log\log n +log 4\pi)$$
로 놓을 경우 재정규화된 $M_{n}$ 또한 굼벨분포로 수렴함이 알려져 있다. 그러나 앞 예제와 비교하였을 때 수렴 속도는 현저히 느리다.

```

$M_{n}$의 수렴속도를 체크하는 것은 중요한 일이다. 왜냐면 우리는 일반화 극단값 분포를 유한개의 표본 최대값들의 점근 분포로 생각하고 사용할 것이기 때문이다. 결국 일반화 극단값 분포를 사용하기 위해 얼마나 많은 데이터가 필요할 것인가라는 문제는 $M_{n}$의 수렴속도와 관계된다.

## 흡인영역(domain of attraction)

이 부분은 [@Huser2013]을 참고하였다. Extremal types 정리는 i.i.d 확률변소의 renormalized maxima의 가능한 세 개의 극한 법칙을 구분해준다. 더 나아가, 만약 수열 $a_{n}>0$, $b_{n}\in\mathbb{R}$이 존재해 $(M_{n}-b_{n})/a_{n}$이 non-degenerate 분포함수 G에 수렴한다면, G는 affine 변환(아핀 변환은 직선을 보존하며, 나아가 직선 위의 점들의 내분비, 외분비를 보존한다. 예를 들어 선분과 그의 중점은 변환 후 여전히 선분과 중점이다.)에 의해 유일하게 정의된다. 다시 말하면, 만약 다음과 같은 수열 $a_{n}'>0$, $b_{n'}\in\mathbb{R}$이 존재해 $(M_{n}-b_{n}')/a_{n}'\stackrel{D}{\rightarrow}G'$라면 $G$와 $G'$는 항상 같은 타입이어야 한다는 것이다. 따라서 우리는 어떤 분포의 최대값이 특별한 극한 법칙으로 흡인되는지에 따라 분포의 class들을 **흡인영역(domain of attraction, MDA)**으로 정의할 수 있다.

```{definition, name="흡인영역"}
확률변수 $Y$에 대해 다음과 같은 상수 $a_{n}>0$, $b_{n}\in\mathbb{R}$가 존재해 $(M_{n}-b_{n})/a_{n}\stackrel{D}{\rightarrow}G$가 될 경우 우리는 $Y$가 극단값 분포 $G$의 **흡인영역(domain of attraction, MDA)**에 속한다고 하고 $Y\in\text{MDA}(G)$라고 쓴다.

```

이 흡인영역을 묘사하는 것에 대해서는 그동안 많은 연구가 진행되어왔다. **tail-equivalent distributions**라는 개념은 세 가지 타입의 극단값 분포의 MDA를 묘사하기 전에 먼제 설명하는 것이 필요하다.

```{definition, name="Tail equivalence"}
두 분포함수 $F_{1}$, $F_{2}$가 같은 right endpoint $y_{F_{1}}=y_{F_{2}}=y_{F}$를 갖고 어떤 양의 상수 $0<c<\infty$에 대해
$$\lim_{y\rightarrow y_{F}}\frac{1-F_{1}(y)}{1-F_{2}(y)}=c$$
를 만족할 경우 두 분포함수는 **tail-equivalent**하다고 한다.

```

각각의 MDA은 tail-equivalence에 대해 닫혀있다. 즉 $F_{1}$, $F_{2}$가 tail-equivalent이면 $F_{1}\in \text{MDA}(G)$는 $F_{2}\in \text{MDA}(G)$와 동치다. 즉 MDA는 riaght tail이 right end point에서 같은비율로 감소하는 분포들로 구성된다.

```{theorem, name="MDA의 chracterization"}
$F$가 upper endpoint $y_{F}$를 갖는 분포함수라고 하자. 그리고 $\sim$을 asymptotic equivalence를 나타내는 기호라고 하자. 그러면 어떤 $\alpha >0$과 $K\in\mathbb{R}$에 대해 다음 설정들은 참이 된다.

- $F\in\text{MDA}(\Phi_{\alpha})$는 $y\rightarrow y_{F}=\infty$임에 따라 $1-F(y)\sim Ky^{-\alpha}$임과 동치다.

- $F\in\text{MDA}(\Psi_{\alpha})$는 $y\rightarrow y_{F}<\infty$임에 따라 $1-F(y)\sim K(y_{F}-y)^{-\alpha}$임과 동치다.

- $F\in\text{MDA}(\Lambda)$는 $F(y)$가 $1-F(y)=c(y)\exp[-\int_{z}^{y}\{1/a(t)\}dt], z<y<y_{F}$를 만족하는 표현을 갖도록 하는 어떤 $z<y_{F}\leq \infty$가 존재함과 동치다. 여기서 $c$는 가측함수이며 $y\rightarrow y_{F}$일 때 $c(y)\rightarrow c>0$이고, $a(y)$는 양의 값을 갖는 (르베그 측도에 대해) absolutely continubous functions이고 density $a'(y)$가 $\lim_{y\rightarrow y_{F}}a'(y)=0$이다.

```

Right endpoint에서 충분해 부드러운 분포의 경우에는 von Mises가 $(M_{n}-b_{n})/a_{n}\rightarrow Z \sim \text{GEV}(0,1,\xi)$로 수렴하는 충분조건들을 발표했다. 이것은 극한 분포의 타입과 normalizing 상수들 $a_{n}>0$, $b_{n}$을 결정하는 데 큰 도움을 준다. 이러한 조건들은 알려진 분포들을 다른 MDA로 분류하는 데 사용된다.

```{proposition, name="von Mises conditions"}
$F$가 right endpoint $y_{F}$를 갖는 분포함수고 $z<y_{F}$가 존재해 $F$가 $(z,y_{F})$에서 두번 미분가능하다고 가정하자. $f=F'$가 $(z,y_{F})$에서 $F$의 밀도라고 하자. 실수들의 수열 $b_{n}=F^{\leftarrow}(1-1/n)과 $a_{n}=r(b_{n})$을 정의하자. 이 때 $r(y)=\{1-F(y)\}/f(y)$가 $F$의 reciprocal harzard function이다. 더 나아가 $\xi=\lim_{y\rightarrow y_{F}}r'(y)$라고 하자. 그러면 $(M_{n}-b_{n})/a_{n}\stackrel{D}{\rightarrow} Z\sim \text{GEV}(0,1,\xi)$이다. 즉 $\xi>0$이면 $F\in \text{MDA}(\Phi_{1/\xi})$이고 $\xi<0$이면 $F\in MDA(\Psi_{-1/\xi})$이고 $\xi\rightarrow 0$이면 $F\in \text{MDA}(\Lambda)$이다.

```

다음은 [@Huser2013]에 있는 흡인영역에 따른 분포들의 분류다.

- $F\in\text{MDA}(\Phi_{\alpha})$: 이들은 Fr\'{e}chet 분포의 MDA이며 Fr\'{e}chet, Pareto, Cauchy, Loggamma, Student-t 분포 등이 속한다.

- $F\in\text{MDA}(\Psi_{\alpha})$: 이들은 reversed Weibull 분포의 MDA이며 Uniform, Beta 그리고 $y_{F}<\infty$에서 power-law behavior를 갖는 모든 분포들이 속한다.

- $F\in\text{MDA}(\Lambda)$: 이들은 Gumbel 분포의 MDA이며 Normal, Gamma, Log-normal, Weibull, Exponential 그리고 모든 tail-equivalent distn들이 속한다.

그러나 maxima의 non-degenerate limiting distribution으로 이끄는 affine normalization이 항상 존재하지 않을 수도 있음을 항상 생각해야한다. 즉 어느 흡인영역에도 속하지 않는 분포가 존재한다는 것이다. 포아송, Geometric, negative binomial 분포 등이 대표적이다. 이러한 분포들은 right tail에서 not well-behaved하는 분포라고 생각하면 된다.

다음에 나오는 결과들은 non-degenerate 분포로의 maxima의 convergence는 right endpoint에서 어떤 continuity condition을 갖을 때에만 일어날 수 있으며, 이런 조건은 앞서 언급한 이산분포에서는 만족하지 못하는 것들이다.

```{proposition}
$u_{n}$이 실수들의 수열이라고 하자. 그러면 모든 $0\leq \lambda \leq \infty$에 대해 두 가정이 동치이다.

1. $n\{1-F(u_{n}) \}\rightarrow \lambda$.

2. $P(M_{n}\leq u_{n}) \rightarrow \exp(-\lambda).$

```

```{theorem, name="Right endpoint에서의 continuity"}
$0<\lambda <\infty$라고 하자. 그러면 $n\{1-F(u_{n})\}\rightarrow\lambda$와 
$$\lim_{y\rightarrow y_{F}}\frac{\{1-F(y)\}}{(1-F_{y_{\cdot}})}=1$$
을 동치로 만드는 수열 $u_{n}$이 존재한다. 이 때 $F_{y_{\cdot}}=\lim_{x\uparrow y }F(x)$이다.

```

$\mathbb{Z}$에서 정의된 무한대의 rightendpoint를 갖는 분포에 대해 조건 $\lim_{y\rightarrow y_{F}}\frac{\{1-F(y)\}}{(1-F_{y_{\cdot}})}=1$은 $k\rightarrow \infty$일 때 $\frac{1-F(k)}{1-F(k-1)}\rightarrow 1$로 바꿀 수 있다. 포아송 분포의 경우에는 이 극한이 0이되며, maxima가 수렴하는 것을 막게 된다. 

Non-degenerate limit distribution for maxima가 존재하지 않는 또 다른 예로 super-heavy-tailed 분포가 있다. 예를 들어 분포함수 $F(y)=1-1/\log(y), y>e$는 매우 천천히 감소하여 적당한 linear normalizing constants $a_{n}>0$, $b_{n}$을 찾을 수 없을지도 모른다.

## 일반화 극단값 분포(generalized extreme value distribution)

앞서 말한 $G$의 극한값에서의 행동은 분포에 따라 달라진다. 예를 들면, 굼벨분포와 프레셰분포는 $z_{+}=\infty$이나 와이블분포는 $z_{+}<\infty$이다. 그리고 굼벨분포에서는 $G$의 분포가 지수적으로 감소(exponentially decay)하나 프레셰분포에서는 다항식 차수로 감소(polynomially decay)한다. 즉 족이 달라지면 극단값의 움직임 또한 다르게 표현될 것임을 알려준다.

그런데 이렇게 족 별로 따로 나누어 분석하는 것은 두 가지 문제점을 지닌다. 첫째, 데이터를 보고 어떤 족으로 분석하는 것이 적절한지 미리 정해야 한다. 둘째, 이러한 결정 후에는 이 결정이 맞다는 전제 하에 추론이 진행되고 이 선택이 가지는 불확실성을 배제하게 된다. 따라서 일반적으로는 앞 정리에 나왔던 식을 재구성하여 다음과 같이 하나의 함수로 표현하여 분석하게 된다.
$$G(z)=\exp\{-[1+\xi(\frac{z-\mu}{\sigma})]^{-1/\xi}\}.$$
이 때 $\{z: 1+\xi(z-\mu)/sigma >0\}$이며 $-\infty < \mu < \infty$, $\sigma >0$ 그리고 $-\infty <\xi <\infty$를 만족한다. 이 분포를 **일반화 극단값 분포(generalized extreme value distribution)**이라고 부른다. 여기서 $\mu$는 **위치모수(location parameter)**, $\sigma$는 **척도모수(scale parameter)**, 그리고 $\xi$는 **형태모수(shape parameter)**라고 한다. 특히 $\xi$는 이 분포가 굼벨족($\xi=0$), 프레셰족($\xi>0$) 또는 와이블족($\xi<0$)이 될지 결정하는 역할을 한다.

## 복귀 수준(return level)

앞서 극단값 분석은 극단적인 사건들을 모델링하고 적합한 위험 평가를 해 줄 수 있는 통계적 방법을 제공한다고 설명하였다. 그렇다면 우리는 어떤 방법을 통해 이 **위험(risk)**를 측정할 수 있을까?

한 가지 방법으로 **복귀 수준(return level)**이라는 것이 있다. **n-복귀 수준**은 우리가 매 $n$년마다 한 번꼴로 초과될 것으로 기대되는 값이다. 다시 말하면 어떤 특정한 해에 그런 강도의 사건을 마주칠 확률이 $\frac{1}{n}$ 정도 될 때 $n$-복귀 수준이 된다. 분위수(quantile)로 봤을 때에는 $1-\frac{1}{n}$분위수에 대응되는 사건이다.

```{example, name="복귀 수준"}
20년 복귀 수준은 매 20년마다 최대 수준이 한 번꼴로 초과할 것으로 기대되는 수준에 대응된다. 이것은 매 년 $\frac{1}{20}=0.05$의 확률로 그러한 사건이 마주칠 것으로 기대되며 $1-\frac{1}{20}=0.95$의 분위수에 대응된다.

```

일반적으로 많이 쓰이는 복귀 수준은 $20, 50, 100$년 정도이다.

이 복귀 수준이라는 개념은 통계학에서 말하는 분위수와 일맥상통한다. $0<p<1$이라 할 때, 일반화 극단값 분포의 분위수 $x_{p}$는
$$x_{p}=\mu-\frac{\sigma}{\xi}[1-\{-\log(1-p)\}^{-\xi}]$$
가 된다. 이것은 $G(x_{p})=1-p$라 놓고 $x_{p}$에 대해 풀어 얻을 수 있다. 이 때 $x_{p}$를 $1/p$ **복귀 주기(return period)**에 대응되는 복귀 수준이라 부른다.

## 극단값 분포에서의 추론(inference in extreme value statistics)

$k$개의 연 최대값 $X_{1}, \ldots , X_{k}$들이 주어져 있을 때, 일반화 극단값 분포의 모수들 $(\mu, \sigma, \xi)$을 추론하는 문제를 생각해보자. 가능한 추론 방법들은 다음과 같다.

- **그래프 기반 방법**: 전통적으로 중요했고, 지금도 기본적인 모형 파악에 유용하다.

- **적률 기반(moment-based) 추정량**: 적률이 존재하지 않을 가능성이 있어 극단값 분석에 유용하지 않은 경우가 많다.

- **확률 가중 적률**: 수문학(hydrology)에서 많이 쓰이나 복잡한 자료로 확장하기 어렵다.

- **가능도(likelihood) 기반 방법**: 통계학에서 가장 많이 이용하는 방법이다. 복잡한 자료에 적용할 수 있고 일반적인 점근적 추정과 검정이론을 적용할 수 있다. 또한 베이지안 방법을 적용할 수도 있다.

일반화 극단값 분포를 가능도 기반 방법으로 분석할 때 정칙 조건(regularity condition)을 만족하는지 항상 확인해야 한다. 일반화 극단값 분포의 모수들로 표현된 값 $\mu-\sigma/\xi$이 $\xi\neq 0$일때 분포의 끝점(end point)이 되기 때문에 정칙 조건을 만족하지 않을 수도 있다.[@Smith1985]

한편 형태모수 값과 최대가능도추정량의 존재 사이의 관계에 대해 다음과 같은 사실이 알려져 있다.

- $\xi > -0.5$인 경우 최대가능도추정량은 정칙조건을 만족하며 일반적인 점근적 성질을 갖는다.

- $-1 < \xi < -0.5$인 경우 최대가능도추정량을 얻을 수는 있으나 일반적인 점근적 성질을 갖지는 않는다.

- $\xi < -1$인 경우 최대가능도추정량을 얻지 못할 가능성이 높다.

## 극단값 분포에서의 최대가능도추정(mle in extreme value statistics)

일반화 극단값 분포에 최대가능도추정을 적용하기 전에 일반적인 가능도 관련 이론에 대해 정리해보자.

```{definition, name="가능도와 최대가능도추정량"}
$y$가 자료이고 이 자료가 다음과 같은 모수 $\theta$로 정의되는 확률변수 $Y \sim f(y;\theta)$의 실현(realization)이라고 하자. 이때 **가능도(likelihood, 우도)**와 로그 가능도는
$$L(\theta)=L(\theta;y)=f_{Y}(y;\theta), l(\theta)=\log L(\theta), \theta \in \Omega_{\theta}$$
로 정의된다.

**최대가능도추정량(maximum likelihood estimate, MLE)**은
$$l(\hat{\theta})\geq l(\theta), \forall \theta \in \Omega_{\theta}$$
을 만족시키는 $\hat{\theta}$이다. 많은 경우 $\hat{\theta}$는 유일하며 다음과 같은 **점수방정식(score equation, 또는 가능도방정식)**
$$\frac{\partial l(\theta)}{\partial \theta}=0$$
을 만족한다. 만약 $\theta$가 $p \times 1$ 벡터라면 점수방정식 또한 $p \times 1$ 벡터가 된다.

**관측정보행렬(observed information matrix)**는 점수방정식을 한 번 더 미분한 것이다.
$$J(\theta)=-\frac{\partial^{2}l(\theta)}{\partial \theta \partial \theta^{T}}.$$
만약 $\theta$가 $p \times 1$ 벡터라면 $J(\theta)$는 $p \times p$ 행렬이 된다.

**가능도 통계량(likelihood ratio statistic, 윌크스의 통계량)**은
$$W(\theta)=2\{l(\hat{\theta})-l(\theta)\}\geq 0$$
이다.

```

가능도 근사(likelihood approximation)은 $n$이 클때, 데이터가 $f(y,\theta^{0})$에서 생성되었다고 가정하면
$$\hat{\theta} \stackrel{\cdot}{\sim} \mathcal{N}_{p}(\theta^{0}, J(\hat{\theta})^{-1})$$
로 근사하는 것이다. 이때 $\hat{\theta}$, $J(\hat{\theta})$는 보통 수치적으로 계산하게 된다. 그리고 검정통계량은 $n$이 작을지라도
$$W(\theta^{0}) \stackrel{\cdot}{\sim} \xi_{p}^{2}$$
근사를 이용한다.

실제로는 $\theta$를 **흥미있는 모수들(interest parameters)** $\psi_{q\times 1}$와 **장애모수들(nuisance parameters)** $\lambda_{p-q\times 1}$로 나누어 분석하게 된다. $\hat{\lambda}_{\psi}$를 $\psi$가 고정되어 있을 때 $\lambda$의 최대가능도추정량이라고 하자. 그러면 **일반화가능도통계량(generalized likelihood ratio statistic)**을
$$W_{p}(\psi)=2\{l(\hat{\psi},\hat{\lambda})-l(\psi,\hat{\lambda}_{\psi})\}=2\{l(\hat{\theta})-l(\hat{\theta}_{\psi})\}$$
로 정의한다. 이 때 다음과 같은 통계량을 정의할 수 있다.

```{definition, name="일반화가능도통계량"}
**일반화가능도통계량(generalized likelihood ratio statistic)**은
$$W_{p}(\psi)=2\{l(\hat{\psi},\hat{\lambda})-l(\psi,\hat{\lambda}_{\psi})\}=2\{l(\hat{\theta})-l(\hat{\theta}_{\psi})\}$$
이다.

```

만약 $\psi^{0}$이 정칙모형(regular model)의 자료로부터 얻은 $\psi$값이라고 하면 앞선 정의에 의해
$$W_{p}(\psi) \stackrel{\cdot}{\sim} \xi_{q}^{2}, \qquad{for large n}$$
이 된다. $\psi^{0}$에 대한 신뢰구간(confidence interval)과 검정은 이것에 기초해서 만들게 된다.

## 극단값 분포에서의 프로파일 가능도(profile likelihood in extreme value statistics)

앞선 상황처럼 알려지지 않은 모수를 흥미있는 모수들과 장애모수들로 나눌 수 있고 이 둘을 모두 추정해야 하나 흥미있는 모수들에만 관심이 있을 경우 사용할 수 있는 것 중의 하나가 **프로파일 가능도(profile likelihood)**이다.

```{definition, name="프로파일 가능도"}
$\theta$를 모수 벡터라 하고 $\theta_{i}$를 제외한 모든 $\theta$의 원소들의 모임을 $\theta_{-i}$라고 했을 때 $\theta_{i}$에 대한 **프로파일 로그 가능도(profile log-likelihood)**는
$$l_{p}(\theta_{i})=\max_{\theta_{-i}}l(\theta_{i},\theta_{-i})$$
이다.

```

```{definition, name="프로파일 로그 가능도"}
$\theta$를 모수 벡터라 하고 이것을 $(\theta^{(1)}, \theta^{(2)})$로 분할할 수 있다고 하자. 여기서 $\theta^{(1)}$은 $k$차원 흥미있는 모수들의 벡터이며 $\theta^{(2)}$는 나머지들의 $(d-k)$차원 벡터이다. 이때 $\theta^{(1)}$에 대한 **프로파일 로그 가능도(profile log-likelihood)**는
$$l_{p}(\theta^{1})=\max_{\theta^{(2)}}l(\theta^{(1)},\theta^{(2)})$$
이다.

```

정칙 조건 하에서 프로파일 로그 가능도를 이용한 통계량은 다음과 같은 점근분포를 갖음이 알려져 있다.[@Coles2001]

```{theorem, name="프로파일 로그 가능도를 이용한 통계량의 점근분포"}
$x_{1}, \ldots , x_{n}$을 모수분포 $\mathcal{F}$ 안에 있는 어떤 분포의 독립적인 실현이라고 하자. $\hat{\theta}_{0}$을 $d$차원 모수벡터 $\theta_{0}=(\theta^{(1)},\theta^{(2)})$의 최대가능도추정량이라고 하자. 이때 $\theta^{(1)}$의 길이를 $k$라고 하자. 그러면 정칙 조건 하에서 $n$이 클 때
$$D_{p}(\theta^{(1)})=2\{l(\hat{\theta}_{0})-l_{p}(\theta^{(1)}) \} \stackrel{\cdot}{\sim} \chi_{k}^{2}$$
이다.

```

위 정리는 크게 두 가지 상황에서 쓰인다. 첫째, 길이 1의 모수 $\theta_{i}$에 대해 $(1-\alpha)$ 신뢰구간 $C_{\alpha}=\{\theta_{i}: D_{p}(\theta_{i})\leq c_{\alpha}\}$를 계산할 때 쓴다. 여기서 $c_{\alpha}$는 $\xi_{1}^{2}$ 분포의 $(1-\alpha)$ 분위수를 뜻한다. 두 번째 응용은 모형 선택이다.

## 확률가중적률 또는 L-적률 (probability weighted moments or L-moments)

[@Dey2015]의 1장을 참고하였다. 분포함수 $F$를 갖는 확률변수 $X$의 **확률가중적률(probability weighted moment)**는 다음과 같이 정의된다.
$$M_{p,r,s}=E\{ X^{p}F^{r}(X)[1-F(X)]^{s} \}.$$
여기서 $p,r,s$는 실수이다. 이러한 양은 $F$의 분위수 함수 $Q$가 closed form을 갖을 때 잘 계산된다.
$$M_{p,r,s} = \int_{0}^{1}Q^{p}(u)u^{r}(1-u)^{s}du.$$
$M_{p,0,0}$은 $X$의 $p$차 적률이며 $\xi < 1/\xi$일 때에만 존재한다. 이러한 제한은 GEV 모수 추정에서 method of moment를 사용하는 것이 매력적이지 않게 한다. [@Hosking1985]는 양 $\beta_{r}=M_{1,r,0},r=0,1,2,\ldots,$을 조사하였고 이들이 평균만 존재하면(즉 $\xi<1$) 모수를 추정하는데 이들의 표본을 사용할 수 있다는 것을 밝혔다. Closed-form quantile function이 존재할 때, [@Hosking1985]에서의 $\beta_{r}$은 다음과 같다.
$$\beta_{r}=\frac{1}{r+1}\Big[ \mu-\frac{\sigma}{\xi}\{ 1-(r+1)^{\xi}\Gamma(1-\xi) \} \Big],\qquad{\xi < 1}.$$
$r\in\{0,1,2\}$일 때 이것은 또한 다음들을 유도한다.

\begin{equation}
\beta_{0}=\mu -\frac{\sigma}{\xi}(1-\Gamma(1-\xi))
(\#eq:PWM01)
\end{equation}

\begin{equation}
2\beta_{1}-\beta_{0}=\frac{\sigma}{\xi}(1-\xi)(2^{\xi}-1)
(\#eq:PWM02)
\end{equation}

\begin{equation}
\frac{3\beta_{2}-\beta_{0}}{2\beta_{1}-\beta_{0}}=\frac{3^{\xi}-1}{2^{\xi}-1}.
(\#eq:PWM03)
\end{equation}

$\beta_{r}$의 추정은 순서통계량 $Y_{1:m},\ldots, Y_{m:m}$에 기반하여 편리하게 계산할 수 있다. $\beta_{r}$의 불편추정량은 다음과 같다.
$$b_{r}=\frac{1}{m}\sum_{j=1}^{m}\Big(\prod_{l=1}^{r}\frac{(j-l)}{(m-l)}\Big)Y_{j:m}.$$
또는 asymptotically equivalent version으로 $\hat{\beta}_{r}$이 있다.
$$\hat{\beta}_{r} =\frac{1}{m}\sum_{j=1}^{m}(\frac{j}{m+1})^{r}Y_{j:m}.$$

**확률가중적률 추정량(probability weighted moment estimator)**는 식 \@ref(eq:PWM01), \@ref(eq:PWM02), \@ref(eq:PWM03)에 있는 $\beta_{r}$을 $b_{r}$ 또는 $\hat{\beta}_{r}$로 대체한 것이다. [@Hosking1985]는 $\beta_{r}$이 $b_{r}$로 교체될 경우, PWM 추정량은 feasibility criterion ($\hat{\xi}<1$, $\hat{\sigma}>0$ almost surely)을 만족함을 보였다.

## 정칙 변동 (regular variation)

정칙 변동 함수 이론은 두꺼운 꼬리, 장기기억 상관관계(long-range dependence) 그리고 흡인영역(domain of attraction)을 다룰 때 필수적인 이론이다. [@Resnick2007] 거칠게 얘기하자면 **정칙 변동 함수들(regularly varying functions)**은 점근적으로 멱함수(power function)처럼 행동하는 함수들을 말한다. 여기서는 단순함을 위해 일변량, 실함수만을 생각하기로 한다.

```{definition, name="정칙 변동 함수"}
어떤 가측 함수 $U: \mathbb{R}_{+}\rightarrow \mathbb{R}_{+}$가 만약 $x>0$에 대해
$$\lim_{t\rightarrow\infty}\frac{U(tx)}{U(x)}=x^{\rho}$$
을 만족한다면 이 함수를 $\infty$에서 **정칙 변동(regularly varying)**이라고 부른다. 그리고 $U\in \text{RV}_{\rho}$와 같이 쓴다. 이 때 index $\rho\in\mathbb{R}$을 **변동지수(exponent of variation)**이라고 부른다.

```

```{definition, name="천천히 변하는 함수"}
앞선 정칙 변동 함수에서 $\rho=0$인 경우에 $U$를 특별히 **천천히 변하는 함수(slowly varying function)**이라고 부른다. 그리고 $U\in \text{RV}_{0}$ 또는 $U\in \text{SV}$로 표현하기로 한다.

```

천천히 변하는 함수는 일반적으로 $L(x)$라고 쓴다. 만약 $U\in\text{RV}_{\rho}$라고 하면, $\frac{U(x)}{x^{\rho}}\in\text{RV}_{0}$이 되고, $L(x)=\frac{U(x)}{x^{\rho}}$라고 둔다면 $\rho$-varying 함수들을 항상 $x^{\rho}L(x)$의 꼴로 나타낼 수 있음을 알 수 있다.

다음은 [@Gut2012] Appendix에 나오는 정칙 변동 함수들의 예다.

```{example, name="정칙 변동 함수들의 예"}
다음 함수들은 정칙 변동이다.
$$x^{rho}, x^{rho}\log^{+}(x), x^{rho}\log^{+}\log^{+}(x), x^{\rho}\frac{\log^{+}(x)}{\log^{+}\log^{+}x}.$$
이 때 $\log^{+}(x)=\max \{1,\log(x)\}$이다.
  
```

전형적인 천천히 변하는 함수들의 예로는 앞선 예제에서 $\rho=0$으로 한 것들을 들 수 있다. 또한 $x\rightarrow\infty$일 때 양의 유한한 극한으로 수렴하는 모든 양의 함수들은 천천히 변한다. [@Gut2012] 

다음은 [@Resnick2007] 2장에 나오는 정칙 변동하지 않는 함수들의 예다.

```{example, name="정칙 변동하지 않는 함수들의 예"}
다음 함수들은 정칙 변동이 아니다.
$$e^{x}, \sin(x+2).$$
  
```

$[\log x ]$의 경우는 천천히 변하나 $\exp \{ [\log x ]\}$는 정칙 변동을 하지 않는다. [@Resnick2007]

다음은 [@Gut2012]에 나오는 정칙 변동 함수들에 대한 따름정리이다.

```{lemma, name="정칙 변동 함수들에 관한 따름정리들"}
함수 $U$가 $U\in \text{RV}_{\rho}$이고 양의 공간에서 양의 함수라고 하자.

1. 만약 $-\infty < \rho < \infty$이면 $U(x)=x^{\rho}l(x)$이며 이 때 $l\in\text{SV}$이다. 더불어 $U$가 monotone derivative $U'$를 갖을 경우
$$\frac{xU'(x)}{U(x)}\rightarrow \rho \qquad{\text{ as } x\rightarrow\infty}.$$
더불어 $\rho \neq 0$일 경우 $sgn(U)\cdot U' \in \text{RV}_{\rho-1}$이다.

2. $\rho >0$일 때 $U^{-1}(y)=\inf\{x: U(x)\geq y\}, y >0$이라고 하자. 그러면 $U^{-1}\in\text{RV}_{1/\rho}$이다.

3. $\log U \in \text{SV}$이다.

4. $U_{i}\in\text{RV}_{\rho_{i}}, i=1,2$라고 하자. 그러면 $U_{1}+U_{2}\in\text{RV}_{(\max \{\rho_{1}, \rho_{2} \})}$이다.

5. $U_{i}\in\text{RV}_{\rho_{i}}, i=1,2$라고 하자. 그리고 $x\rightarrow\infty$일 때 $U_{2}(x)\rightarrow\infty$라고 가정하자. $U(x)=U_{1}(U_{2}(x))$라고 둔다. 그러면 $U\in\text{RV}(\rho_{1}\cdot\rho_{2})$이다. 특별히 $U_{1}, U_{2}$ 둘 중 하나가 천천히 변할 경우 composition 또한 천천히 변한다.

```

```{proof}
1과 2의 경우 증명이 쉽지 않다. 3의 경우 $t\rightarrow\infty$일 때 $\frac{U(tx)}{U(t)}\rightarrow x^{\rho}$이 된다는 사실을 이용해
$$\frac{\log U(tx)}{\log U(t)}=\frac{\log \frac{U(tx)}{U(t)}}{\log U(t)} + 1 \rightarrow 0 + 1 \qquad{ \text{ as } t\rightarrow \infty}$$
를 얻는다. 4는 $\rho_{1}>\rho_{2}$라고 가정할 때 다음과 같이 유도한다.
$$
\begin{align}
&\frac{U_{1}(tx)+U_{2}(tx)}{U_{1}(t)+U_{2}(t)} \\
&= \frac{U_{1}(tx)}{U_{1}(t)}\cdot \frac{U_{1}(t)}{U_{1}(t)+U_{2}(t)} + \frac{U_{2}(tx)}{U_{2}(t)}\cdot \frac{U_{2}(t)}{U_{1}(t)+U_{2}(t)}\\
&\rightarrow x^{\rho_{1}}\cdot 1 + x^{\rho_{2}}\cdot 0\\
&= x^{\rho_{1}} \text{ as } t \rightarrow\infty.
\end{align}
$$
5는 밑의 정칙 변동 함수의 비의 수렴에 관한 따름정리를 적용하면 $t\rightarrow\infty$일 때
$$
\begin{align}
&\frac{U(tx)}{U(t)}\\
&= \frac{U_{1}(U_{2}(tx))}{U_{1}(U_{2}(tx))}\\
&= \frac{U_{1}(\frac{U_{2}(tx)}{U_{2}(t)}\cdot U_{2}(t) )}{U_{1}(U_{2}(tx))}\\
&\rightarrow (x^{\rho_{2}})^{\rho_{1}} = x^{\rho_{1}\cdot \rho_{2}}
\end{align}
$$
를 얻는다. 참고로 5는 3을 포함한다.

```

정칙 변동의 정의에 따라 정칙 변동 함수와 천천히 변하는 함수의 비는 상수다. 한편, 이 비가 상수로 수렴한다면 극한도 똑같이 남게 된다.

```{lemma, name="정칙 변동 함수의 비의 수렴에 관한 따름정리"}
함수 $U$가 $U\in \text{RV}_{\rho}$이고 $-\infty < \rho < \infty$라고 하자. 그리고 $\rho=0$일 때에는 $U$가 (ultimately) monotone이라고 가정하자. 더 나아가 $n\geq 1$일 때 $a_{n}, b_{n}\in\mathbb{R}^{+}$가 존재해 다음을 만족한다고 가정하자.
$$a_{n}, b_{n}\rightarrow \infty,$$
$$\frac{a_{n}}{b_{n}}\rightarrow c \text{ as } n \rightarrow\infty, \text{ where } c\in(0,\infty).$$
그러면 $n\rightarrow\infty$일 때
$$
\frac{U(a_{n})}{U(b_{n})}\rightarrow
\begin{cases}
1, & \rho=0\\
c^{\rho}, & \rho\neq 0
\end{cases}
$$
이다.

```

```{proof}
$\rho >0$이라고 가정하자. 그러면 $U$는 궁극적으로 non-decreasing이다. $\varepsilon >0$이라고 하고 $n$을 충분히 크게 잡아 다음을 만족하도록 한다.
$$b_{n}(c-\varepsilon) < a_{n} < b_{n}(c+\varepsilon).$$
그러면 $n\rightarrow \infty$함에 따라 extreme member들이 $(c\pm \varepsilon)^{\rho}$로 수렴한다는 사실로부터 다음의 결과를 얻을 수 있다.
$$\frac{ U(b_{n}(c-\varepsilon))}{U(b_{n})} < \frac{U(a_{n})}{U(b_{n})} < \frac{U(b_{n}(c+\varepsilon))}{U(b_{n})}.$$
$\rho <0$인 경우도 비슷하게 얻을 수 있고, 부등호의 방향이 달라진다. $\rho=0$인 경우에는 $c^{\rho}$가 항상 1이 된다.

```

함수 $U$가 $U\in \text{RV}_{\rho}$이고 $\rho >-1$인 경우를 생각해보자. 그러면 천천히 변하는 함수는 $x^{\rho}$에 비해 무시가능한 부분이기 때문에 정칙 변동 함수 $U$의 적분은 $\rho+1$의 정칙 변동 함수일 것이라 추론해 보는 것이 타당하다. 이는 사실로, 다음의 따름정리에 나와 있다.

```{lemma, name="정칙 변동 함수의 적분"}
$\rho>-1$이라고 하자.

1. 만약 $U\in \text{RV}_{\rho}$이면 $U(x)=\int_{a}^{x}U(y)dy \in \text{RV}_{\rho +1}$이다.

2. 만약 $L\in \text{SV}$이면 $n\rightarrow \infty$일 때 $\sum_{j\leq n}j^{\rho}L(j) \sim \frac{1}{\rho + 1}n^{\rho + 1}L(n)$이다. 

```

천천히 변하는 함수와 반대되는 빠른 변동 함수도 있다. 이는 $\rho=+\infty$에 대응되는 개념이다.

```{definition, name="빠른 변동 함수"}
어떤 함수 $U$가 만약
$$
\frac{U(tx)}{U(t)}\rightarrow
\begin{cases}
0, & 0<x<1\\
\infty, & x>1
\end{cases}
$$
라면 $\infty$에서 **빠른 변동(rapid varying)**이라고 부른다.

```

빠른 변동의 의미는 무한대에서 어떤 제곱배 함수보다도 빨리 증가한다는 의미다. 지수함수 $e^{x}$가 좋은 예다.

여기서 나타내는 $\sim$ 표현의 의미는 다음과 같다. $f(x)\sim g(x)$라는 것은
$$\lim_{x\rightarrow\infty}\frac{f(x)}{g(x)}=1$$
이라는 것이다.

### 정칙 변동과 확률 응용 (regular variation and probability applications)

확률 응용에서는 보통 꼬리가 정칙 변동인 분포들을 생각하게 된다. 예를 들면
$$1-F(x)=x^{-\alpha}, \qquad{x\geq 1, \alpha >0}$$
와 같은 분포함수를 생각한다. 이것의 극단값 분포(extreme-value distribution)는
$$\Phi_{\alpha}(x) = \exp\{-x^{-\alpha}\}, \qquad{x\geq 0}$$
이 된다. $\exp\{-x^{-\alpha}\}$는 다음과 같은 성질을 갖는다.
$$1-\Phi_{\alpha}(x)\sim x^{-\alpha}, \qquad{\text{ as }x \rightarrow\infty.}$$

Index $\alpha, 0<\alpha <2$인 **Stable law** ([@Resnick2007]의 154쪽 참고)는 다음과 같은 성질을 갖는다.
$$1-G(x) \sim cx^{-\alpha}, \qquad{x\rightarrow\infty, c>0.}$$
코시 밀도함수 $f(x)=\frac{1}{\pi (1+x^2)}$는 분포함수 $F$를 갖으며 $F$는 다음과 같은 성질을 갖는다.
$$1-F(x) \sim \frac{1}{(\pi x)}.$$
만약 $\mathcal{N}(x)$가 표준정규분포함수라면, $1-\mathcal{N}(x)$는 정칙 변화를 따르지 않고 굼벨 극단값 분포 $1-\exp\{-e^{-x}\}$ 꼬리를 갖지도 않는다. [@Resnick2007]

